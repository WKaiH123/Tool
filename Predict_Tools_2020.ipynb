{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def mkdir(path):\n",
    "    folder = os.path.exists(path)\n",
    "    if not folder:                   \n",
    "        os.makedirs(path)            \n",
    "        print(\"---  new folder...  ---\")\n",
    "        print(\"---  OK  ---\")\n",
    "    else:\n",
    "        print(\"---  There is this folder!  ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error,mean_squared_error,median_absolute_error,r2_score,mean_squared_log_error\n",
    "\n",
    "def calculate(y_true, y_predict, n, p):\n",
    "    y_true = y_true.reshape(-1,1)\n",
    "    y_predict = y_predict.reshape(-1,1)\n",
    "    mse = mean_squared_error(y_true, y_predict)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_predict))\n",
    "    mae = mean_absolute_error(y_true, y_predict)\n",
    "    r2 = r2_score(y_true, y_predict)\n",
    "    mad = median_absolute_error(y_true, y_predict)\n",
    "    mape = np.mean(np.abs((y_true - y_predict) / y_true)) * 100\n",
    "    r2_adjusted = 1-((1-r2)*(n-1))/(n-p-1)\n",
    "#     rmsle = np.sqrt(mean_squared_log_error(y_true,y_predict))\n",
    "    print('MSE: ', mse)\n",
    "    print('RMSE: ', rmse)\n",
    "    print('MAE: ', mae)\n",
    "    print('R2: ', r2)\n",
    "    print('MAD:', mad)\n",
    "    print('MAPE:', mape)\n",
    "    print('R2_Adjusted: ',r2_adjusted)\n",
    "#     print(\"RMSLE: \",rmsle)\n",
    "    return mse,rmse,mae,r2,mad,mape,r2_adjusted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the specified file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_name(file_dir): \n",
    "    L=[] \n",
    "    for root, dirs, files in os.walk(file_dir):\n",
    "        for file in files:\n",
    "            if(os.path.splitext(file)[1] == '.csv'):\n",
    "                L.append(os.path.join(root, file))\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save variables for the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pd.DataFrame()\n",
    "Test = data[data[\"Date\"]>=\"2020-05-31\"]\n",
    "pred[\"Date\"] = Test[\"Date\"]\n",
    "pred[\"Name\"] = Test[\"Name\"]\n",
    "pred.reset_index(drop=True,inplace=True)\n",
    "all_assess = []\n",
    "all_pre = []\n",
    "all_mo = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fitting diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_line(name,pred,true,path):\n",
    "    plt.figure(figsize=(20, 10),edgecolor='white',facecolor='white')\n",
    "    plt.plot(pred, '-', label=\"Predicted\", color=\"blue\", linewidth=5,markersize=5)\n",
    "    plt.plot(true, '--', label=\"Real\", color=\"red\", linewidth=5,markersize=5)\n",
    "    plt.title(name)\n",
    "    plt.legend()\n",
    "    plt.ylabel(\"Daily Confirmed\")\n",
    "    plt.grid()\n",
    "    path1 = \"./result/\" + path + \"/\"\n",
    "    if(os.path.exists(path1)):\n",
    "        pass\n",
    "    else:\n",
    "        mkdir(path1)\n",
    "    path2 = path1  + name + \".pdf\"\n",
    "    plt.savefig(path2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![3](./img/3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_line2(Seq,path):\n",
    "    Citys = np.unique(Seq[\"Name\"])\n",
    "    for c in Citys:\n",
    "        S = Seq[Seq[\"Name\"]==c]\n",
    "        S = S.sort_values(by=\"Date\")\n",
    "        La = S[\"Label\"].values.tolist()\n",
    "        if (\"Train\" in La) and (\"Test\" in La):\n",
    "            print(c,path)\n",
    "            S2 = S.set_index(S[\"Date\"],drop=True)\n",
    "            sns.set_style(\"ticks\")\n",
    "            plt.figure(figsize=(20, 10),edgecolor='white',facecolor='white')\n",
    "            S2[\"Real\"].plot(style=\"--\",fontsize=30,linewidth=5,markersize=5)\n",
    "            S2[path].plot(style=\"-\",fontsize=30,linewidth=5,markersize=5)\n",
    "            plt.xlabel(\"Date\",fontsize=30)\n",
    "            plt.ylabel(\"Daily Confirmed\",fontsize=30)\n",
    "            plt.legend(fontsize=30)\n",
    "            plt.title(c,fontsize=30)\n",
    "            plt.grid()\n",
    "            path1 = \"./result/\" + path + \"/\"\n",
    "            if(os.path.exists(path1)):\n",
    "                pass\n",
    "            else:\n",
    "                mkdir(path1)\n",
    "            path2 = path1  + name + \".pdf\"\n",
    "            plt.savefig(path2)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tradictional ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def adjust_knn(train_x_s,test_x_s,train_y,test_y,name=0):\n",
    "    if(name!=0):\n",
    "        path2 = \"./results/KNN_\" + name + \"_\" + \"assess.csv\"\n",
    "        path3 = \"./results/KNN_\" + name + \"_\" + \"parameter.csv\"\n",
    "    else:\n",
    "        path2 = \"./results/KNN_assess.csv\"\n",
    "        path3 = \"./results/KNN_parameter.csv\"  \n",
    "    all_assessed_values = []\n",
    "    all_parameter = []\n",
    "    n_neighbors = [5,7,9,11,13,15,17,19] # 默认为5\n",
    "    weights = ['uniform', 'distance']\n",
    "    algorithm = [\"brute\",\"kd_tree\",\"ball_tree\"]\n",
    "    leaf_size = [25,30,35,40,45] #默认是30\n",
    "    metric = [\"euclidean\",\"manhattan\",\"chebyshev\",\"minkowski\",\"wminkowski\",\"seuclidean\",\"mahalanobis\"]\n",
    "    P = [1,2] # 只在 wminkowski 和 minkowski 调\n",
    "    all_nb = len(n_neighbors) * len(weights) * len(algorithm) * len(leaf_size) * len(metric) * len(P)\n",
    "    num=1\n",
    "    if(os.path.exists(path2)):\n",
    "        data = pd.read_csv(path2,header=None)\n",
    "        nums = int(data.values[-1,0])\n",
    "        for n in n_neighbors:\n",
    "            for l in leaf_size:\n",
    "                for p in P:\n",
    "                    for a in algorithm:\n",
    "                        for m in metric:\n",
    "                            for w in weights:\n",
    "                                if(nums<num):\n",
    "                                    try:\n",
    "                                        if(m==\"wminkowski\" or m==\"minkowski\"):\n",
    "                                            print(\"start....{}/{}\".format(num,all_nb))\n",
    "                                            knn = KNeighborsRegressor(n_neighbors=n,leaf_size=l,p=p,weights=w,metric=m,algorithm=a)\n",
    "                                            knn.fit(train_x_s,train_y)\n",
    "                                            pred_test = knn.predict(test_x_s)\n",
    "                                            pred_test = pred_test.reshape(-1,1)\n",
    "                                            sample_n = pred_test.shape[0]\n",
    "                                            feature_n = test_x_s.shape[1]\n",
    "                                            mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y,pred_test,sample_n,feature_n)\n",
    "                                            all_m = [num,mse,rmse,mae,r2,mad,mape,r2_adjusted]\n",
    "                                            all_p = [num,m,a,w,n,l,p]\n",
    "                                            print(all_m)\n",
    "                                            with open(path2,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                                f = csv.writer(f)\n",
    "                                                f.writerow(all_m)\n",
    "                                            with open(path3,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                                f = csv.writer(f)\n",
    "                                                f.writerow(all_p)\n",
    "                                            print(\"end....\",num)\n",
    "                                            num = num+1\n",
    "                                            print(\"--------------------------------\")  \n",
    "                                        else:\n",
    "                                            print(\"start....{}/{}\".format(num,all_nb))\n",
    "                                            knn = KNeighborsRegressor(n_neighbors=n,leaf_size=l,weights=w,metric=m,algorithm=a)\n",
    "                                            knn.fit(train_x_s,train_y)\n",
    "                                            pred_test = knn.predict(test_x_s)\n",
    "                                            pred_test = pred_test.reshape(-1,1)\n",
    "                                            sample_n = pred_test.shape[0]\n",
    "                                            feature_n = test_x_s.shape[1]\n",
    "                                            mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y,pred_test,sample_n,feature_n)\n",
    "                                            all_m = [num,mse,rmse,mae,r2,mad,mape,r2_adjusted]\n",
    "                                            all_p = [num,m,a,w,n,l,p]\n",
    "                                            print(all_m)\n",
    "                                            with open(path2,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                                f = csv.writer(f)\n",
    "                                                f.writerow(all_m)\n",
    "                                            with open(path3,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                                f = csv.writer(f)\n",
    "                                                f.writerow(all_p)\n",
    "                                            print(\"end....\",num)\n",
    "                                            num = num+1\n",
    "                                            print(\"--------------------------------\") \n",
    "                                    except:\n",
    "                                        print(\"error\")\n",
    "                                else:\n",
    "                                    num = num+1\n",
    "    else:\n",
    "        for n in n_neighbors:\n",
    "            for l in leaf_size:\n",
    "                for p in P:\n",
    "                    for a in algorithm:\n",
    "                        for m in metric:\n",
    "                            for w in weights:\n",
    "                                try:\n",
    "                                    if(m==\"wminkowski\" or m==\"minkowski\"):\n",
    "                                        print(\"start....{}/{}\".format(num,all_nb))\n",
    "                                        knn = KNeighborsRegressor(n_neighbors=n,leaf_size=l,p=p,weights=w,metric=m,algorithm=a)\n",
    "                                        knn.fit(train_x_s,train_y)\n",
    "                                        pred_test = knn.predict(test_x_s)\n",
    "                                        pred_test = pred_test.reshape(-1,1)\n",
    "                                        sample_n = pred_test.shape[0]\n",
    "                                        feature_n = test_x_s.shape[1]\n",
    "                                        mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y,pred_test,sample_n,feature_n)\n",
    "                                        all_m = [num,mse,rmse,mae,r2,mad,mape,r2_adjusted]\n",
    "                                        all_p = [num,m,a,w,n,l,p]\n",
    "                                        print(all_m)\n",
    "                                        with open(path2,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                            f = csv.writer(f)\n",
    "                                            f.writerow(all_m)\n",
    "                                        with open(path3,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                            f = csv.writer(f)\n",
    "                                            f.writerow(all_p)\n",
    "                                        print(\"end....\",num)\n",
    "                                        num = num+1\n",
    "                                        print(\"--------------------------------\")  \n",
    "                                    else:\n",
    "                                        print(\"start....{}/{}\".format(num,all_nb))\n",
    "                                        knn = KNeighborsRegressor(n_neighbors=n,leaf_size=l,weights=w,metric=m,algorithm=a)\n",
    "                                        knn.fit(train_x_s,train_y)\n",
    "                                        pred_test = knn.predict(test_x_s)\n",
    "                                        pred_test = pred_test.reshape(-1,1)\n",
    "                                        sample_n = pred_test.shape[0]\n",
    "                                        feature_n = test_x_s.shape[1]\n",
    "                                        mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y,pred_test,sample_n,feature_n)\n",
    "                                        all_m = [num,mse,rmse,mae,r2,mad,mape,r2_adjusted]\n",
    "                                        all_p = [num,m,a,w,n,l,p]\n",
    "                                        print(all_m)\n",
    "                                        with open(path2,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                            f = csv.writer(f)\n",
    "                                            f.writerow(all_m)\n",
    "                                        with open(path3,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                            f = csv.writer(f)\n",
    "                                            f.writerow(all_p)\n",
    "                                        print(\"end....\",num)\n",
    "                                        num = num+1\n",
    "                                        print(\"--------------------------------\") \n",
    "                                except:\n",
    "                                    print(\"error\")\n",
    "                            else:\n",
    "                                num = num+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = \"manhattan\"\n",
    "a = \"kd_tree\"\n",
    "w = \"distance\"\n",
    "n = 5\n",
    "l = 25\n",
    "p = 2\n",
    "\n",
    "knn = KNeighborsRegressor(n_neighbors=n,leaf_size=l,p=p,weights=w,metric=m,algorithm=a)\n",
    "knn.fit(train_X_n,train_y_n)\n",
    "pred_test = knn.predict(test_X_n)\n",
    "pred_test = pred_test.reshape(-1,1)\n",
    "sample_n = pred_test.shape[0]\n",
    "feature_n = test_X_n.shape[1]\n",
    "mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y_n,pred_test,sample_n,feature_n)\n",
    "plt_line(\"KNN\",pred_test,test_y_n,\"plt\")\n",
    "all_assess.append([\"KNN\",mse,rmse,mae,r2,mad,mape,r2_adjusted])\n",
    "all_pre = all_pre + pred_test.reshape(1,-1)[0].tolist()\n",
    "all_mo = all_mo + [\"KNN\" for n in range(sample_n)]\n",
    "pred[\"KNN\"] = pd.DataFrame(pred_test.reshape(-1,1))\n",
    "# Seq[\"KNN\"] = knn.predict(train_X_n).reshape(1,-1)[0].tolist() + pred_test.reshape(1,-1)[0].tolist()\n",
    "Seq[\"KNN\"] = train_y_n.reshape(1,-1)[0].tolist() + pred_test.reshape(1,-1)[0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import csv\n",
    "import os\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "\n",
    "def adjust_knn(train_x_s,test_x_s,train_y,test_y,name=0):\n",
    "    if(name!=0):\n",
    "        path2 = \"./results/KNN_\" + name + \"_\" + \"assess.csv\"\n",
    "        path3 = \"./results/KNN_\" + name + \"_\" + \"parameter.csv\"\n",
    "    else:\n",
    "        path2 = \"./results/KNN_assess.csv\"\n",
    "        path3 = \"./results/KNN_parameter.csv\"\n",
    "    all_assessed_values = []\n",
    "    all_parameter = []\n",
    "    n_neighbors = [5,7,9,11,13,15,17,19] # default=5\n",
    "    weights = ['uniform', 'distance'] # default=’uniform’\n",
    "    algorithm = [\"auto\",\"brute\",\"kd_tree\",\"ball_tree\"] # default=’auto’\n",
    "    leaf_size = [20,25,30,35,40] # default=30\n",
    "    metric = [\"euclidean\",\"manhattan\",\"chebyshev\",\"minkowski\",\"wminkowski\",\"seuclidean\",\"mahalanobis\"] # default=’minkowski’\n",
    "    P = [1,2] # 只在minkowski调 , default=2\n",
    "    all_nb = len(n_neighbors) * len(weights) * len(algorithm) * len(leaf_size) * len(metric) * len(P)\n",
    "    num=1\n",
    "    n_jobs = -1\n",
    "    if(os.path.exists(path2)):\n",
    "        data = pd.read_csv(path2,header=None)\n",
    "        nums = int(data.values[-1,0])\n",
    "        for n in n_neighbors:\n",
    "            for l in leaf_size:\n",
    "                for p in P:\n",
    "                    for a in algorithm:\n",
    "                        for m in metric:\n",
    "                            for w in weights:\n",
    "                                if(nums<num):\n",
    "                                    try:\n",
    "                                        if(m==\"minkowski\"):\n",
    "                                            print(\"start....{}/{}\".format(num,all_nb))\n",
    "                                            knn = KNeighborsClassifier(n_neighbors=n,leaf_size=l,p=p,weights=w,metric=m,algorithm=a,n_jobs=-1)\n",
    "                                            knn.fit(train_x_s,train_y)\n",
    "                                            pred_test = knn.predict(test_x_s)\n",
    "                                            pred_test = pred_test.reshape(-1,1)\n",
    "                                            sample_n = pred_test.shape[0]\n",
    "                                            feature_n = test_x_s.shape[1]\n",
    "                                            mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y,pred_test,sample_n,feature_n)\n",
    "                                            all_m = [num,mse,rmse,mae,r2,mad,mape,r2_adjusted]\n",
    "                                            all_p = [num,m,a,w,n,l,p]\n",
    "#                                             print(all_m)\n",
    "                                            with open(path2,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                                f = csv.writer(f)\n",
    "                                                f.writerow(all_m)\n",
    "                                            with open(path3,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                                f = csv.writer(f)\n",
    "                                                f.writerow(all_p)\n",
    "                                            print(\"end....\",num)\n",
    "                                            num = num+1\n",
    "                                            print(\"--------------------------------\")  \n",
    "                                        else:\n",
    "                                            print(\"start....{}/{}\".format(num,all_nb))\n",
    "                                            knn = KNeighborsClassifier(n_neighbors=n,leaf_size=l,weights=w,metric=m,algorithm=a,n_jobs=-1)\n",
    "                                            knn.fit(train_x_s,train_y)\n",
    "                                            pred_test = knn.predict(test_x_s)\n",
    "                                            pred_test = pred_test.reshape(-1,1)\n",
    "                                            sample_n = pred_test.shape[0]\n",
    "                                            feature_n = test_x_s.shape[1]\n",
    "                                            mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y,pred_test,sample_n,feature_n)\n",
    "                                            all_m = [num,mse,rmse,mae,r2,mad,mape,r2_adjusted]\n",
    "                                            all_p = [num,m,a,w,n,l,p]\n",
    "#                                             print(all_m)\n",
    "                                            with open(path2,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                                f = csv.writer(f)\n",
    "                                                f.writerow(all_m)\n",
    "                                            with open(path3,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                                f = csv.writer(f)\n",
    "                                                f.writerow(all_p)\n",
    "                                            print(\"end....\",num)\n",
    "                                            num = num+1\n",
    "                                            print(\"--------------------------------\") \n",
    "                                    except:\n",
    "                                        print(\"error\")\n",
    "                                else:\n",
    "                                    num = num+1\n",
    "    else:\n",
    "        for n in n_neighbors:\n",
    "            for l in leaf_size:\n",
    "                for p in P:\n",
    "                    for a in algorithm:\n",
    "                        for m in metric:\n",
    "                            for w in weights:\n",
    "                                try:\n",
    "                                    if(m==\"minkowski\"):\n",
    "                                        print(\"start....{}/{}\".format(num,all_nb))\n",
    "                                        knn = KNeighborsClassifier(n_neighbors=n,leaf_size=l,p=p,weights=w,metric=m,algorithm=a,n_jobs=-1)\n",
    "                                        knn.fit(train_x_s,train_y)\n",
    "                                        pred_test = knn.predict(test_x_s)\n",
    "                                        pred_test = pred_test.reshape(-1,1)\n",
    "                                        sample_n = pred_test.shape[0]\n",
    "                                        feature_n = test_x_s.shape[1]\n",
    "                                        mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y,pred_test,sample_n,feature_n)\n",
    "                                        all_m = [num,mse,rmse,mae,r2,mad,mape,r2_adjusted]\n",
    "                                        all_p = [num,m,a,w,n,l,p]\n",
    "#                                         print(all_m)\n",
    "                                        with open(path2,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                            f = csv.writer(f)\n",
    "                                            f.writerow(all_m)\n",
    "                                        with open(path3,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                            f = csv.writer(f)\n",
    "                                            f.writerow(all_p)\n",
    "                                        print(\"end....\",num)\n",
    "                                        num = num+1\n",
    "                                        print(\"--------------------------------\")  \n",
    "                                    else:\n",
    "                                        print(\"start....{}/{}\".format(num,all_nb))\n",
    "                                        knn = KNeighborsClassifier(n_neighbors=n,leaf_size=l,weights=w,metric=m,algorithm=a,n_jobs=-1)\n",
    "                                        knn.fit(train_x_s,train_y)\n",
    "                                        pred_test = knn.predict(test_x_s)\n",
    "                                        pred_test = pred_test.reshape(-1,1)\n",
    "                                        sample_n = pred_test.shape[0]\n",
    "                                        feature_n = test_x_s.shape[1]\n",
    "                                        mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y,pred_test,sample_n,feature_n)\n",
    "                                        all_m = [num,mse,rmse,mae,r2,mad,mape,r2_adjusted]\n",
    "                                        all_p = [num,m,a,w,n,l,p]\n",
    "#                                         print(all_m)\n",
    "                                        with open(path2,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                            f = csv.writer(f)\n",
    "                                            f.writerow(all_m)\n",
    "                                        with open(path3,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                            f = csv.writer(f)\n",
    "                                            f.writerow(all_p)\n",
    "                                        print(\"end....\",num)\n",
    "                                        num = num+1\n",
    "                                        print(\"--------------------------------\") \n",
    "                                except:\n",
    "                                    print(\"error\")\n",
    "                            else:\n",
    "                                num = num+1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def adjust_dt(train_x_s,test_x_s,train_y,test_y,name):\n",
    "    path2 = \"./results/DT_\" + name + \"_\" + \"assess.csv\"\n",
    "    path3 = \"./results/DT_\" + name + \"_\" + \"parameter.csv\"\n",
    "    all_pred_results = []\n",
    "    all_assessed_values = []\n",
    "    all_parameter = []\n",
    "    criterion = [\"mse\",\"mae\"]\n",
    "    splitter = [\"best\",\"random\"]\n",
    "    max_depth = None\n",
    "    min_samples_split = [\"None\",2,3,4,5,6,7,8,9,10]\n",
    "    max_features = [\"None\",\"auto\",\"sqrt\",\"log2\"]\n",
    "    random_state = 17\n",
    "    max_leaf_nodes = None\n",
    "    all_nb = len(criterion) * len(splitter) * len(min_samples_split) * len(max_features)\n",
    "    num=1\n",
    "    if(os.path.exists(path2)):\n",
    "        data = pd.read_csv(path2,header=None)\n",
    "        nums = int(data.values[-1,0])\n",
    "        for c in criterion:\n",
    "            for s in splitter:\n",
    "                for ma in max_features:\n",
    "                    for mi in min_samples_split:\n",
    "                        if(nums<num):\n",
    "                            print(\"start....{}/{}\".format(num,all_nb))\n",
    "                            if(ma==\"None\" and mi!=\"None\"):\n",
    "                                dt = DecisionTreeRegressor(criterion=c,splitter=s,random_state=random_state,min_samples_split=mi)\n",
    "                            elif(ma!=\"None\" and mi!=\"None\"):\n",
    "                                dt = DecisionTreeRegressor(criterion=c,splitter=s,max_features=ma,random_state=random_state,min_samples_split=mi)\n",
    "                            elif(ma!=\"None\" and mi==\"None\"):\n",
    "                                dt = DecisionTreeRegressor(criterion=c,splitter=s,random_state=random_state,max_features=ma)\n",
    "                            else:\n",
    "                                dt = DecisionTreeRegressor(criterion=c,splitter=s,random_state=random_state)\n",
    "                            dt.fit(train_x_s,train_y)\n",
    "                            pred_test = dt.predict(test_x_s)\n",
    "                            pred_test = pred_test.reshape(-1,1)\n",
    "                            sample_n = pred_test.shape[0]\n",
    "                            feature_n = test_x_s.shape[1]\n",
    "                            mse,rmse,mae,r2,mad,r2_adjusted = calculate(test_y,pred_test,sample_n,feature_n)\n",
    "                            all_m = [num,mse,rmse,mae,r2,mad,r2_adjusted]\n",
    "                            all_p = [num,c,s,ma]\n",
    "    #                         print(all_m)\n",
    "                            with open(path2,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                f = csv.writer(f)\n",
    "                                f.writerow(all_m)\n",
    "                            with open(path3,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                f = csv.writer(f)\n",
    "                                f.writerow(all_p)\n",
    "                            print(\"end....\",num)\n",
    "                            num = num+1\n",
    "                            print(\"--------------------------------\")    \n",
    "                        else:\n",
    "                            num = num+1\n",
    "    else:\n",
    "        for c in criterion:\n",
    "            for s in splitter:\n",
    "                for ma in max_features:\n",
    "                    for mi in min_samples_split:\n",
    "                        print(\"start....{}/{}\".format(num,all_nb))\n",
    "                        if(ma==\"None\" and mi!=\"None\"):\n",
    "                            dt = DecisionTreeRegressor(criterion=c,splitter=s,random_state=random_state,min_samples_split=mi)\n",
    "                        elif(ma!=\"None\" and mi!=\"None\"):\n",
    "                            dt = DecisionTreeRegressor(criterion=c,splitter=s,max_features=ma,random_state=random_state,min_samples_split=mi)\n",
    "                        elif(ma!=\"None\" and mi==\"None\"):\n",
    "                            dt = DecisionTreeRegressor(criterion=c,splitter=s,random_state=random_state,max_features=ma)\n",
    "                        else:\n",
    "                            dt = DecisionTreeRegressor(criterion=c,splitter=s,random_state=random_state)\n",
    "                        dt.fit(train_x_s,train_y)\n",
    "                        pred_test = dt.predict(test_x_s)\n",
    "                        pred_test = pred_test.reshape(-1,1)\n",
    "                        sample_n = pred_test.shape[0]\n",
    "                        feature_n = test_x_s.shape[1]\n",
    "                        mse,rmse,mae,r2,mad,r2_adjusted = calculate(test_y,pred_test,sample_n,feature_n)\n",
    "                        all_m = [num,mse,rmse,mae,r2,mad,r2_adjusted]\n",
    "                        all_p = [num,c,s,ma]\n",
    "    #                     print(all_m)\n",
    "                        with open(path2,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                            f = csv.writer(f)\n",
    "                            f.writerow(all_m)\n",
    "                        with open(path3,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                            f = csv.writer(f)\n",
    "                            f.writerow(all_p)\n",
    "                        print(\"end....\",num)\n",
    "                        num = num+1\n",
    "                        print(\"--------------------------------\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = \"mse\"\n",
    "s = \"best\"\n",
    "ma = \"sqrt\"\n",
    "\n",
    "dt = DecisionTreeRegressor(criterion=c,splitter=s,random_state=17)\n",
    "dt.fit(train_X_lg,train_y_lg)\n",
    "pred_test = dt.predict(test_X_lg)\n",
    "pred_test = 10**pred_test.reshape(-1,1)\n",
    "sample_n = pred_test.shape[0]\n",
    "feature_n = test_X_lg.shape[1]\n",
    "mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y_lg,pred_test,sample_n,feature_n)\n",
    "plt_line(\"DT\",pred_test,test_y_lg,\"plt\")\n",
    "all_assess.append([\"DT\",mse,rmse,mae,r2,mad,mape,r2_adjusted])\n",
    "all_pre = all_pre + pred_test.reshape(1,-1)[0].tolist()\n",
    "all_mo = all_mo + [\"DT\" for n in range(sample_n)]\n",
    "pred[\"DT\"] = pd.DataFrame(pred_test.reshape(-1,1))\n",
    "# Seq[\"DT\"] = (10**dt.predict(train_X_lg).reshape(1,-1)[0]).tolist() + pred_test.reshape(1,-1)[0].tolist()\n",
    "Seq[\"DT\"] = train_y_n.reshape(1,-1)[0].tolist() + pred_test.reshape(1,-1)[0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import csv\n",
    "import os\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "\n",
    "def adjust_dt(train_x_s,test_x_s,train_y,test_y,name=0):\n",
    "    if(name!=0):\n",
    "        path2 = \"./results/DT_\" + name + \"_\" + \"assess.csv\"\n",
    "        path3 = \"./results/DT_\" + name + \"_\" + \"parameter.csv\"\n",
    "    else:\n",
    "        path2 = \"./results/DT_assess.csv\"\n",
    "        path3 = \"./results/DT_parameter.csv\"\n",
    "    all_pred_results = []\n",
    "    all_assessed_values = []\n",
    "    all_parameter = []\n",
    "    criterion = [\"gini\",\"entropy\"] # default=”gini”\n",
    "    splitter = [\"best\",\"random\"] # default=”best”\n",
    "    max_depth = None # default=None\n",
    "    min_samples_split = [\"None\",2,3,4,5,6,7,8,9,10] # default=2\n",
    "    max_features = [\"None\",\"auto\",\"sqrt\",\"log2\"] # default=None\n",
    "    min_samples_leaf = [1,2] # default=1\n",
    "    random_state = 17 # default=None\n",
    "    max_leaf_nodes = None\n",
    "    all_nb = len(criterion) * len(splitter) * len(min_samples_split) * len(max_features) * len(min_samples_leaf)\n",
    "    num=1\n",
    "    if(os.path.exists(path2)):\n",
    "        data = pd.read_csv(path2,header=None)\n",
    "        nums = int(data.values[-1,0])\n",
    "        for c in criterion:\n",
    "            for s in splitter:\n",
    "                for ma in max_features:\n",
    "                    for mi in min_samples_split:\n",
    "                        for ml in min_samples_leaf:\n",
    "                            if(nums<num):\n",
    "                                print(\"start....{}/{}\".format(num,all_nb))\n",
    "                                if(ma==\"None\" and mi!=\"None\"):\n",
    "                                    dt = DecisionTreeClassifier(min_samples_leaf=ml,criterion=c,splitter=s,random_state=random_state,min_samples_split=mi)\n",
    "                                elif(ma!=\"None\" and mi!=\"None\"):\n",
    "                                    dt = DecisionTreeClassifier(min_samples_leaf=ml,criterion=c,splitter=s,max_features=ma,random_state=random_state,min_samples_split=mi)\n",
    "                                elif(ma!=\"None\" and mi==\"None\"):\n",
    "                                    dt = DecisionTreeClassifier(min_samples_leaf=ml,criterion=c,splitter=s,random_state=random_state,max_features=ma)\n",
    "                                else:\n",
    "                                    dt = DecisionTreeClassifier(min_samples_leaf=ml,criterion=c,splitter=s,random_state=random_state)\n",
    "                                dt.fit(train_x_s,train_y)\n",
    "                                pred_test = dt.predict(test_x_s)\n",
    "                                pred_test = pred_test.reshape(-1,1)\n",
    "                                sample_n = pred_test.shape[0]\n",
    "                                feature_n = test_x_s.shape[1]\n",
    "                                mse,rmse,mae,r2,mad,r2_adjusted = calculate(test_y,pred_test,sample_n,feature_n)\n",
    "                                all_m = [num,mse,rmse,mae,r2,mad,r2_adjusted]\n",
    "                                all_p = [num,c,s,ma,mi,ml]\n",
    "        #                         print(all_m)\n",
    "                                with open(path2,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                    f = csv.writer(f)\n",
    "                                    f.writerow(all_m)\n",
    "                                with open(path3,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                    f = csv.writer(f)\n",
    "                                    f.writerow(all_p)\n",
    "                                print(\"end....\",num)\n",
    "                                num = num+1\n",
    "                                print(\"--------------------------------\")    \n",
    "                            else:\n",
    "                                num = num+1\n",
    "    else:\n",
    "        for c in criterion:\n",
    "            for s in splitter:\n",
    "                for ma in max_features:\n",
    "                    for mi in min_samples_split:\n",
    "                        for ml in min_samples_leaf:\n",
    "                            print(\"start....{}/{}\".format(num,all_nb))\n",
    "                            if(ma==\"None\" and mi!=\"None\"):\n",
    "                                dt = DecisionTreeClassifier(min_samples_leaf=ml,criterion=c,splitter=s,random_state=random_state,min_samples_split=mi)\n",
    "                            elif(ma!=\"None\" and mi!=\"None\"):\n",
    "                                dt = DecisionTreeClassifier(min_samples_leaf=ml,criterion=c,splitter=s,max_features=ma,random_state=random_state,min_samples_split=mi)\n",
    "                            elif(ma!=\"None\" and mi==\"None\"):\n",
    "                                dt = DecisionTreeClassifier(min_samples_leaf=ml,criterion=c,splitter=s,random_state=random_state,max_features=ma)\n",
    "                            else:\n",
    "                                dt = DecisionTreeClassifier(min_samples_leaf=ml,criterion=c,splitter=s,random_state=random_state)\n",
    "                            dt.fit(train_x_s,train_y)\n",
    "                            pred_test = dt.predict(test_x_s)\n",
    "                            pred_test = pred_test.reshape(-1,1)\n",
    "                            sample_n = pred_test.shape[0]\n",
    "                            feature_n = test_x_s.shape[1]\n",
    "                            mse,rmse,mae,r2,mad,r2_adjusted = calculate(test_y,pred_test,sample_n,feature_n)\n",
    "                            all_m = [num,mse,rmse,mae,r2,mad,r2_adjusted]\n",
    "                            all_p = [num,c,s,ma,mi,ml]\n",
    "        #                     print(all_m)\n",
    "                            with open(path2,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                f = csv.writer(f)\n",
    "                                f.writerow(all_m)\n",
    "                            with open(path3,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                f = csv.writer(f)\n",
    "                                f.writerow(all_p)\n",
    "                            print(\"end....\",num)\n",
    "                            num = num+1\n",
    "                            print(\"--------------------------------\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def adjust_svr(train_x_s,test_x_s,train_y,test_y,name=0):\n",
    "    if(name!=0):\n",
    "        path2 = \"./results/SVM_\" + name + \"_\" + \"assess.csv\"\n",
    "        path3 = \"./results/SVM_\" + name + \"_\" + \"parameter.csv\"\n",
    "    else:\n",
    "        path2 = \"./results/SVM_assess.csv\"\n",
    "        path3 = \"./results/SVM_parameter.csv\"\n",
    "    all_pred_results = []\n",
    "    all_assessed_values = []\n",
    "    all_parameter = []\n",
    "    kernel = [\"rbf\",\"linear\",\"poly\",\"sigmoid\"]\n",
    "    degree = [2,3,4,5,6,7,8,9,10,11,12]\n",
    "    gamma = [\"auto\",\"scale\"]\n",
    "    tol = [1e-3,3e-3,2e-3,1e-4,4e-3]\n",
    "    all_nb = len(kernel) * len(degree) * len(gamma) * len(tol) \n",
    "    num=1\n",
    "    if(os.path.exists(path2)):\n",
    "        data = pd.read_csv(path2,header=None)\n",
    "        nums = int(data.values[-1,0])\n",
    "        for k in kernel:\n",
    "            if(k==\"poly\"):\n",
    "                for d in degree:\n",
    "                    for g in gamma:\n",
    "                        if(nums<num):\n",
    "                            print(\"start....{}/{}\".format(num,all_nb))\n",
    "                            svr = SVR(kernel=k,degree=d,gamma=g)\n",
    "                            svr.fit(train_x_s,train_y.ravel())\n",
    "                            pred_test = svr.predict(test_x_s)\n",
    "                            pred_test = pred_test.reshape(-1,1)\n",
    "                            sample_n = pred_test.shape[0]\n",
    "                            feature_n = test_x_s.shape[1]\n",
    "                            mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y,pred_test,sample_n,feature_n)\n",
    "                            all_m = [num,mse,rmse,mae,r2,mad,mape,r2_adjusted]\n",
    "                            all_p = [num,k,d,g]\n",
    "#                             print(all_m)\n",
    "                            with open(path2,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                f = csv.writer(f)\n",
    "                                f.writerow(all_m)\n",
    "                            with open(path3,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                f = csv.writer(f)\n",
    "                                f.writerow(all_p)\n",
    "                            print(\"end....\",num)\n",
    "                            num = num+1\n",
    "                            print(\"--------------------------------\")\n",
    "                        else:\n",
    "                            num = num+1\n",
    "            elif(k==\"rbf\" or k==\"sigmoid\"):\n",
    "                for g in gamma:\n",
    "                    if(nums<num):\n",
    "                        print(\"start....{}/{}\".format(num,all_nb))\n",
    "                        svr = SVR(kernel=k,gamma=g)\n",
    "                        svr.fit(train_x_s,train_y.ravel())\n",
    "                        pred_test = svr.predict(test_x_s)\n",
    "                        pred_test = pred_test.reshape(-1,1)\n",
    "                        sample_n = pred_test.shape[0]\n",
    "                        feature_n = test_x_s.shape[1]\n",
    "                        mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y,pred_test,sample_n,feature_n)\n",
    "                        all_m = [num,mse,rmse,mae,r2,mad,mape,r2_adjusted]\n",
    "                        all_p = [num,k,\"None\",g]\n",
    "#                         print(all_m)\n",
    "                        with open(path2,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                            f = csv.writer(f)\n",
    "                            f.writerow(all_m)\n",
    "                        with open(path3,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                            f = csv.writer(f)\n",
    "                            f.writerow(all_p)\n",
    "                        print(\"end....\",num)\n",
    "                        print(\"--------------------------------\")\n",
    "                        num = num+1\n",
    "                    else:\n",
    "                        num = num+1\n",
    "            else:\n",
    "                if(nums<num):\n",
    "                    print(\"start....{}/{}\".format(num,all_nb))\n",
    "                    svr = SVR(kernel=k)\n",
    "                    svr.fit(train_x_s,train_y.ravel())\n",
    "                    pred_test = svr.predict(test_x_s)\n",
    "                    pred_test = pred_test.reshape(-1,1)\n",
    "                    sample_n = pred_test.shape[0]\n",
    "                    feature_n = test_x_s.shape[1]\n",
    "                    mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y,pred_test,sample_n,feature_n)\n",
    "                    all_m = [num,mse,rmse,mae,r2,mad,mape,r2_adjusted]\n",
    "                    all_p = [num,k,\"None\",\"None\"]\n",
    "#                     print(all_m)\n",
    "                    with open(path2,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                        f = csv.writer(f)\n",
    "                        f.writerow(all_m)\n",
    "                    with open(path3,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                        f = csv.writer(f)\n",
    "                        f.writerow(all_p)\n",
    "                    print(\"end....\",num)\n",
    "                    num = num+1\n",
    "                    print(\"--------------------------------\")  \n",
    "                else:\n",
    "                    num = num+1\n",
    "    else:\n",
    "        for k in kernel:\n",
    "            if(k==\"poly\"):\n",
    "                for d in degree:\n",
    "                    for g in gamma:\n",
    "                        print(\"start....{}/{}\".format(num,all_nb))\n",
    "                        svr = SVR(kernel=k,degree=d,gamma=g)\n",
    "                        svr.fit(train_x_s,train_y.ravel())\n",
    "                        pred_test = svr.predict(test_x_s)\n",
    "                        pred_test = pred_test.reshape(-1,1)\n",
    "                        sample_n = pred_test.shape[0]\n",
    "                        feature_n = test_x_s.shape[1]\n",
    "                        mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y,pred_test,sample_n,feature_n)\n",
    "                        all_m = [num,mse,rmse,mae,r2,mad,mape,r2_adjusted]\n",
    "                        all_p = [num,k,d,g]\n",
    "#                         print(all_m)\n",
    "                        with open(path2,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                            f = csv.writer(f)\n",
    "                            f.writerow(all_m)\n",
    "                        with open(path3,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                            f = csv.writer(f)\n",
    "                            f.writerow(all_p)\n",
    "                        print(\"end....\",num)\n",
    "                        num = num+1\n",
    "                        print(\"--------------------------------\")\n",
    "            elif(k==\"rbf\" or k==\"sigmoid\"):\n",
    "                for g in gamma:\n",
    "                    print(\"start....{}/{}\".format(num,all_nb))\n",
    "                    svr = SVR(kernel=k,gamma=g)\n",
    "                    svr.fit(train_x_s,train_y.ravel())\n",
    "                    pred_test = svr.predict(test_x_s)\n",
    "                    pred_test = pred_test.reshape(-1,1)\n",
    "                    sample_n = pred_test.shape[0]\n",
    "                    feature_n = test_x_s.shape[1]\n",
    "                    mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y,pred_test,sample_n,feature_n)\n",
    "                    all_m = [num,mse,rmse,mae,r2,mad,mape,r2_adjusted]\n",
    "                    all_p = [num,k,\"None\",g]\n",
    "#                     print(all_m)\n",
    "                    with open(path2,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                        f = csv.writer(f)\n",
    "                        f.writerow(all_m)\n",
    "                    with open(path3,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                        f = csv.writer(f)\n",
    "                        f.writerow(all_p)\n",
    "                    print(\"end....\",num)\n",
    "                    print(\"--------------------------------\")\n",
    "                    num = num+1\n",
    "            else:\n",
    "                print(\"start....{}/{}\".format(num,all_nb))\n",
    "                svr = SVR(kernel=k)\n",
    "                svr.fit(train_x_s,train_y.ravel())\n",
    "                pred_test = svr.predict(test_x_s)\n",
    "                pred_test = pred_test.reshape(-1,1)\n",
    "                sample_n = pred_test.shape[0]\n",
    "                feature_n = test_x_s.shape[1]\n",
    "                mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y,pred_test,sample_n,feature_n)\n",
    "                all_m = [num,mse,rmse,mae,r2,mad,mape,r2_adjusted]\n",
    "                all_p = [num,k,\"None\",\"None\"]\n",
    "#                 print(all_m)\n",
    "                with open(path2,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                    f = csv.writer(f)\n",
    "                    f.writerow(all_m)\n",
    "                with open(path3,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                    f = csv.writer(f)\n",
    "                    f.writerow(all_p)\n",
    "                print(\"end....\",num)\n",
    "                num = num+1\n",
    "                print(\"--------------------------------\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = \"poly\"\n",
    "d = 3\n",
    "g = \"auto\"\n",
    "\n",
    "svr = SVR(kernel=k,degree=d,gamma=g)\n",
    "svr.fit(train_X_n,train_y_n)\n",
    "pred_test = svr.predict(test_X_n)\n",
    "pred_test = pred_test.reshape(-1,1)\n",
    "sample_n = pred_test.shape[0]\n",
    "feature_n = test_X_n.shape[1]\n",
    "mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y_n,pred_test,sample_n,feature_n)\n",
    "plt_line(\"SVR\",pred_test,test_y_n,\"plt\")\n",
    "all_assess.append([\"SVR\",mse,rmse,mae,r2,mad,mape,r2_adjusted])\n",
    "all_pre = all_pre + pred_test.reshape(1,-1)[0].tolist()\n",
    "all_mo = all_mo + [\"SVR\" for n in range(sample_n)]\n",
    "pred[\"SVR\"] = pd.DataFrame(pred_test.reshape(-1,1))\n",
    "# Seq[\"SVR\"] = svr.predict(train_X_n).reshape(1,-1)[0].tolist() + pred_test.reshape(1,-1)[0].tolist()\n",
    "Seq[\"SVR\"] = train_y_n.reshape(1,-1)[0].tolist() + pred_test.reshape(1,-1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "import csv\n",
    "import os\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "\n",
    "def adjust_svc(train_x_s,test_x_s,train_y,test_y,name):\n",
    "    path2 = \"./results/SVM_\" + name + \"_\" + \"assess.csv\"\n",
    "    path3 = \"./results/SVM_\" + name + \"_\" + \"parameter.csv\"\n",
    "    all_pred_results = []\n",
    "    all_assessed_values = []\n",
    "    all_parameter = []\n",
    "    random_state = 17 # default=None\n",
    "    C = [0.5,1.0,1.5] # default=1.0\n",
    "    shrinking = [True,False] # default=True\n",
    "    kernel = [\"rbf\",\"linear\",\"poly\",\"sigmoid\"] # default=’rbf’\n",
    "    degree = [2,3,4,5,6,7,8,9,10,11,12] # default=3\n",
    "    gamma = [\"auto\",\"scale\"] # default=’scale’\n",
    "    tol = [1e-2,1e-3,3e-3,2e-3,1e-4,4e-3] # default=1e-3\n",
    "    all_nb = len(kernel) * len(degree) * len(gamma) * len(tol) * len(C) * len(shrinking)\n",
    "    num=1\n",
    "    if(os.path.exists(path2)):\n",
    "        data = pd.read_csv(path2,header=None)\n",
    "        nums = int(data.values[-1,0])\n",
    "        for k in kernel:\n",
    "            if(k==\"poly\"):\n",
    "                for d in degree:\n",
    "                    for g in gamma:\n",
    "                        for c in C:\n",
    "                            for s in shrinking:\n",
    "                                if(nums<num):\n",
    "                                    print(\"start....{}/{}\".format(num,all_nb))\n",
    "                                    svr = SVC(kernel=k,degree=d,gamma=g,random_state=random_state)\n",
    "                                    svr.fit(train_x_s,train_y.ravel())\n",
    "                                    pred_test = svr.predict(test_x_s)\n",
    "                                    pred_test = pred_test.reshape(-1,1)\n",
    "                                    sample_n = pred_test.shape[0]\n",
    "                                    feature_n = test_x_s.shape[1]\n",
    "                                    mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y,pred_test,sample_n,feature_n)\n",
    "                                    all_m = [num,mse,rmse,mae,r2,mad,mape,r2_adjusted]\n",
    "                                    all_p = [num,k,d,g,c,s]\n",
    "#                                     print(all_m)\n",
    "                                    with open(path2,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                        f = csv.writer(f)\n",
    "                                        f.writerow(all_m)\n",
    "                                    with open(path3,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                        f = csv.writer(f)\n",
    "                                        f.writerow(all_p)\n",
    "                                    print(\"end....\",num)\n",
    "                                    num = num+1\n",
    "                                    print(\"--------------------------------\")\n",
    "                                else:\n",
    "                                    num = num+1\n",
    "            elif(k==\"rbf\" or k==\"sigmoid\"):\n",
    "                for g in gamma:\n",
    "                    for c in C:\n",
    "                        for s in shrinking:\n",
    "                            if(nums<num):\n",
    "                                print(\"start....{}/{}\".format(num,all_nb))\n",
    "                                svr = SVC(kernel=k,gamma=g,random_state=random_state)\n",
    "                                svr.fit(train_x_s,train_y.ravel())\n",
    "                                pred_test = svr.predict(test_x_s)\n",
    "                                pred_test = pred_test.reshape(-1,1)\n",
    "                                sample_n = pred_test.shape[0]\n",
    "                                feature_n = test_x_s.shape[1]\n",
    "                                mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y,pred_test,sample_n,feature_n)\n",
    "                                all_m = [num,mse,rmse,mae,r2,mad,mape,r2_adjusted]\n",
    "                                all_p = [num,k,\"None\",g,c,s]\n",
    "#                                 print(all_m)\n",
    "                                with open(path2,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                    f = csv.writer(f)\n",
    "                                    f.writerow(all_m)\n",
    "                                with open(path3,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                    f = csv.writer(f)\n",
    "                                    f.writerow(all_p)\n",
    "                                print(\"end....\",num)\n",
    "                                print(\"--------------------------------\")\n",
    "                                num = num+1\n",
    "                            else:\n",
    "                                num = num+1\n",
    "            else:\n",
    "                if(nums<num):\n",
    "                    for c in C:\n",
    "                        for s in shrinking:\n",
    "                            print(\"start....{}/{}\".format(num,all_nb))\n",
    "                            svr = SVC(kernel=k,random_state=random_state)\n",
    "                            svr.fit(train_x_s,train_y.ravel())\n",
    "                            pred_test = svr.predict(test_x_s)\n",
    "                            pred_test = pred_test.reshape(-1,1)\n",
    "                            sample_n = pred_test.shape[0]\n",
    "                            feature_n = test_x_s.shape[1]\n",
    "                            mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y,pred_test,sample_n,feature_n)\n",
    "                            all_m = [num,mse,rmse,mae,r2,mad,mape,r2_adjusted]\n",
    "                            all_p = [num,k,\"None\",\"None\",c,s]\n",
    "#                             print(all_m)\n",
    "                            with open(path2,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                f = csv.writer(f)\n",
    "                                f.writerow(all_m)\n",
    "                            with open(path3,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                f = csv.writer(f)\n",
    "                                f.writerow(all_p)\n",
    "                            print(\"end....\",num)\n",
    "                            num = num+1\n",
    "                            print(\"--------------------------------\")  \n",
    "                        else:\n",
    "                            num = num+1\n",
    "    else:\n",
    "        for k in kernel:\n",
    "            if(k==\"poly\"):\n",
    "                for d in degree:\n",
    "                    for g in gamma:\n",
    "                        for c in C:\n",
    "                            for s in shrinking:\n",
    "                                print(\"start....{}/{}\".format(num,all_nb))\n",
    "                                svr = SVC(kernel=k,degree=d,gamma=g,random_state=random_state)\n",
    "                                svr.fit(train_x_s,train_y.ravel())\n",
    "                                pred_test = svr.predict(test_x_s)\n",
    "                                pred_test = pred_test.reshape(-1,1)\n",
    "                                sample_n = pred_test.shape[0]\n",
    "                                feature_n = test_x_s.shape[1]\n",
    "                                mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y,pred_test,sample_n,feature_n)\n",
    "                                all_m = [num,mse,rmse,mae,r2,mad,mape,r2_adjusted]\n",
    "                                all_p = [num,k,d,g,c,s]\n",
    "#                                 print(all_m)\n",
    "                                with open(path2,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                    f = csv.writer(f)\n",
    "                                    f.writerow(all_m)\n",
    "                                with open(path3,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                    f = csv.writer(f)\n",
    "                                    f.writerow(all_p)\n",
    "                                print(\"end....\",num)\n",
    "                                num = num+1\n",
    "                                print(\"--------------------------------\")\n",
    "            elif(k==\"rbf\" or k==\"sigmoid\"):\n",
    "                for g in gamma:\n",
    "                    for c in C:\n",
    "                        for s in shrinking:\n",
    "                            print(\"start....{}/{}\".format(num,all_nb))\n",
    "                            svr = SVC(kernel=k,gamma=g,random_state=random_state)\n",
    "                            svr.fit(train_x_s,train_y.ravel())\n",
    "                            pred_test = svr.predict(test_x_s)\n",
    "                            pred_test = pred_test.reshape(-1,1)\n",
    "                            sample_n = pred_test.shape[0]\n",
    "                            feature_n = test_x_s.shape[1]\n",
    "                            mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y,pred_test,sample_n,feature_n)\n",
    "                            all_m = [num,mse,rmse,mae,r2,mad,mape,r2_adjusted]\n",
    "                            all_p = [num,k,\"None\",g,c,s]\n",
    "#                             print(all_m)\n",
    "                            with open(path2,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                f = csv.writer(f)\n",
    "                                f.writerow(all_m)\n",
    "                            with open(path3,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                f = csv.writer(f)\n",
    "                                f.writerow(all_p)\n",
    "                            print(\"end....\",num)\n",
    "                            print(\"--------------------------------\")\n",
    "                            num = num+1\n",
    "            else:\n",
    "                for c in C:\n",
    "                    for s in shrinking:\n",
    "                        print(\"start....{}/{}\".format(num,all_nb))\n",
    "                        svr = SVC(kernel=k,random_state=random_state)\n",
    "                        svr.fit(train_x_s,train_y.ravel())\n",
    "                        pred_test = svr.predict(test_x_s)\n",
    "                        pred_test = pred_test.reshape(-1,1)\n",
    "                        sample_n = pred_test.shape[0]\n",
    "                        feature_n = test_x_s.shape[1]\n",
    "                        mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y,pred_test,sample_n,feature_n)\n",
    "                        all_m = [num,mse,rmse,mae,r2,mad,mape,r2_adjusted]\n",
    "                        all_p = [num,k,\"None\",\"None\",c,s]\n",
    "#                         print(all_m)\n",
    "                        with open(path2,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                            f = csv.writer(f)\n",
    "                            f.writerow(all_m)\n",
    "                        with open(path3,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                            f = csv.writer(f)\n",
    "                            f.writerow(all_p)\n",
    "                        print(\"end....\",num)\n",
    "                        num = num+1\n",
    "                        print(\"--------------------------------\")    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def adjust_ada(train_x_s,test_x_s,train_y,test_y,name):\n",
    "    path2 = \"./results/Ada_\" + name + \"_\" + \"assess.csv\"\n",
    "    path3 = \"./results/Ada_\" + name + \"_\" + \"parameter.csv\"\n",
    "    all_assessed_values = []\n",
    "    all_parameter = []\n",
    "    n_estimators = [50,100,200,300,400,500,600,700,800]\n",
    "    learning_rate = [0.1,0.25,0.5,0.75,1]\n",
    "    loss = [\"linear\",\"square\",\"exponential\"]\n",
    "    criterion = [\"mse\",\"mae\"]\n",
    "    splitter = [\"best\",\"random\"]\n",
    "    max_features = [\"None\"] # ,\"log2\",\"sqrt\",\"auto\"\n",
    "    max_leaf_nodes = [\"None\"]\n",
    "    min_samples_split = [2]\n",
    "    min_samples_leaf = [1]\n",
    "    all_nb = len(n_estimators) * len(learning_rate) * len(loss) * len(criterion) * len(splitter) * len(max_features) * len(max_leaf_nodes) * len(min_samples_leaf) * len(min_samples_split)\n",
    "    num = 1\n",
    "    if(os.path.exists(path2)):\n",
    "        data = pd.read_csv(path2,header=None)\n",
    "        nums = int(data.values[-1,0])\n",
    "        for n in n_estimators:\n",
    "            for l in learning_rate:\n",
    "                for lo in loss:\n",
    "                    for mf in max_features:\n",
    "                        for mi in min_samples_split:\n",
    "                            for ms in min_samples_leaf:\n",
    "                                for ml in max_leaf_nodes:\n",
    "                                    for sp in splitter:\n",
    "                                        for c in criterion:\n",
    "                                            if(nums<num):\n",
    "                                                print(\"start....{}/{}\".format(num,all_nb))\n",
    "                                                if(mf == \"None\" and ml != \"None\"):\n",
    "                                                    ada = AdaBoostRegressor(n_estimators=n,learning_rate=l,loss=lo,base_estimator=DecisionTreeRegressor(min_samples_split=mi,min_samples_leaf=ms,max_leaf_nodes=ml,splitter=sp,criterion=c))\n",
    "                                                elif(ml == \"None\" and mf != \"None\"):\n",
    "                                                    ada = AdaBoostRegressor(n_estimators=n,learning_rate=l,loss=lo,base_estimator=DecisionTreeRegressor(min_samples_split=mi,min_samples_leaf=ms,splitter=sp,max_features=mf,criterion=c))\n",
    "                                                elif(ml == \"None\" and mf == \"None\"):\n",
    "                                                    ada = AdaBoostRegressor(n_estimators=n,learning_rate=l,loss=lo,base_estimator=DecisionTreeRegressor(min_samples_split=mi,min_samples_leaf=ms,splitter=sp,criterion=c))\n",
    "                                                else:\n",
    "                                                    ada = AdaBoostRegressor(n_estimators=n,learning_rate=l,loss=lo,base_estimator=DecisionTreeRegressor(min_samples_split=mi,min_samples_leaf=ms,max_leaf_nodes=ml,splitter=sp,max_features=mf,criterion=c))\n",
    "                                                ada.fit(train_x_s,train_y.ravel())\n",
    "                                                pred_test = ada.predict(test_x_s)\n",
    "                                                pred_test = pred_test.reshape(-1,1)\n",
    "                                                sample_n = pred_test.shape[0]\n",
    "                                                feature_n = test_x_s.shape[1]\n",
    "                                                mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y,pred_test,sample_n,feature_n)\n",
    "                                                all_m = [num,mse,rmse,mae,r2,mad,mape,r2_adjusted]\n",
    "                                                all_p = [num,n,l,lo,mf,mi,ms,ml,sp,c]\n",
    "                                                print(all_m)\n",
    "                                                with open(path2,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                                    f = csv.writer(f)\n",
    "                                                    f.writerow(all_m)\n",
    "                                                with open(path3,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                                    f = csv.writer(f)\n",
    "                                                    f.writerow(all_p)\n",
    "                                                print(\"end....\",num)\n",
    "                                                num = num+1\n",
    "                                                print(\"--------------------------------\")\n",
    "                                            else:\n",
    "                                                num = num+1\n",
    "    else:\n",
    "        for n in n_estimators:\n",
    "            for l in learning_rate:\n",
    "                for lo in loss:\n",
    "                    for mf in max_features:\n",
    "                        for mi in min_samples_split:\n",
    "                            for ms in min_samples_leaf:\n",
    "                                for ml in max_leaf_nodes:\n",
    "                                    for sp in splitter:\n",
    "                                        for c in criterion:\n",
    "                                            print(\"start....{}/{}\".format(num,all_nb))\n",
    "                                            if(mf == \"None\" and ml != \"None\"):\n",
    "                                                ada = AdaBoostRegressor(n_estimators=n,learning_rate=l,loss=lo,base_estimator=DecisionTreeRegressor(min_samples_split=mi,min_samples_leaf=ms,max_leaf_nodes=ml,splitter=sp,criterion=c))\n",
    "                                            elif(ml == \"None\" and mf != \"None\"):\n",
    "                                                ada = AdaBoostRegressor(n_estimators=n,learning_rate=l,loss=lo,base_estimator=DecisionTreeRegressor(min_samples_split=mi,min_samples_leaf=ms,splitter=sp,max_features=mf,criterion=c))\n",
    "                                            elif(ml == \"None\" and mf == \"None\"):\n",
    "                                                ada = AdaBoostRegressor(n_estimators=n,learning_rate=l,loss=lo,base_estimator=DecisionTreeRegressor(min_samples_split=mi,min_samples_leaf=ms,splitter=sp,criterion=c))\n",
    "                                            else:\n",
    "                                                ada = AdaBoostRegressor(n_estimators=n,learning_rate=l,loss=lo,base_estimator=DecisionTreeRegressor(min_samples_split=mi,min_samples_leaf=ms,max_leaf_nodes=ml,splitter=sp,max_features=mf,criterion=c))\n",
    "                                            ada.fit(train_x_s,train_y.ravel())\n",
    "                                            pred_test = ada.predict(test_x_s)\n",
    "                                            pred_test = pred_test.reshape(-1,1)\n",
    "                                            sample_n = pred_test.shape[0]\n",
    "                                            feature_n = test_x_s.shape[1]\n",
    "                                            mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y,pred_test,sample_n,feature_n)\n",
    "                                            all_m = [num,mse,rmse,mae,r2,mad,mape,r2_adjusted]\n",
    "                                            all_p = [num,n,l,lo,mf,mi,ms,ml,sp,c]\n",
    "                                            print(all_m)\n",
    "                                            with open(path2,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                                f = csv.writer(f)\n",
    "                                                f.writerow(all_m)\n",
    "                                            with open(path3,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                                f = csv.writer(f)\n",
    "                                                f.writerow(all_p)\n",
    "                                            print(\"end....\",num)\n",
    "                                            num = num+1\n",
    "                                            print(\"--------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50\n",
    "l = 0.25\n",
    "lo = \"exponential\"\n",
    "mi = 2\n",
    "ms = 1\n",
    "sp = \"random\"\n",
    "c = \"mse\"\n",
    "\n",
    "ada = AdaBoostRegressor(n_estimators=n,learning_rate=l,loss=lo,base_estimator=DecisionTreeRegressor(min_samples_split=mi,min_samples_leaf=ms,splitter=sp,criterion=c))\n",
    "ada.fit(train_X_n,train_y_n)\n",
    "pred_test = ada.predict(test_X_n)\n",
    "pred_test = pred_test.reshape(-1,1)\n",
    "sample_n = pred_test.shape[0]\n",
    "feature_n = test_X_n.shape[1]\n",
    "mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y_n,pred_test,sample_n,feature_n)\n",
    "plt_line(\"Ada\",pred_test,test_y_n,\"plt\")\n",
    "all_assess.append([\"Ada\",mse,rmse,mae,r2,mad,mape,r2_adjusted])\n",
    "all_pre = all_pre + pred_test.reshape(1,-1)[0].tolist()\n",
    "all_mo = all_mo + [\"Ada\" for n in range(sample_n)]\n",
    "pred[\"Ada\"] = pd.DataFrame(pred_test.reshape(-1,1))\n",
    "# Seq[\"Ada\"] = ada.predict(train_X_n).reshape(1,-1)[0].tolist() + pred_test.reshape(1,-1)[0].tolist()\n",
    "Seq[\"Ada\"] = train_y_n.reshape(1,-1)[0].tolist() + pred_test.reshape(1,-1)[0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GBDT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def adjust_gbdt(train_x_s,test_x_s,train_y,test_y,name):\n",
    "    path2 = \"./results/GBDT_\" + name + \"_\" + \"assess.csv\"\n",
    "    path3 = \"./results/GBDT_\" + name + \"_\" + \"parameter.csv\"\n",
    "    all_assessed_values = []\n",
    "    all_parameter = []\n",
    "    random_state = 17\n",
    "    n_estimators = [100,200,300,400,500,600,700,800]\n",
    "    learning_rate = [0.1,1e-2,0.2,2e-2,1e-3,0.3]\n",
    "    loss = [\"huber\",\"ls\",\"lad\"]\n",
    "    subsample = [1,0.6,0.2]\n",
    "    min_samples_split = [2]\n",
    "    max_depth = [3,7,11]\n",
    "    min_samples_leaf = [1]\n",
    "    all_nb = len(max_depth) * len(n_estimators) * len(learning_rate) * len(loss) * len(subsample) * len(min_samples_leaf) * len(min_samples_split)\n",
    "    num = 1\n",
    "    if(os.path.exists(path2)):\n",
    "        data = pd.read_csv(path2,header=None)\n",
    "        nums = int(data.values[-1,0])\n",
    "        for n in n_estimators:\n",
    "            for l in learning_rate:\n",
    "                for lo in loss:\n",
    "                    for sub in subsample:\n",
    "                        for mi in min_samples_split:\n",
    "                            for ma in max_depth:\n",
    "                                for ms in min_samples_leaf:\n",
    "                                    if(nums<num):\n",
    "                                        print(\"start....{}/{}\".format(num,all_nb))\n",
    "                                        gbrg = GradientBoostingRegressor(random_state=random_state,n_estimators=n,learning_rate=l,loss=lo,subsample=sub,max_depth=ma,min_samples_split=mi,min_samples_leaf=ms)\n",
    "                                        gbrg.fit(train_x_s,train_y)\n",
    "                                        pred_test = gbrg.predict(test_x_s)\n",
    "                                        pred_test = pred_test.reshape(-1,1)\n",
    "                                        sample_n = pred_test.shape[0]\n",
    "                                        feature_n = test_x_s.shape[1]\n",
    "                                        mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y,pred_test,sample_n,feature_n)\n",
    "                                        all_m = [num,mse,rmse,mae,r2,mad,mape,r2_adjusted]\n",
    "                                        all_p = [num,n,l,lo,sub,mi,ma,ms]\n",
    "                                        print(all_m)\n",
    "                                        with open(path2,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                            f = csv.writer(f)\n",
    "                                            f.writerow(all_m)\n",
    "                                        with open(path3,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                            f = csv.writer(f)\n",
    "                                            f.writerow(all_p)\n",
    "                                        print(\"end....\",num)\n",
    "                                        num = num+1\n",
    "                                        print(\"--------------------------------\")    \n",
    "                                    else:\n",
    "                                        num = num+1\n",
    "    else:\n",
    "        for n in n_estimators:\n",
    "            for l in learning_rate:\n",
    "                for lo in loss:\n",
    "                    for sub in subsample:\n",
    "                        for mi in min_samples_split:\n",
    "                            for ma in max_depth:\n",
    "                                for ms in min_samples_leaf:\n",
    "                                    print(\"start....{}/{}\".format(num,all_nb))\n",
    "                                    gbrg = GradientBoostingRegressor(random_state=random_state,n_estimators=n,learning_rate=l,loss=lo,subsample=sub,max_depth=ma,min_samples_split=mi,min_samples_leaf=ms)\n",
    "                                    gbrg.fit(train_x_s,train_y)\n",
    "                                    pred_test = gbrg.predict(test_x_s)\n",
    "                                    pred_test = pred_test.reshape(-1,1)\n",
    "                                    sample_n = pred_test.shape[0]\n",
    "                                    feature_n = test_x_s.shape[1]\n",
    "                                    mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y,pred_test,sample_n,feature_n)\n",
    "                                    all_m = [num,mse,rmse,mae,r2,mad,mape,r2_adjusted]\n",
    "                                    all_p = [num,n,l,lo,sub,mi,ma,ms]\n",
    "                                    print(all_m)\n",
    "                                    with open(path2,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                        f = csv.writer(f)\n",
    "                                        f.writerow(all_m)\n",
    "                                    with open(path3,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                        f = csv.writer(f)\n",
    "                                        f.writerow(all_p)\n",
    "                                    print(\"end....\",num)\n",
    "                                    num = num+1\n",
    "                                    print(\"--------------------------------\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 200\n",
    "l = 0.3\n",
    "lo = \"huber\"\n",
    "sub = 0.2\n",
    "mi = 2\n",
    "ma = 3\n",
    "ms = 1\n",
    "\n",
    "gbrg = GradientBoostingRegressor(random_state=17,n_estimators=n,learning_rate=l,loss=lo,subsample=sub,max_depth=ma,min_samples_split=mi,min_samples_leaf=ms)\n",
    "gbrg.fit(train_X_lg,train_y_lg)\n",
    "pred_test = gbrg.predict(test_X_lg)\n",
    "pred_test = 10**pred_test.reshape(-1,1)\n",
    "sample_n = pred_test.shape[0]\n",
    "feature_n = test_X_lg.shape[1]\n",
    "mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y_lg,pred_test,sample_n,feature_n)\n",
    "plt_line(\"GBDT\",pred_test,test_y_lg,\"plt\")\n",
    "all_assess.append([\"GBDT\",mse,rmse,mae,r2,mad,mape,r2_adjusted])\n",
    "all_pre = all_pre + pred_test.reshape(1,-1)[0].tolist()\n",
    "all_mo = all_mo + [\"GBDT\" for n in range(sample_n)]\n",
    "pred[\"GBDT\"] = pd.DataFrame(pred_test.reshape(-1,1))\n",
    "# Seq[\"GBDT\"] = (10**gbrg.predict(train_X_lg).reshape(1,-1)[0]).tolist() + pred_test.reshape(1,-1)[0].tolist()\n",
    "Seq[\"GBDT\"] = train_y_n.reshape(1,-1)[0].tolist() + pred_test.reshape(1,-1)[0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import csv\n",
    "import os\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\n",
    "\n",
    "def adjust_gbdt(train_x_s,test_x_s,train_y,test_y,name=0):\n",
    "    if(name!=0):\n",
    "        path2 = \"./results/GBDT_\" + name + \"_\" + \"assess.csv\"\n",
    "        path3 = \"./results/GBDT_\" + name + \"_\" + \"parameter.csv\"\n",
    "    else:\n",
    "        path2 = \"./results/GBDT_assess.csv\"\n",
    "        path3 = \"./results/GBDT_parameter.csv\"\n",
    "    all_assessed_values = []\n",
    "    all_parameter = []\n",
    "    random_state = 17\n",
    "    n_estimators = [100,200,300,400,500,600] # default=100\n",
    "    learning_rate = [0.1,1e-2,0.2,1e-3,0.3,0.5]\n",
    "    loss = [\"deviance\",\"exponential\"] # default=’deviance’\n",
    "    subsample = [1,0.6,0.2] # default=1.0\n",
    "    min_samples_split = [2] # default=2\n",
    "    max_depth = [3,4,5,6] # default=3\n",
    "    min_samples_leaf = [1]\n",
    "    max_features = [\"auto\",\"sqrt\",\"log2\"]\n",
    "    all_nb = len(max_features) * len(max_depth) * len(n_estimators) * len(learning_rate) * len(loss) * len(subsample) * len(min_samples_leaf) * len(min_samples_split)\n",
    "    num = 1\n",
    "    if(os.path.exists(path2)):\n",
    "        data = pd.read_csv(path2,header=None)\n",
    "        nums = int(data.values[-1,0])\n",
    "        for n in n_estimators:\n",
    "            for l in learning_rate:\n",
    "                for lo in loss:\n",
    "                    for sub in subsample:\n",
    "                        for mi in min_samples_split:\n",
    "                            for ma in max_depth:\n",
    "                                for ms in min_samples_leaf:\n",
    "                                    for mf in max_features:\n",
    "                                        if(nums<num):\n",
    "                                            print(\"start....{}/{}\".format(num,all_nb))\n",
    "                                            gbrg = GradientBoostingClassifier(max_features=mf,random_state=random_state,n_estimators=n,learning_rate=l,loss=lo,subsample=sub,max_depth=ma,min_samples_split=mi,min_samples_leaf=ms)\n",
    "                                            gbrg.fit(train_x_s,train_y)\n",
    "                                            pred_test = gbrg.predict(test_x_s)\n",
    "                                            pred_test = pred_test.reshape(-1,1)\n",
    "                                            sample_n = pred_test.shape[0]\n",
    "                                            feature_n = test_x_s.shape[1]\n",
    "                                            mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y,pred_test,sample_n,feature_n)\n",
    "                                            all_m = [num,mse,rmse,mae,r2,mad,mape,r2_adjusted]\n",
    "                                            all_p = [num,n,l,lo,sub,mi,ma,ms,mf]\n",
    "#                                             print(all_m)\n",
    "                                            with open(path2,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                                f = csv.writer(f)\n",
    "                                                f.writerow(all_m)\n",
    "                                            with open(path3,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                                f = csv.writer(f)\n",
    "                                                f.writerow(all_p)\n",
    "                                            print(\"end....\",num)\n",
    "                                            num = num+1\n",
    "                                            print(\"--------------------------------\")    \n",
    "                                        else:\n",
    "                                            num = num+1\n",
    "    else:\n",
    "        for n in n_estimators:\n",
    "            for l in learning_rate:\n",
    "                for lo in loss:\n",
    "                    for sub in subsample:\n",
    "                        for mi in min_samples_split:\n",
    "                            for ma in max_depth:\n",
    "                                for ms in min_samples_leaf:\n",
    "                                    for mf in max_features:\n",
    "                                        print(\"start....{}/{}\".format(num,all_nb))\n",
    "                                        gbrg = GradientBoostingClassifier(max_features=mf,random_state=random_state,n_estimators=n,learning_rate=l,loss=lo,subsample=sub,max_depth=ma,min_samples_split=mi,min_samples_leaf=ms)\n",
    "                                        gbrg.fit(train_x_s,train_y)\n",
    "                                        pred_test = gbrg.predict(test_x_s)\n",
    "                                        pred_test = pred_test.reshape(-1,1)\n",
    "                                        sample_n = pred_test.shape[0]\n",
    "                                        feature_n = test_x_s.shape[1]\n",
    "                                        mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y,pred_test,sample_n,feature_n)\n",
    "                                        all_m = [num,mse,rmse,mae,r2,mad,mape,r2_adjusted]\n",
    "                                        all_p = [num,n,l,lo,sub,mi,ma,ms,mf]\n",
    "#                                         print(all_m)\n",
    "                                        with open(path2,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                            f = csv.writer(f)\n",
    "                                            f.writerow(all_m)\n",
    "                                        with open(path3,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                            f = csv.writer(f)\n",
    "                                            f.writerow(all_p)\n",
    "                                        print(\"end....\",num)\n",
    "                                        num = num+1\n",
    "                                        print(\"--------------------------------\")    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def adjust_rf(train_x_s,test_x_s,train_y,test_y,name):\n",
    "    path2 = \"./results/RF_\" + name + \"_\" + \"assess.csv\"\n",
    "    path3 = \"./results/RF_\" + name + \"_\" + \"parameter.csv\"\n",
    "    all_assessed_values = []\n",
    "    all_parameter = []\n",
    "    n_estimators = [100,200,300,400,500,600,700,800]\n",
    "    criterion = [\"mae\",\"mse\"]\n",
    "    max_features = [\"None\",\"log2\",\"sqrt\",\"auto\"]\n",
    "    max_leaf_nodes = [\"None\"]\n",
    "    min_samples_split = [2,3]\n",
    "    min_samples_leaf = [1,2,3]\n",
    "    oob_score = [\"True\",\"False\"]\n",
    "    random_state = 17\n",
    "    n_jobs = -1\n",
    "    all_nb = len(oob_score) * len(n_estimators) * len(criterion) * len(max_features) * len(max_leaf_nodes) * len(min_samples_leaf) * len(min_samples_split)\n",
    "    num = 1\n",
    "    if(os.path.exists(path2)):\n",
    "        data = pd.read_csv(path2,header=None)\n",
    "        nums = int(data.values[-1,0])\n",
    "        for n in n_estimators:\n",
    "            for o in oob_score:\n",
    "                for mf in max_features:\n",
    "                    for mi in min_samples_split:\n",
    "                        for ms in min_samples_leaf:\n",
    "                            for ml in max_leaf_nodes:\n",
    "                                for c in criterion:\n",
    "                                    if(nums<num):\n",
    "                                        print(\"start....{}/{}\".format(num,all_nb))\n",
    "                                        if(ml==\"None\" and mf!= \"None\"):\n",
    "                                            rf = RandomForestRegressor(n_jobs=n_jobs,random_state=random_state,n_estimators=n,oob_score=o,max_features=mf,min_samples_leaf=ms,min_samples_split=mi,criterion=c)\n",
    "                                        elif(ml!=\"None\" and mf==\"None\"):\n",
    "                                            rf = RandomForestRegressor(n_jobs=n_jobs,random_state=random_state,n_estimators=n,oob_score=o,max_leaf_nodes=ml,min_samples_leaf=ms,min_samples_split=mi,criterion=c)\n",
    "                                        elif(ml==\"None\" and mf==\"None\"):\n",
    "                                            rf = RandomForestRegressor(n_jobs=n_jobs,random_state=random_state,n_estimators=n,oob_score=o,min_samples_leaf=ms,min_samples_split=mi,criterion=c)\n",
    "                                        else:\n",
    "                                            rf = RandomForestRegressor(n_jobs=n_jobs,random_state=random_state,n_estimators=n,oob_score=o,max_features=mf,max_leaf_nodes=ml,min_samples_leaf=ms,min_samples_split=mi,criterion=c)\n",
    "                                        rf.fit(train_x_s,train_y.ravel())\n",
    "                                        pred_test = rf.predict(test_x_s)\n",
    "                                        pred_test = pred_test.reshape(-1,1)\n",
    "                                        sample_n = pred_test.shape[0]\n",
    "                                        feature_n = test_x_s.shape[1]\n",
    "                                        mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y,pred_test,sample_n,feature_n)\n",
    "                                        all_m = [num,mse,rmse,mae,r2,mad,mape,r2_adjusted]\n",
    "                                        all_p = [num,n,o,mf,mi,ms,ml,c]\n",
    "                                        print(all_m)\n",
    "                                        with open(path2,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                            f = csv.writer(f)\n",
    "                                            f.writerow(all_m)\n",
    "                                        with open(path3,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                            f = csv.writer(f)\n",
    "                                            f.writerow(all_p)\n",
    "                                        print(\"end....\",num)\n",
    "                                        num = num+1\n",
    "                                        print(\"--------------------------------\")  \n",
    "                                    else:\n",
    "                                        num = num+1\n",
    "    else:\n",
    "        for n in n_estimators:\n",
    "            for o in oob_score:\n",
    "                for mf in max_features:\n",
    "                    for mi in min_samples_split:\n",
    "                        for ms in min_samples_leaf:\n",
    "                            for ml in max_leaf_nodes:\n",
    "                                for c in criterion:\n",
    "                                    print(\"start....{}/{}\".format(num,all_nb))\n",
    "                                    if(ml==\"None\" and mf!= \"None\"):\n",
    "                                        rf = RandomForestRegressor(n_jobs=n_jobs,random_state=random_state,n_estimators=n,oob_score=o,max_features=mf,min_samples_leaf=ms,min_samples_split=mi,criterion=c)\n",
    "                                    elif(ml!=\"None\" and mf==\"None\"):\n",
    "                                        rf = RandomForestRegressor(n_jobs=n_jobs,random_state=random_state,n_estimators=n,oob_score=o,max_leaf_nodes=ml,min_samples_leaf=ms,min_samples_split=mi,criterion=c)\n",
    "                                    elif(ml==\"None\" and mf==\"None\"):\n",
    "                                        rf = RandomForestRegressor(n_jobs=n_jobs,random_state=random_state,n_estimators=n,oob_score=o,min_samples_leaf=ms,min_samples_split=mi,criterion=c)\n",
    "                                    else:\n",
    "                                        rf = RandomForestRegressor(n_jobs=n_jobs,random_state=random_state,n_estimators=n,oob_score=o,max_features=mf,max_leaf_nodes=ml,min_samples_leaf=ms,min_samples_split=mi,criterion=c)\n",
    "                                    rf.fit(train_x_s,train_y.ravel())\n",
    "                                    pred_test = rf.predict(test_x_s)\n",
    "                                    pred_test = pred_test.reshape(-1,1)\n",
    "                                    sample_n = pred_test.shape[0]\n",
    "                                    feature_n = test_x_s.shape[1]\n",
    "                                    mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y,pred_test,sample_n,feature_n)\n",
    "                                    all_m = [num,mse,rmse,mae,r2,mad,mape,r2_adjusted]\n",
    "                                    all_p = [num,n,o,mf,mi,ms,ml,c]\n",
    "                                    print(all_m)\n",
    "                                    with open(path2,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                        f = csv.writer(f)\n",
    "                                        f.writerow(all_m)\n",
    "                                    with open(path3,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                        f = csv.writer(f)\n",
    "                                        f.writerow(all_p)\n",
    "                                    print(\"end....\",num)\n",
    "                                    num = num+1\n",
    "                                    print(\"--------------------------------\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 800\n",
    "o = \"True\"\n",
    "mi = 2\n",
    "ms = 1\n",
    "c = \"mae\"\n",
    "\n",
    "rf = RandomForestRegressor(n_jobs=-1,random_state=17,n_estimators=n,oob_score=o,min_samples_leaf=ms,min_samples_split=mi,criterion=c)\n",
    "rf.fit(train_X_n,train_y_n)\n",
    "pred_test = rf.predict(test_X_n)\n",
    "pred_test = pred_test.reshape(-1,1)\n",
    "sample_n = pred_test.shape[0]\n",
    "feature_n = test_X_n.shape[1]\n",
    "mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y_n,pred_test,sample_n,feature_n)\n",
    "plt_line(\"RF\",pred_test,test_y_n,\"plt\")\n",
    "all_assess.append([\"RF\",mse,rmse,mae,r2,mad,mape,r2_adjusted])\n",
    "all_pre = all_pre + pred_test.reshape(1,-1)[0].tolist()\n",
    "all_mo = all_mo + [\"RF\" for n in range(sample_n)]\n",
    "pred[\"RF\"] = pd.DataFrame(pred_test.reshape(-1,1))\n",
    "# Seq[\"RF\"] = rf.predict(train_X_n).reshape(1,-1)[0].tolist() + pred_test.reshape(1,-1)[0].tolist()\n",
    "Seq[\"RF\"] = train_y_n.reshape(1,-1)[0].tolist() + pred_test.reshape(1,-1)[0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import csv\n",
    "import os\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n",
    "\n",
    "def adjust_rf(train_x_s,test_x_s,train_y,test_y,name=0):\n",
    "    if(name!=0):\n",
    "        path2 = \"./results/RF_\" + name + \"_\" + \"assess.csv\"\n",
    "        path3 = \"./results/RF_\" + name + \"_\" + \"parameter.csv\"\n",
    "    else:\n",
    "        path2 = \"./results/RF_assess.csv\"\n",
    "        path3 = \"./results/RF_parameter.csv\"\n",
    "    all_assessed_values = []\n",
    "    all_parameter = []\n",
    "    n_estimators = [100,200,300,400,500,600,700,800] # default=100\n",
    "    criterion = [\"gini\",\"entropy\"] # default=”gini”\n",
    "    max_features = [\"None\",\"log2\",\"sqrt\",\"auto\"] # default=”auto”\n",
    "    max_leaf_nodes = [\"None\"] # default=None\n",
    "    min_samples_split = [2,3] # default=2\n",
    "    min_samples_leaf = [1,2,3] # default=1\n",
    "    oob_score = [\"True\",\"False\"] # default=False\n",
    "    random_state = 17 # default=None\n",
    "    n_jobs = -1\n",
    "    all_nb = len(oob_score) * len(n_estimators) * len(criterion) * len(max_features) * len(max_leaf_nodes) * len(min_samples_leaf) * len(min_samples_split)\n",
    "    num = 1\n",
    "    if(os.path.exists(path2)):\n",
    "        data = pd.read_csv(path2,header=None)\n",
    "        nums = int(data.values[-1,0])\n",
    "        for n in n_estimators:\n",
    "            for o in oob_score:\n",
    "                for mf in max_features:\n",
    "                    for mi in min_samples_split:\n",
    "                        for ms in min_samples_leaf:\n",
    "                            for ml in max_leaf_nodes:\n",
    "                                for c in criterion:\n",
    "                                    if(nums<num):\n",
    "                                        print(\"start....{}/{}\".format(num,all_nb))\n",
    "                                        if(ml==\"None\" and mf!= \"None\"):\n",
    "                                            rf = RandomForestClassifier(n_jobs=n_jobs,random_state=random_state,n_estimators=n,oob_score=o,max_features=mf,min_samples_leaf=ms,min_samples_split=mi,criterion=c)\n",
    "                                        elif(ml!=\"None\" and mf==\"None\"):\n",
    "                                            rf = RandomForestClassifier(n_jobs=n_jobs,random_state=random_state,n_estimators=n,oob_score=o,max_leaf_nodes=ml,min_samples_leaf=ms,min_samples_split=mi,criterion=c)\n",
    "                                        elif(ml==\"None\" and mf==\"None\"):\n",
    "                                            rf = RandomForestClassifier(n_jobs=n_jobs,random_state=random_state,n_estimators=n,oob_score=o,min_samples_leaf=ms,min_samples_split=mi,criterion=c)\n",
    "                                        else:\n",
    "                                            rf = RandomForestClassifier(n_jobs=n_jobs,random_state=random_state,n_estimators=n,oob_score=o,max_features=mf,max_leaf_nodes=ml,min_samples_leaf=ms,min_samples_split=mi,criterion=c)\n",
    "                                        rf.fit(train_x_s,train_y.ravel())\n",
    "                                        pred_test = rf.predict(test_x_s)\n",
    "                                        pred_test = pred_test.reshape(-1,1)\n",
    "                                        sample_n = pred_test.shape[0]\n",
    "                                        feature_n = test_x_s.shape[1]\n",
    "                                        mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y,pred_test,sample_n,feature_n)\n",
    "                                        all_m = [num,mse,rmse,mae,r2,mad,mape,r2_adjusted]\n",
    "                                        all_p = [num,n,o,mf,mi,ms,ml,c]\n",
    "                                        with open(path2,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                            f = csv.writer(f)\n",
    "                                            f.writerow(all_m)\n",
    "                                        with open(path3,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                            f = csv.writer(f)\n",
    "                                            f.writerow(all_p)\n",
    "                                        print(\"end....\",num)\n",
    "                                        num = num+1\n",
    "                                        print(\"--------------------------------\")  \n",
    "                                    else:\n",
    "                                        num = num+1\n",
    "    else:\n",
    "        for n in n_estimators:\n",
    "            for o in oob_score:\n",
    "                for mf in max_features:\n",
    "                    for mi in min_samples_split:\n",
    "                        for ms in min_samples_leaf:\n",
    "                            for ml in max_leaf_nodes:\n",
    "                                for c in criterion:\n",
    "                                    print(\"start....{}/{}\".format(num,all_nb))\n",
    "                                    if(ml==\"None\" and mf!= \"None\"):\n",
    "                                        rf = RandomForestClassifier(n_jobs=n_jobs,random_state=random_state,n_estimators=n,oob_score=o,max_features=mf,min_samples_leaf=ms,min_samples_split=mi,criterion=c)\n",
    "                                    elif(ml!=\"None\" and mf==\"None\"):\n",
    "                                        rf = RandomForestClassifier(n_jobs=n_jobs,random_state=random_state,n_estimators=n,oob_score=o,max_leaf_nodes=ml,min_samples_leaf=ms,min_samples_split=mi,criterion=c)\n",
    "                                    elif(ml==\"None\" and mf==\"None\"):\n",
    "                                        rf = RandomForestClassifier(n_jobs=n_jobs,random_state=random_state,n_estimators=n,oob_score=o,min_samples_leaf=ms,min_samples_split=mi,criterion=c)\n",
    "                                    else:\n",
    "                                        rf = RandomForestClassifier(n_jobs=n_jobs,random_state=random_state,n_estimators=n,oob_score=o,max_features=mf,max_leaf_nodes=ml,min_samples_leaf=ms,min_samples_split=mi,criterion=c)\n",
    "                                    rf.fit(train_x_s,train_y.ravel())\n",
    "                                    pred_test = rf.predict(test_x_s)\n",
    "                                    pred_test = pred_test.reshape(-1,1)\n",
    "                                    sample_n = pred_test.shape[0]\n",
    "                                    feature_n = test_x_s.shape[1]\n",
    "                                    mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y,pred_test,sample_n,feature_n)\n",
    "                                    all_m = [num,mse,rmse,mae,r2,mad,mape,r2_adjusted]\n",
    "                                    all_p = [num,n,o,mf,mi,ms,ml,c]\n",
    "                                    with open(path2,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                        f = csv.writer(f)\n",
    "                                        f.writerow(all_m)\n",
    "                                    with open(path3,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                        f = csv.writer(f)\n",
    "                                        f.writerow(all_p)\n",
    "                                    print(\"end....\",num)\n",
    "                                    num = num+1\n",
    "                                    print(\"--------------------------------\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def adjust_extreme(train_x_s,test_x_s,train_y,test_y,name):\n",
    "    path2 = \"./results/EXT_\" + name + \"_\" + \"assess.csv\"\n",
    "    path3 = \"./results/EXT_\" + name + \"_\" + \"parameter.csv\"\n",
    "    all_assessed_values = []\n",
    "    all_parameter = []\n",
    "    n_estimators = [10,50,100,200,300,400,500,600,700,800]\n",
    "    criterion = [\"mae\",\"mse\"]\n",
    "    max_features = [\"None\",\"log2\",\"sqrt\",\"auto\"]\n",
    "    max_leaf_nodes = [\"None\"]\n",
    "    min_samples_split = [2,3,4,5,6,7,8]\n",
    "    min_samples_leaf = [1]\n",
    "    random_state = 17\n",
    "    n_jobs = -1\n",
    "    all_nb = len(n_estimators) * len(criterion) * len(max_features) * len(min_samples_leaf) * len(min_samples_split)\n",
    "    num = 1\n",
    "    if(os.path.exists(path2)):\n",
    "        data = pd.read_csv(path2,header=None)\n",
    "        nums = int(data.values[-1,0])\n",
    "        for n in n_estimators:\n",
    "            for mf in max_features:\n",
    "                for mi in min_samples_split:\n",
    "                    for ms in min_samples_leaf:\n",
    "                        for ml in max_leaf_nodes:\n",
    "                            for c in criterion:\n",
    "                                if(nums<num):\n",
    "                                    print(\"start....{}/{}\".format(num,all_nb))\n",
    "                                    if(ml==\"None\" and mf!= \"None\"):\n",
    "                                        ext = ExtraTreesRegressor(n_jobs=n_jobs,random_state=random_state,n_estimators=n,max_features=mf,min_samples_leaf=ms,min_samples_split=mi,criterion=c)\n",
    "                                    elif(ml!=\"None\" and mf==\"None\"):\n",
    "                                        ext = ExtraTreesRegressor(n_jobs=n_jobs,random_state=random_state,n_estimators=n,max_leaf_nodes=ml,min_samples_leaf=ms,min_samples_split=mi,criterion=c)\n",
    "                                    elif(ml==\"None\" and mf==\"None\"):\n",
    "                                        ext = ExtraTreesRegressor(n_jobs=n_jobs,random_state=random_state,n_estimators=n,min_samples_leaf=ms,min_samples_split=mi,criterion=c)\n",
    "                                    else:\n",
    "                                        ext = ExtraTreesRegressor(n_jobs=n_jobs,random_state=random_state,n_estimators=n,max_features=mf,max_leaf_nodes=ml,min_samples_leaf=ms,min_samples_split=mi,criterion=c)\n",
    "                                    ext.fit(train_x_s,train_y.ravel())\n",
    "                                    pred_test = ext.predict(test_x_s)\n",
    "                                    pred_test = pred_test.reshape(-1,1)\n",
    "                                    sample_n = pred_test.shape[0]\n",
    "                                    feature_n = test_x_s.shape[1]\n",
    "                                    mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y,pred_test,sample_n,feature_n)\n",
    "                                    all_m = [num,mse,rmse,mae,r2,mad,mape,r2_adjusted]\n",
    "                                    all_p = [num,n,mf,mi,ms,ml,c]\n",
    "                                    print(all_m)\n",
    "                                    with open(path2,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                        f = csv.writer(f)\n",
    "                                        f.writerow(all_m)\n",
    "                                    with open(path3,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                        f = csv.writer(f)\n",
    "                                        f.writerow(all_p)\n",
    "                                    print(\"end....\",num)\n",
    "                                    num = num+1\n",
    "                                    print(\"--------------------------------\")  \n",
    "                                else:\n",
    "                                    num = num+1\n",
    "    else:\n",
    "        for n in n_estimators:\n",
    "            for mf in max_features:\n",
    "                for mi in min_samples_split:\n",
    "                    for ms in min_samples_leaf:\n",
    "                        for ml in max_leaf_nodes:\n",
    "                            for c in criterion:\n",
    "                                print(\"start....{}/{}\".format(num,all_nb))\n",
    "                                if(ml==\"None\" and mf!= \"None\"):\n",
    "                                    ext = ExtraTreesRegressor(n_jobs=n_jobs,random_state=random_state,n_estimators=n,max_features=mf,min_samples_leaf=ms,min_samples_split=mi,criterion=c)\n",
    "                                elif(ml!=\"None\" and mf==\"None\"):\n",
    "                                    ext = ExtraTreesRegressor(n_jobs=n_jobs,random_state=random_state,n_estimators=n,max_leaf_nodes=ml,min_samples_leaf=ms,min_samples_split=mi,criterion=c)\n",
    "                                elif(ml==\"None\" and mf==\"None\"):\n",
    "                                    ext = ExtraTreesRegressor(n_jobs=n_jobs,random_state=random_state,n_estimators=n,min_samples_leaf=ms,min_samples_split=mi,criterion=c)\n",
    "                                else:\n",
    "                                    ext = ExtraTreesRegressor(n_jobs=n_jobs,random_state=random_state,n_estimators=n,max_features=mf,max_leaf_nodes=ml,min_samples_leaf=ms,min_samples_split=mi,criterion=c)\n",
    "                                ext.fit(train_x_s,train_y.ravel())\n",
    "                                pred_test = ext.predict(test_x_s)\n",
    "                                pred_test = pred_test.reshape(-1,1)\n",
    "                                sample_n = pred_test.shape[0]\n",
    "                                feature_n = test_x_s.shape[1]\n",
    "                                mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y,pred_test,sample_n,feature_n)\n",
    "                                all_m = [num,mse,rmse,mae,r2,mad,mape,r2_adjusted]\n",
    "                                all_p = [num,n,mf,mi,ms,ml,c]\n",
    "                                print(all_m)\n",
    "                                with open(path2,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                    f = csv.writer(f)\n",
    "                                    f.writerow(all_m)\n",
    "                                with open(path3,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                    f = csv.writer(f)\n",
    "                                    f.writerow(all_p)\n",
    "                                print(\"end....\",num)\n",
    "                                num = num+1\n",
    "                                print(\"--------------------------------\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 500\n",
    "mi = 2 \n",
    "ms = 1\n",
    "c = \"mse\"\n",
    "\n",
    "ext = ExtraTreesRegressor(n_jobs=-1,random_state=17,n_estimators=n,min_samples_leaf=ms,min_samples_split=mi,criterion=c)\n",
    "ext.fit(train_X_n,train_y_n)\n",
    "pred_test = ext.predict(test_X_n)\n",
    "pred_test = pred_test.reshape(-1,1)\n",
    "sample_n = pred_test.shape[0]\n",
    "feature_n = test_X_n.shape[1]\n",
    "mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y_n,pred_test,sample_n,feature_n)\n",
    "plt_line(\"ET\",pred_test,test_y_n,\"plt\")\n",
    "all_assess.append([\"ET\",mse,rmse,mae,r2,mad,mape,r2_adjusted])\n",
    "all_pre = all_pre + pred_test.reshape(1,-1)[0].tolist()\n",
    "all_mo = all_mo + [\"ET\" for n in range(sample_n)]\n",
    "pred[\"ET\"] = pd.DataFrame(pred_test.reshape(-1,1))\n",
    "# Seq[\"ET\"] = ext.predict(train_X_n).reshape(1,-1)[0].tolist() + pred_test.reshape(1,-1)[0].tolist()\n",
    "Seq[\"ET\"] = train_y_n.reshape(1,-1)[0].tolist() + pred_test.reshape(1,-1)[0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def adjust_xgb(train_x_s,test_x_s,train_y,test_y,name):\n",
    "    path2 = \"./results/XGB_\" + name + \"_\" + \"assess.csv\"\n",
    "    path3 = \"./results/XGB_\" + name + \"_\" + \"parameter.csv\"\n",
    "    all_assessed_values = []\n",
    "    all_parameter = []\n",
    "    max_depth = [5,6,7,8,9]\n",
    "    n_estimator = [10,50,75,100,150,200,250,300,400,500,600,700]\n",
    "    learning_rate=[0.01,0.1,0.2,0.3,0.4,0.5]\n",
    "    subample = [0.5,0.7,0.9,1]\n",
    "    gamma = [0.01,1,5,]\n",
    "    reg_lambda = [0.01,1]\n",
    "    reg_alpha = [0.01,1]\n",
    "    colsample_bytree = [0.8,0.9,1]\n",
    "    all_nb = len(max_depth)*len(n_estimator)*len(learning_rate)*len(subample)*len(gamma)*len(reg_alpha)*len(reg_lambda)*len(colsample_bytree)\n",
    "    num=1\n",
    "    if(os.path.exists(path2)):\n",
    "        data = pd.read_csv(path2,header=None)\n",
    "        nums = int(data.values[-1,0])\n",
    "        for ma in max_depth:\n",
    "            for s in subample:\n",
    "                for l in learning_rate:\n",
    "                    for g in gamma:\n",
    "                        for rl in reg_lambda:\n",
    "                            for ra in reg_alpha:\n",
    "                                for c in colsample_bytree:\n",
    "                                    for n in n_estimator:\n",
    "                                        if(nums<num):\n",
    "                                            try:\n",
    "                                                print(\"train...{}/{}\".format(num,all_nb))\n",
    "                                                xg = xgb.XGBRegressor(n_estimator=n,colsample_bytree=c,reg_lambda=rl,reg_alpha=ra,subample=s,gamma=g,max_depth=ma,learning_rate=l,subsample=s)\n",
    "                                                xg.fit(train_x_s,train_y)\n",
    "                                                pred_test = xg.predict(test_x_s)\n",
    "                                                pred_test = pred_test.reshape(-1,1)\n",
    "                                                sample_n = pred_test.shape[0]\n",
    "                                                feature_n = test_x_s.shape[1]\n",
    "                                                mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y,pred_test,sample_n,feature_n)\n",
    "                                                all_m = [num,mse,rmse,mae,r2,mad,mape,r2_adjusted]\n",
    "                                                all_p = [num,ma,s,l,g,rl,ra,c,n]\n",
    "                                                with open(path2,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                                    f = csv.writer(f)\n",
    "                                                    f.writerow(all_m)\n",
    "                                                with open(path3,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                                    f = csv.writer(f)\n",
    "                                                    f.writerow(all_p)\n",
    "                                                print(\"end....\",num)\n",
    "                                                num = num+1\n",
    "                                                print(\"--------------------------------\") \n",
    "                                            except:\n",
    "                                                print(\"error\")\n",
    "                                        else:\n",
    "                                            num = num+1\n",
    "    else:\n",
    "        for ma in max_depth:\n",
    "            for s in subample:\n",
    "                for l in learning_rate:\n",
    "                    for g in gamma:\n",
    "                        for rl in reg_lambda:\n",
    "                            for ra in reg_alpha:\n",
    "                                for c in colsample_bytree:\n",
    "                                    for n in n_estimator:\n",
    "                                        try:\n",
    "                                            print(\"train...{}/{}\".format(num,all_nb))\n",
    "                                            xg = xgb.XGBRegressor(n_estimator=n,colsample_bytree=c,reg_lambda=rl,reg_alpha=ra,subample=s,gamma=g,max_depth=ma,learning_rate=l,subsample=s)\n",
    "                                            xg.fit(train_x_s,train_y)\n",
    "                                            pred_test = xg.predict(test_x_s)\n",
    "                                            pred_test = pred_test.reshape(-1,1)\n",
    "                                            sample_n = pred_test.shape[0]\n",
    "                                            feature_n = test_x_s.shape[1]\n",
    "                                            mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y,pred_test,sample_n,feature_n)\n",
    "                                            all_m = [num,mse,rmse,mae,r2,mad,mape,r2_adjusted]\n",
    "                                            all_p = [num,ma,s,l,g,rl,ra,c,n]\n",
    "                                            with open(path2,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                                f = csv.writer(f)\n",
    "                                                f.writerow(all_m)\n",
    "                                            with open(path3,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                                f = csv.writer(f)\n",
    "                                                f.writerow(all_p)\n",
    "                                            print(\"end....\",num)\n",
    "                                            num = num+1\n",
    "                                            print(\"--------------------------------\") \n",
    "                                        except:\n",
    "                                            print(\"error\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma = 5\n",
    "s = 0.7\n",
    "l = 0.2\n",
    "g = 5\n",
    "rl = 1\n",
    "ra = 0.01\n",
    "c = 0.8\n",
    "n = 10\n",
    "\n",
    "xg = xgb.XGBRegressor(n_estimator=n,colsample_bytree=c,reg_lambda=rl,reg_alpha=ra,subample=s,gamma=g,max_depth=ma,learning_rate=l,subsample=s)\n",
    "xg.fit(train_X_n,train_y_n)\n",
    "pred_test = xg.predict(test_X_n)\n",
    "pred_test = pred_test.reshape(-1,1)\n",
    "sample_n = pred_test.shape[0]\n",
    "feature_n = test_X_n.shape[1]\n",
    "mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y_n,pred_test,sample_n,feature_n)\n",
    "plt_line(\"XGB\",pred_test,test_y_n,\"plt\")\n",
    "all_assess.append([\"XGB\",mse,rmse,mae,r2,mad,mape,r2_adjusted])\n",
    "all_pre = all_pre + pred_test.reshape(1,-1)[0].tolist()\n",
    "all_mo = all_mo + [\"XGB\" for n in range(sample_n)]\n",
    "pred[\"XGB\"] = pd.DataFrame(pred_test.reshape(-1,1))\n",
    "# Seq[\"XGB\"] = xg.predict(train_X_n).reshape(1,-1)[0].tolist() + pred_test.reshape(1,-1)[0].tolist()\n",
    "Seq[\"XGB\"] = train_y_n.reshape(1,-1)[0].tolist() + pred_test.reshape(1,-1)[0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def adjust_lgb(train_x_s,test_x_s,train_y,test_y,name):\n",
    "    path2 = \"./results/LGB\" + name + \"_\" + \"assess.csv\"\n",
    "    path3 = \"./results/LGB\" + name + \"_\" + \"parameter.csv\"\n",
    "    all_assessed_values = []\n",
    "    all_parameter = []\n",
    "    depth = [5,6,7,8,9,10]\n",
    "    learning_rate = [0.01,0.03,0.05,0.07,0.09,0.1,0.15,0.2]\n",
    "    n_estimators = [100,200,300,400,500,600,700,800,900,1000,1200,1500]\n",
    "    feature_fraction = [1,0.9,0.8,0.7]\n",
    "    lambda_l1 = [0,0.01,0.5,1]\n",
    "    lambda_l2 = [0,0.01,0.5,1]\n",
    "    all_nb = len(depth)*len(learning_rate)*len(n_estimators)*len(feature_fraction)*len(lambda_l1)*len(lambda_l2)\n",
    "    num = 1\n",
    "    if(os.path.exists(path2)):\n",
    "        data = pd.read_csv(path2,header=None)\n",
    "        nums = int(data.values[-1,0])\n",
    "        for d in depth:\n",
    "            for l in learning_rate:\n",
    "                for n in n_estimators:\n",
    "                    for l2 in lambda_l2:\n",
    "                        for l1 in lambda_l1:\n",
    "                            for f in feature_fraction:\n",
    "                                if(nums<num):\n",
    "                                    try:\n",
    "                                        print(\"train...{}/{}\".format(num,all_nb))\n",
    "                                        lgbr = lgb.LGBMRegressor(objective='regression',max_depth=d,learning_rate=l,n_estimators=n,lambda_l1=l1,lambda_l2=l2,feature_fraction=f)\n",
    "                                        lgbr.fit(train_x_s,train_y)\n",
    "                                        pred_test = lgbr.predict(test_x_s)\n",
    "                                        pred_test = pred_test.reshape(-1,1)\n",
    "                                        sample_n = pred_test.shape[0]\n",
    "                                        feature_n = test_x_s.shape[1]\n",
    "                                        mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y,pred_test,sample_n,feature_n)\n",
    "                                        all_m = [num,mse,rmse,mae,r2,mad,mape,r2_adjusted]\n",
    "                                        all_p = [num,d,l,n,l1,l2,f]\n",
    "                                        with open(path2,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                            f = csv.writer(f)\n",
    "                                            f.writerow(all_m)\n",
    "                                        with open(path3,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                            f = csv.writer(f)\n",
    "                                            f.writerow(all_p)\n",
    "                                        print(\"end....\",num)\n",
    "                                        num = num+1\n",
    "                                        print(\"--------------------------------\")   \n",
    "                                    except:\n",
    "                                        print(\"error\")\n",
    "                                else:\n",
    "                                    num = num+1\n",
    "    else:\n",
    "        for d in depth:\n",
    "            for l in learning_rate:\n",
    "                for n in n_estimators:\n",
    "                    for l2 in lambda_l2:\n",
    "                        for l1 in lambda_l1:\n",
    "                            for f in feature_fraction:\n",
    "                                try:\n",
    "                                    print(\"train...{}/{}\".format(num,all_nb))\n",
    "                                    lgbr = lgb.LGBMRegressor(objective='regression',max_depth=d,learning_rate=l,n_estimators=n,lambda_l1=l1,lambda_l2=l2,feature_fraction=f)\n",
    "                                    lgbr.fit(train_x_s,train_y)\n",
    "                                    pred_test = lgbr.predict(test_x_s)\n",
    "                                    pred_test = pred_test.reshape(-1,1)\n",
    "                                    sample_n = pred_test.shape[0]\n",
    "                                    feature_n = test_x_s.shape[1]\n",
    "                                    mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y,pred_test,sample_n,feature_n)\n",
    "                                    all_m = [num,mse,rmse,mae,r2,mad,mape,r2_adjusted]\n",
    "                                    all_p = [num,d,l,n,l1,l2,f]\n",
    "                                    with open(path2,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                        f = csv.writer(f)\n",
    "                                        f.writerow(all_m)\n",
    "                                    with open(path3,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                        f = csv.writer(f)\n",
    "                                        f.writerow(all_p)\n",
    "                                    print(\"end....\",num)\n",
    "                                    num = num+1\n",
    "                                    print(\"--------------------------------\")   \n",
    "                                except:\n",
    "                                    print(\"error\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 10\n",
    "l = 0.1\n",
    "n = 1500\n",
    "l1 = 0\n",
    "l2 = 0.5\n",
    "f = 0.7\n",
    "\n",
    "lgbr = lgb.LGBMRegressor(objective='regression',max_depth=d,learning_rate=l,n_estimators=n,lambda_l1=l1,lambda_l2=l2,feature_fraction=f)\n",
    "lgbr.fit(train_X_lg,train_y_lg)\n",
    "pred_test = lgbr.predict(test_X_lg)\n",
    "pred_test = 10**pred_test.reshape(-1,1)\n",
    "sample_n = pred_test.shape[0]\n",
    "feature_n = test_X_lg.shape[1]\n",
    "mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y_lg,pred_test,sample_n,feature_n)\n",
    "plt_line(\"LGB\",pred_test,test_y_lg,\"plt\")\n",
    "all_assess.append([\"LGB\",mse,rmse,mae,r2,mad,mape,r2_adjusted])\n",
    "all_pre = all_pre + pred_test.reshape(1,-1)[0].tolist()\n",
    "all_mo = all_mo + [\"LGB\" for n in range(sample_n)]\n",
    "pred[\"LGB\"] = pd.DataFrame(pred_test.reshape(-1,1))\n",
    "# Seq[\"LGB\"] = (10**lgbr.predict(train_X_lg).reshape(1,-1)[0]).tolist() + pred_test.reshape(1,-1)[0].tolist()\n",
    "Seq[\"LGB\"] = train_y_n.reshape(1,-1)[0].tolist() + pred_test.reshape(1,-1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost as cb\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def adjust_cat(train_x_s,test_x_s,train_y,test_y,name):\n",
    "    path2 = \"./results/CAT_\" + name + \"_\" + \"assess.csv\"\n",
    "    path3 = \"./results/CAT_\" + name + \"_\" + \"parameter.csv\"\n",
    "    all_assessed_values = []\n",
    "    all_parameter = []\n",
    "    depth=[5,6,7,8,9,10]\n",
    "    learning_rate=[0.001,0.01,0.03,0.05,0.07,0.09,0.1,0.2,0.3]\n",
    "    iterations = [1500,1400,1300,1200,1100,1000,900,800]\n",
    "    l2_leaf_reg = [0,1,2,3,4,5]\n",
    "    all_nb = len(depth)*len(learning_rate)*len(iterations)*len(l2_leaf_reg)\n",
    "    num=1\n",
    "    if(os.path.exists(path2)):\n",
    "        data = pd.read_csv(path2,header=None)\n",
    "        nums = int(data.values[-1,0])\n",
    "        for d in depth:\n",
    "            for l in learning_rate:\n",
    "                for i in iterations:\n",
    "                    for l2 in l2_leaf_reg:\n",
    "                        if(nums<num):\n",
    "                            try:\n",
    "                                print(\"train...{}/{}\".format(num,all_nb))\n",
    "                                cbr = cb.CatBoostRegressor(depth=d,learning_rate=l,iterations=i,l2_leaf_reg=l2,logging_level='Silent')\n",
    "                                cbr.fit(train_x_s,train_y)\n",
    "                                pred_test = cbr.predict(test_x_s)\n",
    "                                pred_test = pred_test.reshape(-1,1)\n",
    "                                sample_n = pred_test.shape[0]\n",
    "                                feature_n = test_x_s.shape[1]\n",
    "                                mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y,pred_test,sample_n,feature_n)\n",
    "                                all_m = [num,mse,rmse,mae,r2,mad,mape,r2_adjusted]\n",
    "                                all_p = [num,d,l,i,l2]\n",
    "                                print(all_m)\n",
    "                                with open(path2,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                    f = csv.writer(f)\n",
    "                                    f.writerow(all_m)\n",
    "                                with open(path3,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                    f = csv.writer(f)\n",
    "                                    f.writerow(all_p)\n",
    "                                print(\"end....\",num)\n",
    "                                num = num+1\n",
    "                                print(\"--------------------------------\")   \n",
    "                            except:\n",
    "                                print(\"error\")\n",
    "                        else:\n",
    "                            num = num+1\n",
    "    else:\n",
    "        for d in depth:\n",
    "            for l in learning_rate:\n",
    "                for i in iterations:\n",
    "                    for l2 in l2_leaf_reg:\n",
    "                        try:\n",
    "                            print(\"train...{}/{}\".format(num,all_nb))\n",
    "                            cbr = cb.CatBoostRegressor(depth=d,learning_rate=l,iterations=i,l2_leaf_reg=l2,logging_level='Silent')\n",
    "                            cbr.fit(train_x_s,train_y)\n",
    "                            pred_test = cbr.predict(test_x_s)\n",
    "                            pred_test = pred_test.reshape(-1,1)\n",
    "                            sample_n = pred_test.shape[0]\n",
    "                            feature_n = test_x_s.shape[1]\n",
    "                            mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y,pred_test,sample_n,feature_n)\n",
    "                            all_m = [num,mse,rmse,mae,r2,mad,mape,r2_adjusted]\n",
    "                            all_p = [num,d,l,i,l2]\n",
    "                            print(all_m)\n",
    "                            with open(path2,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                f = csv.writer(f)\n",
    "                                f.writerow(all_m)\n",
    "                            with open(path3,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                f = csv.writer(f)\n",
    "                                f.writerow(all_p)\n",
    "                            print(\"end....\",num)\n",
    "                            num = num+1\n",
    "                            print(\"--------------------------------\")   \n",
    "                        except:\n",
    "                            print(\"error\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 5\n",
    "l = 0.09\n",
    "i = 1500\n",
    "l2 = 5\n",
    "\n",
    "cbr = cb.CatBoostRegressor(depth=d,learning_rate=l,iterations=i,l2_leaf_reg=l2,logging_level='Silent')\n",
    "cbr.fit(train_X_n,train_y_n)\n",
    "pred_test = cbr.predict(test_X_n)\n",
    "pred_test = pred_test.reshape(-1,1)\n",
    "sample_n = pred_test.shape[0]\n",
    "feature_n = test_X_n.shape[1]\n",
    "mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y_n,pred_test,sample_n,feature_n)\n",
    "plt_line(\"CAT\",pred_test,test_y_n,\"plt\")\n",
    "all_assess.append([\"CAT\",mse,rmse,mae,r2,mad,mape,r2_adjusted])\n",
    "all_pre = all_pre + pred_test.reshape(1,-1)[0].tolist()\n",
    "all_mo = all_mo + [\"CAT\" for n in range(sample_n)]\n",
    "pred[\"CAT\"] = pd.DataFrame(pred_test.reshape(-1,1))\n",
    "# Seq[\"CAT\"] = cbr.predict(train_X_n).reshape(1,-1)[0].tolist() + pred_test.reshape(1,-1)[0].tolist()\n",
    "Seq[\"CAT\"] = train_y_n.reshape(1,-1)[0].tolist() + pred_test.reshape(1,-1)[0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi # 查看显卡信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available() # 查看 GPU cuda 是否可用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.device_count() # 查看 GPU 数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.current_device() # 查看当前 GPU 索引号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.get_device_name(0) # 根据索引号查看GPU名字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.cuda(0) # 将CPU上的转换到GPU上，并指定使用索引号为0的GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.device # 查看数据 x 所在位置（CPU/GPU）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自动指定CPU或GPU的例子\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else\n",
    "'cpu')\n",
    "\n",
    "x = torch.tensor([1, 2, 3], device=device)\n",
    "x = torch.tensor([1, 2, 3]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1' #指定0、1显卡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.cpu() # x 从GPU转换为CPU的 x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.cuda() # 模型net使用cuda加速，注意输入的数据也要使用cuda加速"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.nn.DataParallel(net) # 多GPU模型的cuda加速"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.version.cuda) # 查看版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() # 清除显存缓存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.randn(3,4,5)\n",
    "print(tensor.type())  # 数据类型\n",
    "print(tensor.size())  # 张量的shape，是个元组\n",
    "print(tensor.dim())   # 维度的数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置默认类型，pytorch中的FloatTensor远远快于DoubleTensor\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "\n",
    "# 类型转换\n",
    "tensor = tensor.cuda()\n",
    "tensor = tensor.cpu()\n",
    "tensor = tensor.float()\n",
    "tensor = tensor.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.Tensor与np.ndarray转换\n",
    "darray = tensor.cpu().numpy()\n",
    "tensor = torch.from_numpy(ndarray).float()\n",
    "tensor = torch.from_numpy(ndarray.copy()).float() # If ndarray has negative stride."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从只包含一个元素的张量中提取值\n",
    "value = torch.rand(1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = tensor[torch.randperm(tensor.size(0))]  # 打乱第一个维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 张量拼接\n",
    "'''\n",
    "注意torch.cat和torch.stack的区别在于torch.cat沿着给定的维度拼接，\n",
    "而torch.stack会新增一维。例如当参数是3个10x5的张量，torch.cat的结果是30x5的张量，\n",
    "而torch.stack的结果是3x10x5的张量。\n",
    "'''\n",
    "tensor = torch.cat(list_of_tensors, dim=0)\n",
    "tensor = torch.stack(list_of_tensors, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算模型整体参数\n",
    "num_parameters = sum(torch.numel(parameter) for parameter in model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() # 模型测试\n",
    "model.train() # 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Dropout(p=0.5) # p指定丢弃的概率 train = (1-p)*train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Init net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化权重\n",
    "def weight_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "#         nn.init.normal_(m.weight,mean=0,std=1) 正太分布随机初始化\n",
    "        nn.init.xavier_normal_(m.weight) # Xavier权重初始化\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "\n",
    "# 自定义初始化 ， 只改变值，不涉及记录梯度\n",
    "def normal_(tensor, mean=0, std=1):\n",
    "    with torch.no_grad():\n",
    "        return tensor.normal_(mean, std)\n",
    "        \n",
    "net.apply(weight_init) #使用方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意 model.modules() 和 model.children() 的区别：model.modules() 会迭代地遍历模型的所有子层，\n",
    "# 而 model.children() 只会遍历模型下的一层\n",
    "# Common practise for initialization.\n",
    "for layer in model.modules():\n",
    "    if isinstance(layer, torch.nn.Conv2d):\n",
    "        torch.nn.init.kaiming_normal_(layer.weight, mode='fan_out',\n",
    "                                      nonlinearity='relu')\n",
    "        if layer.bias is not None:\n",
    "            torch.nn.init.constant_(layer.bias, val=0.0)\n",
    "    elif isinstance(layer, torch.nn.BatchNorm2d):\n",
    "        torch.nn.init.constant_(layer.weight, val=1.0)\n",
    "        torch.nn.init.constant_(layer.bias, val=0.0)\n",
    "    elif isinstance(layer, torch.nn.Linear):\n",
    "        torch.nn.init.xavier_normal_(layer.weight)\n",
    "        if layer.bias is not None:\n",
    "            torch.nn.init.constant_(layer.bias, val=0.0)\n",
    "\n",
    "# Initialization with given tensor.\n",
    "layer.weight = torch.nn.Parameter(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLoss(torch.nn.Moudle):\n",
    "    def __init__(self):\n",
    "        super(MyLoss, self).__init__()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        loss = torch.mean((x - y) ** 2)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不对偏置项进行权重衰减（weight decay）\n",
    "# weight decay相当于l2正则\n",
    "\n",
    "optimizer = torch.optim.SGD(parameters, lr=1e-2, momentum=0.9, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 梯度裁剪（gradient clipping）\n",
    "\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_best = current_acc > best_acc\n",
    "best_acc = max(current_acc, best_acc)\n",
    "checkpoint = {\n",
    "    'best_acc': best_acc,\n",
    "    'epoch': epoch + 1,\n",
    "    'model': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "}\n",
    "model_path = os.path.join('model', 'checkpoint.tar')\n",
    "best_model_path = os.path.join('model', 'best_checkpoint.tar')\n",
    "torch.save(checkpoint, model_path)\n",
    "if is_best:\n",
    "    shutil.copy(model_path, best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "...\n",
    "\n",
    "# set flags / seeds\n",
    "torch.backends.cudnn.benchmark = True\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed(1)\n",
    "...\n",
    "\n",
    "# Start with main code\n",
    "if __name__ ==  __main__ :\n",
    "    # argparse for additional flags for experiment\n",
    "    parser = argparse.ArgumentParser(description=\"Train a network for ...\")\n",
    "    ...\n",
    "    opt = parser.parse_args() \n",
    "\n",
    "    # add code for datasets (we always use train and validation/ test set)\n",
    "    data_transforms = transforms.Compose([\n",
    "        transforms.Resize((opt.img_size, opt.img_size)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.ImageFolder(\n",
    "        root=os.path.join(opt.path_to_data, \"train\"),\n",
    "        transform=data_transforms)\n",
    "    train_data_loader = data.DataLoader(train_dataset, ...)\n",
    "\n",
    "    test_dataset = datasets.ImageFolder(\n",
    "        root=os.path.join(opt.path_to_data, \"test\"),\n",
    "        transform=data_transforms)\n",
    "    test_data_loader = data.DataLoader(test_dataset ...)\n",
    "    ...\n",
    "\n",
    "    # instantiate network (which has been imported from *networks.py*)\n",
    "    net = MyNetwork(...)\n",
    "    ...\n",
    "\n",
    "    # create losses (criterion in pytorch)\n",
    "    criterion_L1 = torch.nn.L1Loss()\n",
    "    ...\n",
    "\n",
    "    # if running on GPU and we want to use cuda move model there\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    if use_cuda:\n",
    "        net = net.cuda()\n",
    "        ...\n",
    "\n",
    "    # create optimizers\n",
    "    optim = torch.optim.Adam(net.parameters(), lr=opt.lr)\n",
    "    ...\n",
    "\n",
    "    # load checkpoint if needed/ wanted\n",
    "    start_n_iter = 0\n",
    "    start_epoch = 0\n",
    "    if opt.resume:\n",
    "        ckpt = load_checkpoint(opt.path_to_checkpoint) # custom method for loading last checkpoint\n",
    "        net.load_state_dict(ckpt[ net ])\n",
    "        start_epoch = ckpt[ epoch ]\n",
    "        start_n_iter = ckpt[ n_iter ]\n",
    "        optim.load_state_dict(ckpt[ optim ])\n",
    "        print(\"last checkpoint restored\")\n",
    "        ...\n",
    "\n",
    "    # if we want to run experiment on multiple GPUs we move the models there\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    ...\n",
    "\n",
    "    # typically we use tensorboardX to keep track of experiments\n",
    "    writer = SummaryWriter(...)\n",
    "\n",
    "    # now we start the main loop\n",
    "    n_iter = start_n_iter\n",
    "    for epoch in range(start_epoch, opt.epochs):\n",
    "        # set models to train mode\n",
    "        net.train()\n",
    "        ...\n",
    "\n",
    "        # use prefetch_generator and tqdm for iterating through data\n",
    "        pbar = tqdm(enumerate(BackgroundGenerator(train_data_loader, ...)),\n",
    "                    total=len(train_data_loader))\n",
    "        start_time = time.time()\n",
    "\n",
    "        # for loop going through dataset\n",
    "        for i, data in pbar:\n",
    "            # data preparation\n",
    "            img, label = data\n",
    "            if use_cuda:\n",
    "                img = img.cuda()\n",
    "                label = label.cuda()\n",
    "            ...\n",
    "\n",
    "            # It s very good practice to keep track of preparation time and computation time using tqdm to find any issues in your dataloader\n",
    "            prepare_time = start_time-time.time()\n",
    "\n",
    "            # forward and backward pass\n",
    "            optim.zero_grad()\n",
    "            ...\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            ...\n",
    "\n",
    "            # udpate tensorboardX\n",
    "            writer.add_scalar(..., n_iter)\n",
    "            ...\n",
    "\n",
    "            # compute computation time and *compute_efficiency*\n",
    "            process_time = start_time-time.time()-prepare_time\n",
    "            pbar.set_description(\"Compute efficiency: {:.2f}, epoch: {}/{}:\".format(\n",
    "                process_time/(process_time+prepare_time), epoch, opt.epochs))\n",
    "            start_time = time.time()\n",
    "\n",
    "        # maybe do a test pass every x epochs\n",
    "        if epoch % x == x-1:\n",
    "            # bring models to evaluation mode\n",
    "            net.eval()\n",
    "            ...\n",
    "            #do some tests\n",
    "            pbar = tqdm(enumerate(BackgroundGenerator(test_data_loader, ...)),\n",
    "                    total=len(test_data_loader)) \n",
    "            for i, data in pbar:\n",
    "                ...\n",
    "\n",
    "            # save checkpoint if needed\n",
    "            ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import csv\n",
    "def adjust_mlp(train_x_s,test_x_s,train_y,test_y,name):\n",
    "    path2 = \"./results/MLP_\" + name + \"_\" + \"assess.csv\"\n",
    "    path3 = \"./results/MLP_\" + name + \"_\" + \"parameter.csv\"\n",
    "    all_assessed_values = []\n",
    "    all_parameter = []\n",
    "    max_iter = [5000,10000,15000,20000]\n",
    "    tol = [1e-3,2e-3,1e-4,1e-2]\n",
    "    learning_rate_init = [1e-2,1e-3,1e-4]\n",
    "    hidden_layer_sizes = list(combinations([64,32,16,8,4], 3))\n",
    "    all_nb = len(max_iter) * len(tol) * len(learning_rate_init) * len(hidden_layer_sizes)\n",
    "    num = 1\n",
    "    if(os.path.exists(path2)):\n",
    "        data = pd.read_csv(path2,header=None)\n",
    "        nums = int(data.values[-1,0])\n",
    "        for m in max_iter:\n",
    "            for t in tol:\n",
    "                for l in learning_rate_init:\n",
    "                    for hd in hidden_layer_sizes:\n",
    "                        if(nums<num):\n",
    "                            print(\"start....{}/{}\".format(num,all_nb))\n",
    "                            mlp = MLPRegressor(hidden_layer_sizes=hd, activation=\"relu\",\n",
    "                                             solver='adam', alpha=0.0001,\n",
    "                                             batch_size='auto', learning_rate=\"constant\",\n",
    "                                             learning_rate_init=l,\n",
    "                                             power_t=0.5, max_iter=m,tol=t)\n",
    "                            mlp.fit(train_X,train_y)\n",
    "                            pred_test = mlp.predict(test_x_s)\n",
    "                            pred_test = pred_test.reshape(-1,1)\n",
    "                            sample_n = pred_test.shape[0]\n",
    "                            feature_n = test_x_s.shape[1]\n",
    "                            mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y,pred_test,sample_n,feature_n)\n",
    "                            all_m = [num,mse,rmse,mae,r2,mad,mape,r2_adjusted]\n",
    "                            all_p = [num,m,t,l,hd]\n",
    "                            with open(path2,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                f = csv.writer(f)\n",
    "                                f.writerow(all_m)\n",
    "                            with open(path3,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                f = csv.writer(f)\n",
    "                                f.writerow(all_p)\n",
    "                            print(\"end....\",num)\n",
    "                            num = num+1\n",
    "                            print(\"--------------------------------\")  \n",
    "                        else:\n",
    "                            num = num+1\n",
    "    else:\n",
    "        for m in max_iter:\n",
    "            for t in tol:\n",
    "                for l in learning_rate_init:\n",
    "                    for hd in hidden_layer_sizes:\n",
    "                        print(\"start....{}/{}\".format(num,all_nb))\n",
    "                        mlp = MLPRegressor(hidden_layer_sizes=hd, activation=\"relu\",\n",
    "                                         solver='adam', alpha=0.0001,\n",
    "                                         batch_size='auto', learning_rate=\"constant\",\n",
    "                                         learning_rate_init=l,\n",
    "                                         power_t=0.5, max_iter=m,tol=t)\n",
    "                        mlp.fit(train_X,train_y)\n",
    "                        pred_test = mlp.predict(test_x_s)\n",
    "                        pred_test = pred_test.reshape(-1,1)\n",
    "                        sample_n = pred_test.shape[0]\n",
    "                        feature_n = test_x_s.shape[1]\n",
    "                        mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y,pred_test,sample_n,feature_n)\n",
    "                        all_m = [num,mse,rmse,mae,r2,mad,mape,r2_adjusted]\n",
    "                        all_p = [num,m,t,l,hd]\n",
    "                        with open(path2,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                            f = csv.writer(f)\n",
    "                            f.writerow(all_m)\n",
    "                        with open(path3,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                            f = csv.writer(f)\n",
    "                            f.writerow(all_p)\n",
    "                        print(\"end....\",num)\n",
    "                        num = num+1\n",
    "                        print(\"--------------------------------\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LinearNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNet(nn.Module):\n",
    "    def __init__(self, n_feature):\n",
    "        super(LinearNet, self).__init__()\n",
    "        self.linear = nn.Linear(n_feature, 1)\n",
    "    def forward(self, x):\n",
    "        y = self.linear(x)\n",
    "        return y\n",
    "    \n",
    "# 初始化权重\n",
    "def weight_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight) # Xavier权重初始化\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "# 学习率衰减\n",
    "# for param_group in optimizer.param_groups:\n",
    "#     param_group['lr'] *= 0.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = LinearNet(train_x.shape[1])# 定义模型\n",
    "print(net)\n",
    "net.apply(weight_init)# 初始化权重\n",
    "\n",
    "learning_rate = 0.01\n",
    "optimizer = optim.SGD(net.parameters(),learning_rate)\n",
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1000\n",
    "loss_1 = 0\n",
    "loss_2 = 0\n",
    "for epoch in range(1,num_epochs+1):\n",
    "    for X,y in data_iter:\n",
    "        output = net(X)\n",
    "        l = loss(output,y.view(-1,1))\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "    if(epoch % 50 == 0):\n",
    "        loss_2 = l.item()\n",
    "        print('epoch %d, loss: %f ,change : %f' % (epoch, l.item(),loss_2 - loss_1))\n",
    "        loss_1 = l.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lstm1(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size=1,num_layers=2,dropout=2):\n",
    "        super(Lstm1,self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size=input_size,hidden_size=hidden_size,num_layers=num_layers,dropout=dropout,batch_first=True)\n",
    "        self.reg = nn.Linear(hidden_size,output_size)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x,_ = self.lstm(x)\n",
    "        b,s,h = x.shape\n",
    "        x = x.view(b*s,h)\n",
    "        x = self.reg(x)\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lstm2(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size=1,num_layers=2,dropout=0):\n",
    "        super(Lstm2,self).__init__()\n",
    "        self.lstm2 = nn.LSTM(input_size=input_size,hidden_size=hidden_size,num_layers=num_layers,dropout=dropout,batch_first=True,bidirectional=True)\n",
    "        self.reg = nn.Linear(hidden_size*2,output_size)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x,_ = self.lstm(x)\n",
    "        b,s,h = x.shape\n",
    "        x = x.view(b*s,h)\n",
    "        x = self.reg(x)\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gru1(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size=1,num_layers=2,dropout=0):\n",
    "        super(Gru1,self).__init__()\n",
    "        self.gru = nn.GRU(input_size=input_size,hidden_size=hidden_size,num_layers=num_layers,dropout=dropout,batch_first=True,bidirectional=True)\n",
    "        self.reg = nn.Linear(hidden_size,output_size)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x,_ = self.gru(x)\n",
    "        s,b,h = x.shape\n",
    "        x = x.view(s*b,h)\n",
    "        x = self.reg(x)\n",
    "        x = self.relu(x)\n",
    "        x = x.view(s,b,-1)\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gru2(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size=1,num_layers=2,dropout=0):\n",
    "        super(Gru2,self).__init__()\n",
    "        self.gru = nn.GRU(input_size=input_size,hidden_size=hidden_size,num_layers=num_layers,dropout=dropout,batch_first=True,bidirectional=True)\n",
    "        self.reg = nn.Linear(hidden_size*2,output_size)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x,_ = self.gru(x)\n",
    "        s,b,h = x.shape\n",
    "        x = x.view(s*b,h)\n",
    "        x = self.reg(x)\n",
    "        x = self.relu(x)\n",
    "        x = x.view(s,b,-1)\n",
    "        return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self,input_size,output_size):\n",
    "        super(Generator,self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size,128)\n",
    "        self.layer2 = nn.Linear(128,64)\n",
    "        self.layer3 = nn.Linear(64,32)\n",
    "        self.layer4 = nn.Linear(32,output_size)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,input_size,output_size):\n",
    "        super(Discriminator,self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size,128)\n",
    "        self.layer2 = nn.Linear(128,64)\n",
    "        self.layer3 = nn.Linear(64,32)\n",
    "        self.layer4 = nn.Linear(32,output_size)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "D = Discriminator(8,1)\n",
    "G = Generator(1,8)\n",
    "criterion = nn.MSELoss()\n",
    "d_optimizer = torch.optim.Adam(D.parameters(), lr=0.0002)\n",
    "g_optimizer = torch.optim.Adam(G.parameters(), lr=0.0002)\n",
    "\n",
    "def reset_grad():\n",
    "    d_optimizer.zero_grad()\n",
    "    g_optimizer.zero_grad()\n",
    "\n",
    "for epoch in range(num_epochs): \n",
    "    # ================================================================== #\n",
    "    #                      训练判别模型                      #\n",
    "    # ================================================================== #\n",
    "    outputs = D(train_d_v)\n",
    "    d_loss_real = criterion(outputs,train_d_y_v)\n",
    "\n",
    "    # 计算fake损失\n",
    "    # 生成模型输入生成\n",
    "    g_s = G(train_g_y_v) \n",
    "    outputs = D(g_s)\n",
    "    d_loss_fake = criterion(outputs,train_g_v)\n",
    "    \n",
    "    # 反向传播和优化\n",
    "    d_loss = d_loss_real + d_loss_fake\n",
    "#     if((epoch+1) % 1000 ==0):\n",
    "#         for param_group in d_optimizer.param_groups:\n",
    "#             param_group[\"lr\"] *=0.1\n",
    "    d_loss.backward()\n",
    "    d_optimizer.step()\n",
    "    reset_grad()\n",
    "\n",
    "\n",
    "    # ================================================================== #\n",
    "    #                       训练生成模型                       #\n",
    "    # ================================================================== #\n",
    "\n",
    "    # 生成模型根据随机输入生成,然后判别模型进行判别\n",
    "    g_s = G(train_g_y_v)\n",
    "    outputs = D(g_s)\n",
    "\n",
    "    # 训练生成模型，使之最大化 f(D(G(z)) ，而不是最小化 lf(1-D(G(z)))\n",
    "    # 大致含义就是在训练初期，生成模型G还很菜，判别模型会拒绝高置信度的样本，因为这些样本与训练数据不同。\n",
    "    # 这样f(1-D(G(z)))就近乎饱和，梯度计算得到的值很小，不利于反向传播和训练。\n",
    "    # 换一种思路，通过计算最大化f(D(G(z))，就能够在训练初期提供较大的梯度值，利于快速收敛\n",
    "    g_loss = criterion(outputs,train_g_y_v)\n",
    "\n",
    "    # 反向传播和优化\n",
    "#     if((epoch+1) % 1000 ==0):\n",
    "#         for param_group in g_optimizer.param_groups:\n",
    "#             param_group[\"lr\"] *=0.1\n",
    "    g_loss.backward()\n",
    "    g_optimizer.step()\n",
    "    reset_grad()\n",
    "\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print('Epoch [{}/{}],, d_loss: {:.4f}, g_loss: {:.4f}' \n",
    "              .format(epoch+1, num_epochs,d_loss.item(), g_loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broad learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from numpy import random\n",
    "from sklearn import preprocessing\n",
    "import csv\n",
    "\n",
    "def tansig(x):\n",
    "    return (2/(1+np.exp(-2*x)))-1\n",
    "\n",
    "def pinv(A,reg):\n",
    "    return np.mat(reg*np.eye(A.shape[1])+A.T.dot(A)).I.dot(A.T)\n",
    "\n",
    "def shrinkage(a,b):\n",
    "    z = np.maximum(a - b, 0) - np.maximum( -a - b, 0)\n",
    "    return z\n",
    "\n",
    "def sparse_bls(A,b):\n",
    "    lam = 0.001\n",
    "    itrs = 50\n",
    "    AA = np.dot(A.T,A)\n",
    "    m = A.shape[1]\n",
    "    n = b.shape[1]\n",
    "    wk = np.zeros([m,n],dtype = 'double')\n",
    "    ok = np.zeros([m,n],dtype = 'double')\n",
    "    uk = np.zeros([m,n],dtype = 'double')\n",
    "    L1 = np.mat(AA + np.eye(m)).I\n",
    "    L2 = np.dot(np.dot(L1,A.T),b)\n",
    "    for i in range(itrs):\n",
    "        tempc = ok - uk\n",
    "        ck =  L2 + np.dot(L1,tempc)\n",
    "        ok = shrinkage(ck + uk, lam)\n",
    "        uk += ck - ok\n",
    "        wk = ok\n",
    "    return wk\n",
    "\n",
    "def bls_regression(train_x,train_y,test_x,test_y,s,C,NumFea,NumWin,NumEnhan):\n",
    "    u = 0\n",
    "    WF = list()\n",
    "    for i in range(NumWin):\n",
    "        random.seed(i+u)\n",
    "        WeightFea=2*random.randn(train_x.shape[1]+1,NumFea)-1\n",
    "        WF.append(WeightFea)\n",
    "    WeightEnhan=2*random.randn(NumWin*NumFea+1,NumEnhan)-1\n",
    "    time_start = time.time()\n",
    "    H1 = np.hstack([train_x, 0.1 * np.ones([train_x.shape[0],1])])\n",
    "    y = np.zeros([train_x.shape[0],NumWin*NumFea])\n",
    "    WFSparse = list()\n",
    "    distOfMaxAndMin = np.zeros(NumWin)\n",
    "    meanOfEachWindow = np.zeros(NumWin)\n",
    "    for i in range(NumWin):\n",
    "        WeightFea = WF[i]\n",
    "        A1 = H1.dot(WeightFea)        \n",
    "        scaler1 = preprocessing.MinMaxScaler(feature_range=(-1, 1)).fit(A1)\n",
    "        A1 = scaler1.transform(A1)\n",
    "        WeightFeaSparse  = sparse_bls(A1,H1).T\n",
    "        WFSparse.append(WeightFeaSparse)\n",
    "    \n",
    "        T1 = H1.dot(WeightFeaSparse)\n",
    "        meanOfEachWindow[i] = T1.mean()\n",
    "        distOfMaxAndMin[i] = T1.max() - T1.min()\n",
    "        T1 = (T1 - meanOfEachWindow[i])/distOfMaxAndMin[i] \n",
    "        y[:,NumFea*i:NumFea*(i+1)] = T1\n",
    "\n",
    "    H2 = np.hstack([y,0.1 * np.ones([y.shape[0],1])])\n",
    "    T2 = H2.dot(WeightEnhan)\n",
    "    T2 = tansig(T2)\n",
    "    T3 = np.hstack([y,T2])\n",
    "    WeightTop = pinv(T3,C).dot(train_y)\n",
    "    NetoutTrain = T3.dot(WeightTop)\n",
    "\n",
    "    HH1 = np.hstack([test_x, 0.1 * np.ones([test_x.shape[0],1])])\n",
    "    yy1=np.zeros([test_x.shape[0],NumWin*NumFea])\n",
    "    for i in range(NumWin):\n",
    "        WeightFeaSparse = WFSparse[i]\n",
    "        TT1 = HH1.dot(WeightFeaSparse)\n",
    "        TT1  = (TT1 - meanOfEachWindow[i])/distOfMaxAndMin[i]   \n",
    "        yy1[:,NumFea*i:NumFea*(i+1)] = TT1\n",
    "\n",
    "    HH2 = np.hstack([yy1, 0.1 * np.ones([yy1.shape[0],1])])\n",
    "    TT2 = tansig(HH2.dot( WeightEnhan))\n",
    "    TT3 = np.hstack([yy1,TT2])\n",
    "    NetoutTest = TT3.dot(WeightTop)\n",
    "    return NetoutTest\n",
    "\n",
    "\n",
    "def adjust_bl(train_x_s,test_x_s,train_y,test_y,name):\n",
    "    path2 = \"./results/BLS_\" + name + \"_\" + \"assess.csv\"\n",
    "    path3 = \"./results/BLS_\" + name + \"_\" + \"parameter.csv\"\n",
    "    all_assessed_values = []\n",
    "    all_parameter = []\n",
    "    NumFea = [i for i in range(2,40,4)]\n",
    "    NumWin = [i for i in range(5,40,5)]\n",
    "    NumEnhan = [i for i in range(5,60,10)]\n",
    "    S = [0.4,0.6,0.8,1,1.2,4]\n",
    "    C = [2**-30,2**-10,2**-20,2**-40,1**-30]\n",
    "    all_nb = len(NumFea)*len(NumWin)*len(S)*len(C)*len(NumEnhan)\n",
    "    num=1\n",
    "    if(os.path.exists(path2)):\n",
    "        data = pd.read_csv(path2,header=None)\n",
    "        nums = int(data.values[-1,0])\n",
    "        for nf in NumFea:\n",
    "            for nw in NumWin:\n",
    "                for s in S:\n",
    "                    for c in C:\n",
    "                        for ne in NumEnhan:\n",
    "                            if(nums<num):\n",
    "                                print(\"train...{}/{}\".format(num,all_nb))\n",
    "                                pred_test = bls_regression(train_X, train_y, test_X, test_y, s=s, C=c, NumFea=nf, NumWin=nw, NumEnhan=ne)\n",
    "                                pred_test = pred_test.reshape(-1,1)\n",
    "                                sample_n = pred_test.shape[0]\n",
    "                                feature_n = test_x_s.shape[1]\n",
    "                                mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y,pred_test,sample_n,feature_n)\n",
    "                                all_m = [num,mse,rmse,mae,r2,mad,mape,r2_adjusted]\n",
    "                                all_p = [num,s,c,nf,nw,ne]\n",
    "                                with open(path2,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                    f = csv.writer(f)\n",
    "                                    f.writerow(all_m)\n",
    "                                with open(path3,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                    f = csv.writer(f)\n",
    "                                    f.writerow(all_p)\n",
    "                                print(\"end....\",num)\n",
    "                                num = num+1\n",
    "                                print(\"--------------------------------\")   \n",
    "                            else:\n",
    "                                num = num+1\n",
    "    else:\n",
    "        for nf in NumFea:\n",
    "            for nw in NumWin:\n",
    "                for s in S:\n",
    "                    for c in C:\n",
    "                        for ne in NumEnhan:\n",
    "                            print(\"train...{}/{}\".format(num,all_nb))\n",
    "                            pred_test = bls_regression(train_X, train_y, test_X, test_y, s=s, C=c, NumFea=nf, NumWin=nw, NumEnhan=ne)\n",
    "                            pred_test = pred_test.reshape(-1,1)\n",
    "                            sample_n = pred_test.shape[0]\n",
    "                            feature_n = test_x_s.shape[1]\n",
    "                            mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y,pred_test,sample_n,feature_n)\n",
    "                            all_m = [num,mse,rmse,mae,r2,mad,mape,r2_adjusted]\n",
    "                            all_p = [num,s,c,nf,nw,ne]\n",
    "                            with open(path2,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                f = csv.writer(f)\n",
    "                                f.writerow(all_m)\n",
    "                            with open(path3,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                f = csv.writer(f)\n",
    "                                f.writerow(all_p)\n",
    "                            print(\"end....\",num)\n",
    "                            num = num+1\n",
    "                            print(\"--------------------------------\")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLSregressor:\n",
    "    def __init__(self,s,C,NumFea,NumWin,NumEnhan):\n",
    "        self.s = s\n",
    "        self.C = C\n",
    "        self.NumFea = NumFea\n",
    "        self.NumEnhan = NumEnhan\n",
    "        self.NumWin = NumWin\n",
    "\n",
    "    def shrinkage(self,a,b):\n",
    "        z = np.maximum(a - b, 0) - np.maximum( -a - b, 0)\n",
    "        return z\n",
    "        \n",
    "    def tansig(self,x):\n",
    "        return (2/(1+np.exp(-2*x)))-1\n",
    "\n",
    "    def pinv(self,A,reg):\n",
    "        return np.mat(reg*np.eye(A.shape[1])+A.T.dot(A)).I.dot(A.T)\n",
    "    \n",
    "    def sparse_bls(self,A,b):\n",
    "        lam = 0.001\n",
    "        itrs = 50\n",
    "        AA = np.dot(A.T,A)\n",
    "        m = A.shape[1]\n",
    "        n = b.shape[1]\n",
    "        wk = np.zeros([m,n],dtype = 'double')\n",
    "        ok = np.zeros([m,n],dtype = 'double')\n",
    "        uk = np.zeros([m,n],dtype = 'double')\n",
    "        L1 = np.mat(AA + np.eye(m)).I\n",
    "        L2 = np.dot(np.dot(L1,A.T),b)\n",
    "        for i in range(itrs):\n",
    "            tempc = ok - uk\n",
    "            ck =  L2 + np.dot(L1,tempc)\n",
    "            ok = self.shrinkage(ck + uk, lam)\n",
    "            uk += ck - ok\n",
    "            wk = ok\n",
    "        return wk\n",
    "    \n",
    "    def fit(self,train_x,train_y):  \n",
    "        train_y = train_y.reshape(-1,1)\n",
    "        u = 0\n",
    "        WF = list()\n",
    "        for i in range(self.NumWin):\n",
    "            random.seed(i+u)\n",
    "            WeightFea=2*random.randn(train_x.shape[1]+1,self.NumFea)-1\n",
    "            WF.append(WeightFea)\n",
    "        random.seed(100)\n",
    "        WeightEnhan=2*random.randn(self.NumWin*self.NumFea+1,self.NumEnhan)-1\n",
    "        H1 = np.hstack([train_x, 0.1 * np.ones([train_x.shape[0],1])])\n",
    "        y = np.zeros([train_x.shape[0],self.NumWin*self.NumFea])\n",
    "        WFSparse = list()\n",
    "        distOfMaxAndMin = np.zeros(self.NumWin)\n",
    "        meanOfEachWindow = np.zeros(self.NumWin)\n",
    "        for i in range(self.NumWin):\n",
    "            WeightFea = WF[i]\n",
    "            A1 = H1.dot(WeightFea)        \n",
    "            scaler1 = preprocessing.MinMaxScaler(feature_range=(-1, 1)).fit(A1)\n",
    "            A1 = scaler1.transform(A1)\n",
    "            WeightFeaSparse  = self.sparse_bls(A1,H1).T\n",
    "            WFSparse.append(WeightFeaSparse)\n",
    "        \n",
    "            T1 = H1.dot(WeightFeaSparse)\n",
    "            meanOfEachWindow[i] = T1.mean()\n",
    "            distOfMaxAndMin[i] = T1.max() - T1.min()\n",
    "            T1 = (T1 - meanOfEachWindow[i])/distOfMaxAndMin[i] \n",
    "            y[:,self.NumFea*i:self.NumFea*(i+1)] = T1\n",
    "        H2 = np.hstack([y,0.1 * np.ones([y.shape[0],1])])\n",
    "        T2 = H2.dot(WeightEnhan)\n",
    "        T2 = self.tansig(T2)\n",
    "        T3 = np.hstack([y,T2])\n",
    "        WeightTop = self.pinv(T3,self.C).dot(train_y)\n",
    "        self.WeightTop = WeightTop\n",
    "        self.WFSparse = WFSparse\n",
    "        self.meanOfEachWindow = meanOfEachWindow\n",
    "        self.distOfMaxAndMin = distOfMaxAndMin\n",
    "        self.WeightEnhan = WeightEnhan\n",
    "        return self\n",
    "\n",
    "    def predict(self,test_x):\n",
    "        HH1 = np.hstack([test_x, 0.1 * np.ones([test_x.shape[0],1])])\n",
    "        yy1=np.zeros([test_x.shape[0],self.NumWin*self.NumFea])\n",
    "        for i in range(self.NumWin):\n",
    "            WeightFeaSparse = self.WFSparse[i]\n",
    "            TT1 = HH1.dot(WeightFeaSparse)\n",
    "            TT1  = (TT1 - self.meanOfEachWindow[i])/self.distOfMaxAndMin[i]   \n",
    "            yy1[:,self.NumFea*i:self.NumFea*(i+1)] = TT1\n",
    "        HH2 = np.hstack([yy1, 0.1 * np.ones([yy1.shape[0],1])])\n",
    "        TT2 = self.tansig(HH2.dot(self.WeightEnhan))\n",
    "        TT3 = np.hstack([yy1,TT2])\n",
    "        NetoutTest = TT3.dot(self.WeightTop)\n",
    "        NetoutTest = np.array(NetoutTest).reshape(1,-1)\n",
    "        return NetoutTest\n",
    "    \n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "            \n",
    "    def get_params(self,deep = False):\n",
    "        return {\n",
    "            's':self.s,\n",
    "            'C':self.C,\n",
    "            'NumFea':self.NumFea,\n",
    "            'NumWin':self.NumWin,\n",
    "            'NumEnhan':self.NumEnhan\n",
    "        }\n",
    "\n",
    "\n",
    "s = 1\n",
    "c = 2**-20\n",
    "nf = 10\n",
    "nw = 20\n",
    "ne = 35\n",
    "\n",
    "BLS = BLSregressor(s=s,C=c,NumFea=nf,NumWin=nw,NumEnhan=ne)\n",
    "BLS.fit(train_x_s,train_y_n)\n",
    "pred_test = BLS.predict(test_x_s)\n",
    "sample_n,feature_n = test_x_s.shape\n",
    "calculate(test_y_n,predict,sample_n,feature_n)\n",
    "mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y_n,pred_test,sample_n,feature_n)\n",
    "plt_line(\"BL\",pred_test,test_y_n,\"plt\")\n",
    "all_assess.append([\"BL\",mse,rmse,mae,r2,mad,mape,r2_adjusted])\n",
    "all_pre = all_pre + pred_test.reshape(1,-1)[0].tolist()\n",
    "all_mo = all_mo + [\"BL\" for n in range(sample_n)]\n",
    "pred[\"BL\"] = pd.DataFrame(pred_test.reshape(-1,1))\n",
    "# Seq[\"BL\"] = np.array(train_p).reshape(1,-1)[0].tolist() + pred_test.reshape(1,-1)[0].tolist()\n",
    "Seq[\"BL\"] = train_y_n.reshape(1,-1)[0].tolist() + pred_test.reshape(1,-1)[0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![57](./img/57.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主要是确定p，d，q三个参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![54](./img/54.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![55](./img/55.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![56](./img/56.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjust （p,d,q）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![58](./img/58.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "画图 acf 和 pacf 确定 p,q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "fig = plt.figure(figsize=(12,8))\n",
    " \n",
    "ax1 = fig.add_subplot(211)\n",
    "fig = sm.graphics.tsa.plot_acf(train, lags=20,ax=ax1)\n",
    "ax1.xaxis.set_ticks_position('bottom')\n",
    "fig.tight_layout()\n",
    " \n",
    "ax2 = fig.add_subplot(212)\n",
    "fig = sm.graphics.tsa.plot_pacf(train, lags=20, ax=ax2)\n",
    "ax2.xaxis.set_ticks_position('bottom')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![60](./img/60.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用BIC（Bayesian InformationCriterion） 或 AIC（Akaike Information Criterion） 确定 p,q\n",
    "\n",
    "两种方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一种\n",
    "\n",
    "#遍历，寻找适宜的参数\n",
    "import itertools\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    " \n",
    "p_min = 0\n",
    "d_min = 0\n",
    "q_min = 0\n",
    "p_max = 5\n",
    "d_max = 0\n",
    "q_max = 5\n",
    " \n",
    "# Initialize a DataFrame to store the results,，以BIC准则\n",
    "results_bic = pd.DataFrame(index=['AR{}'.format(i) for i in range(p_min,p_max+1)],\n",
    "                           columns=['MA{}'.format(i) for i in range(q_min,q_max+1)])\n",
    " \n",
    "for p,d,q in itertools.product(range(p_min,p_max+1),\n",
    "                               range(d_min,d_max+1),\n",
    "                               range(q_min,q_max+1)):\n",
    "    if p==0 and d==0 and q==0:\n",
    "        results_bic.loc['AR{}'.format(p), 'MA{}'.format(q)] = np.nan\n",
    "        continue\n",
    " \n",
    "    try:\n",
    "        model = sm.tsa.ARIMA(train, order=(p, d, q),\n",
    "                               #enforce_stationarity=False,\n",
    "                               #enforce_invertibility=False,\n",
    "                              )\n",
    "        results = model.fit()\n",
    "        results_bic.loc['AR{}'.format(p), 'MA{}'.format(q)] = results.bic\n",
    "    except:\n",
    "        continue\n",
    "results_bic = results_bic[results_bic.columns].astype(float)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax = sns.heatmap(results_bic,\n",
    "                 mask=results_bic.isnull(),\n",
    "                 ax=ax,\n",
    "                 annot=True,\n",
    "                 fmt='.2f',\n",
    "                 )\n",
    "ax.set_title('BIC')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![61](./img/61.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第二种\n",
    "train_results = sm.tsa.arma_order_select_ic(train, ic=['aic', 'bic'], trend='nc', max_ar=8, max_ma=8)\n",
    " \n",
    "print('AIC', train_results.aic_min_order)\n",
    "print('BIC', train_results.bic_min_order)\n",
    "\n",
    "#一般来说，BIC准则得到的ARMA模型的阶数较AIC的低。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型检验\n",
    "\n",
    "这里的模型检验主要有两个：\n",
    "\n",
    "1）检验参数估计的显著性（t检验）\n",
    "\n",
    "2）检验残差序列的随机性，即残差之间是独立的\n",
    "\n",
    "残差序列的随机性可以通过自相关函数法来检验，即做残差的自相关函数图："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.tsa.ARIMA(train, order=(1, 0, 0))\n",
    "results = model.fit()\n",
    "resid = results.resid #赋值\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "fig = sm.graphics.tsa.plot_acf(resid.values.squeeze(), lags=40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![62](./img/62.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预测主要有两个函数，一个是predict函数，一个是forecast函数，predict中进行预测的时间段必须在我们训练ARIMA模型的数据中，forecast则是对训练数据集末尾下一个时间段的值进行预估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.tsa.ARIMA(sub, order=(1, 0, 0))\n",
    "results = model.fit()\n",
    "predict_sunspots = results.predict(start=str('2014-04'),end=str('2014-05'),dynamic=False)\n",
    "print(predict_sunspots)\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax = sub.plot(ax=ax)\n",
    "predict_sunspots.plot(ax=ax)\n",
    "plt.show()\n",
    "\n",
    "results.forecast()[0] # 预估下一个值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![63](./img/63.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Math "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(train_X_n,train_y_n)\n",
    "pred_test = lr.predict(test_X_n)\n",
    "pred_test = pred_test.reshape(-1,1)\n",
    "sample_n = pred_test.shape[0]\n",
    "feature_n = test_X_n.shape[1]\n",
    "mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y_n,pred_test,sample_n,feature_n)\n",
    "plt_line(\"LR\",pred_test,test_y_n,\"plt\")\n",
    "all_assess.append([\"LR\",mse,rmse,mae,r2,mad,mape,r2_adjusted])\n",
    "all_pre = all_pre + pred_test.reshape(1,-1)[0].tolist()\n",
    "all_mo = all_mo + [\"LR\" for n in range(sample_n)]\n",
    "pred[\"LR\"] = pd.DataFrame(pred_test.reshape(-1,1))\n",
    "# Seq[\"LR\"] = lr.predict(train_X_n).reshape(1,-1)[0].tolist() + pred_test.reshape(1,-1)[0].tolist()\n",
    "Seq[\"LR\"] = train_y_n.reshape(1,-1)[0].tolist() + pred_test.reshape(1,-1)[0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge or combine models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging - BLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "like RF = bagging + tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from numpy import random\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "class BLSregressor:\n",
    "    def __init__(self,s,C,NumFea,NumWin,NumEnhan):\n",
    "        self.s = s\n",
    "        self.C = C\n",
    "        self.NumFea = NumFea\n",
    "        self.NumEnhan = NumEnhan\n",
    "        self.NumWin = NumWin\n",
    "\n",
    "    def shrinkage(self,a,b):\n",
    "        z = np.maximum(a - b, 0) - np.maximum( -a - b, 0)\n",
    "        return z\n",
    "        \n",
    "    def tansig(self,x):\n",
    "        return (2/(1+np.exp(-2*x)))-1\n",
    "\n",
    "    def pinv(self,A,reg):\n",
    "        return np.mat(reg*np.eye(A.shape[1])+A.T.dot(A)).I.dot(A.T)\n",
    "    \n",
    "    def sparse_bls(self,A,b):\n",
    "        lam = 0.001\n",
    "        itrs = 50\n",
    "        AA = np.dot(A.T,A)\n",
    "        m = A.shape[1]\n",
    "        n = b.shape[1]\n",
    "        wk = np.zeros([m,n],dtype = 'double')\n",
    "        ok = np.zeros([m,n],dtype = 'double')\n",
    "        uk = np.zeros([m,n],dtype = 'double')\n",
    "        L1 = np.mat(AA + np.eye(m)).I\n",
    "        L2 = np.dot(np.dot(L1,A.T),b)\n",
    "        for i in range(itrs):\n",
    "            tempc = ok - uk\n",
    "            ck =  L2 + np.dot(L1,tempc)\n",
    "            ok = self.shrinkage(ck + uk, lam)\n",
    "            uk += ck - ok\n",
    "            wk = ok\n",
    "        return wk\n",
    "    \n",
    "    def fit(self,train_x,train_y):  \n",
    "        train_y = train_y.reshape(-1,1)\n",
    "        u = 0\n",
    "        WF = list()\n",
    "        for i in range(self.NumWin):\n",
    "            random.seed(i+u)\n",
    "            WeightFea=2*random.randn(train_x.shape[1]+1,self.NumFea)-1\n",
    "            WF.append(WeightFea)\n",
    "        random.seed(100)\n",
    "        WeightEnhan=2*random.randn(self.NumWin*self.NumFea+1,self.NumEnhan)-1\n",
    "        H1 = np.hstack([train_x, 0.1 * np.ones([train_x.shape[0],1])])\n",
    "        y = np.zeros([train_x.shape[0],self.NumWin*self.NumFea])\n",
    "        WFSparse = list()\n",
    "        distOfMaxAndMin = np.zeros(self.NumWin)\n",
    "        meanOfEachWindow = np.zeros(self.NumWin)\n",
    "        for i in range(self.NumWin):\n",
    "            WeightFea = WF[i]\n",
    "            A1 = H1.dot(WeightFea)        \n",
    "            scaler1 = preprocessing.MinMaxScaler(feature_range=(-1, 1)).fit(A1)\n",
    "            A1 = scaler1.transform(A1)\n",
    "            WeightFeaSparse  = self.sparse_bls(A1,H1).T\n",
    "            WFSparse.append(WeightFeaSparse)\n",
    "        \n",
    "            T1 = H1.dot(WeightFeaSparse)\n",
    "            meanOfEachWindow[i] = T1.mean()\n",
    "            distOfMaxAndMin[i] = T1.max() - T1.min()\n",
    "            T1 = (T1 - meanOfEachWindow[i])/distOfMaxAndMin[i] \n",
    "            y[:,self.NumFea*i:self.NumFea*(i+1)] = T1\n",
    "        H2 = np.hstack([y,0.1 * np.ones([y.shape[0],1])])\n",
    "        T2 = H2.dot(WeightEnhan)\n",
    "        T2 = self.tansig(T2)\n",
    "        T3 = np.hstack([y,T2])\n",
    "        WeightTop = self.pinv(T3,self.C).dot(train_y)\n",
    "        self.WeightTop = WeightTop\n",
    "        self.WFSparse = WFSparse\n",
    "        self.meanOfEachWindow = meanOfEachWindow\n",
    "        self.distOfMaxAndMin = distOfMaxAndMin\n",
    "        self.WeightEnhan = WeightEnhan\n",
    "        return self\n",
    "\n",
    "    def predict(self,test_x):\n",
    "        HH1 = np.hstack([test_x, 0.1 * np.ones([test_x.shape[0],1])])\n",
    "        yy1=np.zeros([test_x.shape[0],self.NumWin*self.NumFea])\n",
    "        for i in range(self.NumWin):\n",
    "            WeightFeaSparse = self.WFSparse[i]\n",
    "            TT1 = HH1.dot(WeightFeaSparse)\n",
    "            TT1  = (TT1 - self.meanOfEachWindow[i])/self.distOfMaxAndMin[i]   \n",
    "            yy1[:,self.NumFea*i:self.NumFea*(i+1)] = TT1\n",
    "        HH2 = np.hstack([yy1, 0.1 * np.ones([yy1.shape[0],1])])\n",
    "        TT2 = self.tansig(HH2.dot(self.WeightEnhan))\n",
    "        TT3 = np.hstack([yy1,TT2])\n",
    "        NetoutTest = TT3.dot(self.WeightTop)\n",
    "        NetoutTest = np.array(NetoutTest).reshape(1,-1)\n",
    "        return NetoutTest\n",
    "    \n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "            \n",
    "    def get_params(self,deep = False):\n",
    "        return {\n",
    "            's':self.s,\n",
    "            'C':self.C,\n",
    "            'NumFea':self.NumFea,\n",
    "            'NumWin':self.NumWin,\n",
    "            'NumEnhan':self.NumEnhan\n",
    "        }\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "def adjust_bl_bagging(train_X,test_X,train_y,test_y,name):\n",
    "    path2 = \"./results/Bagging-BLS_\" + name + \"_\" + \"access.csv\"\n",
    "    path3 = \"./results/Bagging-BLS_\" + name + \"_\" + \"parameter.csv\"\n",
    "    s = 1.2\n",
    "    c = 2**-20\n",
    "    nf = 10\n",
    "    nw = 15\n",
    "    ne = 55\n",
    "    N = [10+(5*i) for i in range(200)]\n",
    "    MF = [0.7,0.8,0.9,1.0]\n",
    "    MS = [0.7,0.8,0.9,1.0]\n",
    "    num=1\n",
    "    all_len = len(N) * len(MF) * len(MS)\n",
    "    if(os.path.exists(path2)):\n",
    "        data = pd.read_csv(path2,header=None)\n",
    "        nums = int(data.values[-1,0])\n",
    "        for mf in MF:\n",
    "            for ma in MS:\n",
    "                for n in N:\n",
    "                    if(nums<num):\n",
    "                        print(\"train...{}/{}\".format(num,all_len))\n",
    "                        regr = BaggingRegressor(base_estimator=BLSregressor(s=s,C=c,NumFea=nf,NumWin=nw,NumEnhan=ne),n_estimators=n, random_state=17,max_samples=ms,max_features=mf).fit(train_X,train_y.ravel())\n",
    "                        pred_test = regr.predict(test_X)\n",
    "                        pred_test = pred_test.reshape(-1,1)\n",
    "                        sample_n = pred_test.shape[0]\n",
    "                        feature_n = test_X.shape[1]\n",
    "                        mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y,pred_test,sample_n,feature_n)\n",
    "                        all_m = [num,mse,rmse,mae,r2,mad,mape,r2_adjusted]\n",
    "                        all_p = [num,s,c,nf,nw,ne]\n",
    "                        print(all_m)\n",
    "                        with open(path2,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                            f = csv.writer(f)\n",
    "                            f.writerow(all_m)\n",
    "\n",
    "                        with open(path3,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                            f = csv.writer(f)\n",
    "                            f.writerow(all_p)\n",
    "\n",
    "                        print(\"end....\",num)\n",
    "                        num = num+1\n",
    "                        print(\"--------------------------------\") \n",
    "                    else:\n",
    "                        num = num+1\n",
    "    else:\n",
    "        for mf in MF:\n",
    "            for ma in MS:\n",
    "                for n in N:\n",
    "                    print(\"train...{}/{}\".format(num,all_len))\n",
    "                    regr = BaggingRegressor(base_estimator=BLSregressor(s=s,C=c,NumFea=nf,NumWin=nw,NumEnhan=ne),n_estimators=n, random_state=17,max_samples=ms,max_features=mf).fit(train_X,train_y.ravel())\n",
    "                    pred_test = regr.predict(test_X)\n",
    "                    pred_test = pred_test.reshape(-1,1)\n",
    "                    sample_n = pred_test.shape[0]\n",
    "                    feature_n = test_X.shape[1]\n",
    "                    mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y,pred_test,sample_n,feature_n)\n",
    "                    all_m = [num,mse,rmse,mae,r2,mad,mape,r2_adjusted]\n",
    "                    all_p = [num,s,c,nf,nw,ne]\n",
    "                    print(all_m)\n",
    "                    with open(path2,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                        f = csv.writer(f)\n",
    "                        f.writerow(all_m)\n",
    "\n",
    "                    with open(path3,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                        f = csv.writer(f)\n",
    "                        f.writerow(all_p)\n",
    "\n",
    "                    print(\"end....\",num)\n",
    "                    num = num+1\n",
    "                    print(\"--------------------------------\") \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "s = 1\n",
    "c = 2**-20\n",
    "nf = 10\n",
    "nw = 20\n",
    "ne = 35\n",
    "\n",
    "regr = BaggingRegressor(base_estimator=BLSregressor(s=s,C=c,NumFea=nf,NumWin=nw,NumEnhan=ne),n_estimators=10, random_state=0).fit(train_x_s,train_y_n.ravel())\n",
    "predict = regr.predict(test_x_s)\n",
    "sample_n,feature_n = test_x_s.shape\n",
    "calculate(test_y_n,predict,sample_n,feature_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaboost - BLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from numpy import random\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "import os\n",
    "\n",
    "class BLSregressor:\n",
    "    def __init__(self,s,C,NumFea,NumWin,NumEnhan):\n",
    "        self.s = s\n",
    "        self.C = C\n",
    "        self.NumFea = NumFea\n",
    "        self.NumEnhan = NumEnhan\n",
    "        self.NumWin = NumWin\n",
    "\n",
    "    def shrinkage(self,a,b):\n",
    "        z = np.maximum(a - b, 0) - np.maximum( -a - b, 0)\n",
    "        return z\n",
    "        \n",
    "    def tansig(self,x):\n",
    "        return (2/(1+np.exp(-2*x)))-1\n",
    "\n",
    "    def pinv(self,A,reg):\n",
    "        return np.mat(reg*np.eye(A.shape[1])+A.T.dot(A)).I.dot(A.T)\n",
    "    \n",
    "    def sparse_bls(self,A,b):\n",
    "        lam = 0.001\n",
    "        itrs = 50\n",
    "        AA = np.dot(A.T,A)\n",
    "        m = A.shape[1]\n",
    "        n = b.shape[1]\n",
    "        wk = np.zeros([m,n],dtype = 'double')\n",
    "        ok = np.zeros([m,n],dtype = 'double')\n",
    "        uk = np.zeros([m,n],dtype = 'double')\n",
    "        L1 = np.mat(AA + np.eye(m)).I\n",
    "        L2 = np.dot(np.dot(L1,A.T),b)\n",
    "        for i in range(itrs):\n",
    "            tempc = ok - uk\n",
    "            ck =  L2 + np.dot(L1,tempc)\n",
    "            ok = self.shrinkage(ck + uk, lam)\n",
    "            uk += ck - ok\n",
    "            wk = ok\n",
    "        return wk\n",
    "    \n",
    "    def fit(self,train_x,train_y):  \n",
    "        train_y = train_y.reshape(-1,1)\n",
    "        u = 0\n",
    "        WF = list()\n",
    "        for i in range(self.NumWin):\n",
    "            random.seed(i+u)\n",
    "            WeightFea=2*random.randn(train_x.shape[1]+1,self.NumFea)-1\n",
    "            WF.append(WeightFea)\n",
    "        WeightEnhan=2*random.randn(self.NumWin*self.NumFea+1,self.NumEnhan)-1\n",
    "        H1 = np.hstack([train_x, 0.1 * np.ones([train_x.shape[0],1])])\n",
    "        y = np.zeros([train_x.shape[0],self.NumWin*self.NumFea])\n",
    "        WFSparse = list()\n",
    "        distOfMaxAndMin = np.zeros(self.NumWin)\n",
    "        meanOfEachWindow = np.zeros(self.NumWin)\n",
    "        for i in range(self.NumWin):\n",
    "            WeightFea = WF[i]\n",
    "            A1 = H1.dot(WeightFea)        \n",
    "            scaler1 = preprocessing.MinMaxScaler(feature_range=(-1, 1)).fit(A1)\n",
    "            A1 = scaler1.transform(A1)\n",
    "            WeightFeaSparse  = self.sparse_bls(A1,H1).T\n",
    "            WFSparse.append(WeightFeaSparse)\n",
    "            T1 = H1.dot(WeightFeaSparse)\n",
    "            meanOfEachWindow[i] = T1.mean()\n",
    "            distOfMaxAndMin[i] = T1.max() - T1.min()\n",
    "            T1 = (T1 - meanOfEachWindow[i])/distOfMaxAndMin[i] \n",
    "            y[:,self.NumFea*i:self.NumFea*(i+1)] = T1\n",
    "        H2 = np.hstack([y,0.1 * np.ones([y.shape[0],1])])\n",
    "        T2 = H2.dot(WeightEnhan)\n",
    "        T2 = self.tansig(T2)\n",
    "        T3 = np.hstack([y,T2])\n",
    "        WeightTop = self.pinv(T3,self.C).dot(train_y)\n",
    "        self.WeightTop = WeightTop\n",
    "        self.WFSparse = WFSparse\n",
    "        self.meanOfEachWindow = meanOfEachWindow\n",
    "        self.distOfMaxAndMin = distOfMaxAndMin\n",
    "        self.WeightEnhan = WeightEnhan\n",
    "        return self\n",
    "\n",
    "    def predict(self,test_x):\n",
    "        HH1 = np.hstack([test_x, 0.1 * np.ones([test_x.shape[0],1])])\n",
    "        yy1=np.zeros([test_x.shape[0],self.NumWin*self.NumFea])\n",
    "        for i in range(self.NumWin):\n",
    "            WeightFeaSparse = self.WFSparse[i]\n",
    "            TT1 = HH1.dot(WeightFeaSparse)\n",
    "            TT1  = (TT1 - self.meanOfEachWindow[i])/self.distOfMaxAndMin[i]   \n",
    "            yy1[:,self.NumFea*i:self.NumFea*(i+1)] = TT1\n",
    "        HH2 = np.hstack([yy1, 0.1 * np.ones([yy1.shape[0],1])])\n",
    "        TT2 = self.tansig(HH2.dot(self.WeightEnhan))\n",
    "        TT3 = np.hstack([yy1,TT2])\n",
    "        NetoutTest = TT3.dot(self.WeightTop)\n",
    "        NetoutTest = np.array(NetoutTest).reshape(-1,1).ravel()\n",
    "        return NetoutTest\n",
    "    \n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "            \n",
    "    def get_params(self,deep = False):\n",
    "        return {\n",
    "            's':self.s,\n",
    "            'C':self.C,\n",
    "            'NumFea':self.NumFea,\n",
    "            'NumWin':self.NumWin,\n",
    "            'NumEnhan':self.NumEnhan\n",
    "        }\n",
    "                      \n",
    "        \n",
    "def adjust_bl_ada(train_X,test_X,train_y,test_y,name):\n",
    "    path2 = \"./Boost4/\" + name + \"_\" + \"access.csv\"\n",
    "    path3 = \"./Boost4/\" + name + \"_\" + \"parameter.csv\"\n",
    "    NumFea = [i for i in range(2,40,4)]\n",
    "    NumWin = [i for i in range(5,40,5)]\n",
    "    NumEnhan = [i for i in range(5,60,10)]\n",
    "    S = [0.4,0.6,0.8,1,1.2,4]\n",
    "    C = [2**-30,2**-10,2**-20,2**-40,1**-30]\n",
    "    n_estimators = [50,100,200,300,400,500,600]\n",
    "    learning_rate = [0.25,0.5,0.75,1]\n",
    "    loss = [\"linear\",\"square\"] # ,\"exponential\"\n",
    "    all_len = len(n_estimators) * len(learning_rate) * len(loss) * len(NumFea) * len(NumWin) * len(S) * len(C)\n",
    "    num=1\n",
    "    if(os.path.exists(path2)):\n",
    "        data = pd.read_csv(path2,header=None)\n",
    "        nums = int(data.values[-1,0])\n",
    "        for nf in NumFea:\n",
    "            for nw in NumWin:\n",
    "                for s in S:\n",
    "                    for c in C:\n",
    "                        for ne in NumEnhan:\n",
    "                            for n in n_estimators:\n",
    "                                for lr in learning_rate:\n",
    "                                    for lo in loss:\n",
    "                                        if(nums<num):\n",
    "                                            print(\"train...{}/{}\".format(num,all_len))\n",
    "                                            abrbl = AdaBoostRegressor(n_estimators=n,learning_rate=lr,loss=lo,base_estimator=BLSregressor(s=s,C=c,NumFea=nf,NumWin=nw,NumEnhan=ne)).fit(train_x_s,train_y_n.ravel())\n",
    "                                            pred_test = abrbl.predict(test_X)\n",
    "                                            pred_test = pred_test.reshape(-1,1)\n",
    "                                            sample_n = pred_test.shape[0]\n",
    "                                            feature_n = test_X.shape[1]\n",
    "                                            mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y,pred_test,sample_n,feature_n)\n",
    "                                            all_m = [num,mse,rmse,mae,r2,mad,mape,r2_adjusted]\n",
    "                                            all_p = [num,lo,lr,n,nf,nw,s,c,ne]\n",
    "                                            print(all_m)\n",
    "                                            with open(path2,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                                f = csv.writer(f)\n",
    "                                                f.writerow(all_m)\n",
    "                                            with open(path3,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                                f = csv.writer(f)\n",
    "                                                f.writerow(all_p)\n",
    "                                            print(\"end....\",num)\n",
    "                                            num = num+1\n",
    "                                            print(\"--------------------------------\")   \n",
    "                                        else:\n",
    "                                            num = num+1\n",
    "    else:\n",
    "        for nf in NumFea:\n",
    "            for nw in NumWin:\n",
    "                for s in S:\n",
    "                    for c in C:\n",
    "                        for ne in NumEnhan:\n",
    "                            for n in n_estimators:\n",
    "                                for lr in learning_rate:\n",
    "                                    for lo in loss:\n",
    "                                        print(\"train...{}/{}\".format(num,all_len))\n",
    "                                        abrbl = AdaBoostRegressor(n_estimators=n,learning_rate=lr,loss=lo,base_estimator=BLSregressor(s=s,C=c,NumFea=nf,NumWin=nw,NumEnhan=ne)).fit(train_x_s,train_y_n.ravel())\n",
    "                                        pred_test = abrbl.predict(test_X)\n",
    "                                        pred_test = pred_test.reshape(-1,1)\n",
    "                                        sample_n = pred_test.shape[0]\n",
    "                                        feature_n = test_X.shape[1]\n",
    "                                        mse,rmse,mae,r2,mad,mape,r2_adjusted = calculate(test_y,pred_test,sample_n,feature_n)\n",
    "                                        all_m = [num,mse,rmse,mae,r2,mad,mape,r2_adjusted]\n",
    "                                        all_p = [num,lo,lr,n,nf,nw,s,c,ne]\n",
    "                                        print(all_m)\n",
    "                                        with open(path2,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                            f = csv.writer(f)\n",
    "                                            f.writerow(all_m)\n",
    "                                        with open(path3,\"a\",encoding=\"utf-8\",newline=\"\")as f:\n",
    "                                            f = csv.writer(f)\n",
    "                                            f.writerow(all_p)\n",
    "                                        print(\"end....\",num)\n",
    "                                        num = num+1\n",
    "                                        print(\"--------------------------------\")   \n",
    "                                        \n",
    "                                        \n",
    "adjust_bl_ada(train_x_s,test_x_s,train_y_n,test_y_n,\"Ada-BLS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple display results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### assess 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_assess1 = pd.DataFrame(all_assess,columns=[\"Model\",\"MSE\",\"RMSE\",\"MAE\",\"R2\",\"MAD\",\"MAPE\",\"R2_Adjusted\"])\n",
    "\n",
    "path1 = \"./results/parameter1/\"\n",
    "mkdir(path1)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.barplot(x=\"Model\",y=\"MSE\",data=all_assess1)\n",
    "path = path1 + \"MSE.pdf\"\n",
    "plt.savefig(path)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.barplot(x=\"Model\",y=\"RMSE\",data=all_assess1)\n",
    "path = path1 + \"RMSE.pdf\"\n",
    "plt.savefig(path)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.barplot(x=\"Model\",y=\"MAE\",data=all_assess1)\n",
    "path = path1 + \"MAE.pdf\"\n",
    "plt.savefig(path)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.barplot(x=\"Model\",y=\"MAD\",data=all_assess1)\n",
    "path = path1 + \"MAD.pdf\"\n",
    "plt.savefig(path)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.barplot(x=\"Model\",y=\"MAPE\",data=all_assess1)\n",
    "path = path1 + \"MAPE.pdf\"\n",
    "plt.savefig(path)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.barplot(x=\"Model\",y=\"R2\",data=all_assess1)\n",
    "path = path1 + \"R2.pdf\"\n",
    "plt.savefig(path)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.barplot(x=\"Model\",y=\"R2_Adjusted\",data=all_assess1)\n",
    "path = path1 + \"R2_Adjusted.pdf\"\n",
    "plt.savefig(path)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![4](./img/4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### assess 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_assess2 = []\n",
    "pa = [\"PA\",\"MSE\",\"RMSE\",\"MAE\",\"R2\",\"MAD\",\",MAPE\",\"R2_Adjusted\"]\n",
    "for a in all_assess:\n",
    "    for p in range(len(pa)-1):\n",
    "        all_assess2.append([a[0],pa[p+1],a[p+1]])\n",
    "all_assess2 = pd.DataFrame(all_assess2,columns=[\"Model\",\"Paremeter\",\"Values\"])\n",
    "\n",
    "path1 = \"./results/parameter2/\"\n",
    "mkdir(path1)\n",
    "\n",
    "for n in np.unique(all_assess2[\"Paremeter\"]):\n",
    "    plt.figure(figsize=(8,8))\n",
    "    sns.barplot(x=\"Paremeter\",y=\"Values\",data=all_assess2[all_assess2[\"Paremeter\"]==n],hue=\"Model\")\n",
    "    path = path1 + n +\".pdf\"\n",
    "    plt.savefig(path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![5](./img/5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### assess 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_assess3 = pd.DataFrame(all_mo,columns=[\"Model\"])\n",
    "all_assess3[\"Predicted\"] = pd.DataFrame(all_pre)\n",
    "all_real = []\n",
    "for i in range(np.unique(all_assess3[\"Model\"]).shape[0]):\n",
    "    all_real = all_real + test_y_n.reshape(1,-1)[0].tolist()\n",
    "all_assess3[\"Real\"] = pd.DataFrame(all_real)\n",
    "\n",
    "if((all_assess3[\"Real\"].max())>(all_assess3[\"Predicted\"].max())):\n",
    "    all_max = all_assess3[\"Real\"].max()+10000\n",
    "else:\n",
    "    all_max = all_assess3[\"Predicted\"].max()+10000\n",
    "\n",
    "path1 = \"./results/\"\n",
    "mkdir(path1)\n",
    "    \n",
    "path = path1 + \"all_pre_sca.pdf\"\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.relplot(x=\"Real\", y=\"Predicted\", data = all_assess3,hue=\"Model\")\n",
    "plt.plot([0,all_max],[0,all_max])\n",
    "plt.title(\"Predict result\")\n",
    "plt.savefig(path,bbox_inches = 'tight')\n",
    "plt.show()\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "path1 = \"./results/scat/\"\n",
    "mkdir(path1)\n",
    "\n",
    "pred[\"Real\"] = pd.DataFrame(test_y_n)\n",
    "for n in pred.columns[2:-1]:\n",
    "    if((pred[\"Real\"].max())>(pred[n].max())):\n",
    "        one_max = pred[\"Real\"].max()+10000\n",
    "    else:\n",
    "        one_max = pred[n].max()+10000\n",
    "    plt.figure(figsize=(8,8))\n",
    "    sns.relplot(x=\"Real\",y=n,data = pred)\n",
    "    plt.plot([0,one_max],[0,one_max])\n",
    "    plt.ylabel(\"Predicted\")\n",
    "    plt.title(n)\n",
    "    path = path1 + n + \".pdf\"\n",
    "    plt.savefig(path,bbox_inches = 'tight')\n",
    "    plt.show()\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "\n",
    "path = path1 + \"all_fit.pdf\"\n",
    "\n",
    "pred[\"Real\"] = pd.DataFrame(test_y_n)\n",
    "color_sequence = [\"violet\",\"tomato\",\"greenyellow\",\"deepskyblue\",\"indigo\",\"deeppink\",\"cyan\",\n",
    "                \"hotpink\",\"aquamarine\",\"limegreen\",\"cornflowerblue\",\"crimson\",\"darkgoldenrod\"]\n",
    "plt.figure(figsize=(20, 10),edgecolor='white',facecolor='white')\n",
    "cs = 0\n",
    "for mo in np.unique(all_assess3[\"Model\"]):\n",
    "    all_one = all_assess3[all_assess3[\"Model\"]==mo]\n",
    "    plt.plot(all_one[\"Index\"], all_one[\"Predicted\"], '-o', label=mo, color=color_sequence[cs], linewidth=3,markersize=5)\n",
    "    cs = cs+1\n",
    "plt.plot(all_one[\"Index\"], all_one[\"Real\"], '-o', label=\"Real\", color=\"gold\", linewidth=3,markersize=5)\n",
    "# plt.gcf().autofmt_xdate()  # 自动旋转日期标记\n",
    "plt.xticks(fontsize=30)\n",
    "plt.yticks(fontsize=30)\n",
    "plt.xlabel(\"Sample\",fontsize=30)\n",
    "plt.ylabel(\"Confirmed\",fontsize=30)\n",
    "plt.grid()\n",
    "plt.title(\"Predict fit\",fontsize=30)\n",
    "plt.legend(prop={\"size\":25})\n",
    "plt.savefig(path)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![6](./img/6.png)\n",
    "![7](./img/7.png)\n",
    "![8](./img/8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plant results plot 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def machine_ensemble_plot_t(true, DT, Catb, Ada, EXT, LR, SVM, KNN, \n",
    "                            XG, Bagging, GBDT, RF, LGBM, dates=ts):\n",
    "    \n",
    "    # 生成横纵坐标信息\n",
    "    xs = [datetime.strptime(d, '%Y/%m/%d %H') for d in dates]\n",
    "    \n",
    "    # 配置横坐标\n",
    "    #plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y/%m/%d %H'))\n",
    "    #plt.gca().xaxis.set_major_locator(mdates.DayLocator())\n",
    "    # Plot\n",
    "    color_sequence = ['#1f77b4', '#aec7e8', '#ff7f0e', '#ffbb78', '#2ca02c',\n",
    "                      '#98df8a', '#d62728', '#ff9896', '#9467bd', '#c5b0d5',\n",
    "                      '#8c564b', '#c49c94', '#e377c2', '#f7b6d2', '#7f7f7f',\n",
    "                      '#c7c7c7', '#bcbd22', '#dbdb8d', '#17becf', '#9edae5']\n",
    "\n",
    "    plt.figure(num=None, figsize=(20, 10), dpi=80, facecolor='w', edgecolor='k')\n",
    "    plt.plot(xs, true, '-o', label='True', color=color_sequence[0], linewidth=2)\n",
    "    plt.plot(xs, DT, '-o', label='DT', color=color_sequence[1], linewidth=2)\n",
    "    plt.plot(xs, Catb, '-o', label='Catb', color=color_sequence[2], linewidth=2)\n",
    "    plt.plot(xs, Ada, '-o', label='Ada', color=color_sequence[3], linewidth=2)\n",
    "    plt.plot(xs, EXT, '-o', label='EXT', color=color_sequence[4], linewidth=2)\n",
    "    plt.plot(xs, LR, '-o', label='LR', color=color_sequence[5], linewidth=2)\n",
    "    plt.plot(xs, SVM, '-o', label='SVM', color=color_sequence[6], linewidth=2)\n",
    "    plt.plot(xs, KNN, '-o', label='KNN', color=color_sequence[7], linewidth=2)\n",
    "    plt.plot(xs, XG, '-o', label='XG', color=color_sequence[8], linewidth=2)\n",
    "    plt.plot(xs, Bagging, '-o', label='Bagging', color=color_sequence[9], linewidth=2)\n",
    "    plt.plot(xs, GBDT, '-o', label='GBDT', color=color_sequence[10], linewidth=2)\n",
    "    plt.plot(xs, RF, '-o', label='RF', color=color_sequence[11], linewidth=2)\n",
    "    plt.plot(xs, LGBM, '-o', label='LGBM', color=color_sequence[12], linewidth=2)\n",
    "    #plt.gcf().autofmt_xdate()  # 自动旋转日期标记\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "machine_ensemble_plot_t(test_y, machine_ensemble[0], machine_ensemble[1], \n",
    "                        machine_ensemble[2], machine_ensemble[3], machine_ensemble[4], \n",
    "                        machine_ensemble[5], machine_ensemble[6], machine_ensemble[7],\n",
    "                        machine_ensemble[8], machine_ensemble[9], machine_ensemble[10], machine_ensemble[11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1](./img/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plant results plot 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_evl(model_num, model_name, value, err, score_name):\n",
    "    '''\n",
    "    model_num：模型个数 int\n",
    "    model_name：模型名称 str list\n",
    "    value：不同模型的同一评估指标 list\n",
    "    err：不同模型评估指标的误差 list\n",
    "    score_name: 当前画的指标名字 str\n",
    "    '''\n",
    "    color_sequence = ['#1f77b4', '#aec7e8', '#ff7f0e', '#ffbb78', '#2ca02c',\n",
    "                      '#98df8a', '#d62728', '#ff9896', '#9467bd', '#c5b0d5',\n",
    "                      '#8c564b', '#c49c94', '#e377c2', '#f7b6d2', '#7f7f7f',\n",
    "                      '#c7c7c7', '#bcbd22', '#dbdb8d', '#17becf', '#9edae5']\n",
    "    \n",
    "    ind = np.arange(model_num)\n",
    "    \n",
    "    plt.figure(num=None, figsize=(10, 8), dpi=80, facecolor='w', edgecolor='k')\n",
    "    plt.title('Score of ' + score_name)\n",
    "    for i in range(model_num):\n",
    "        plt.bar(i, value[i], yerr=err[i], color = color_sequence[i])\n",
    "        \n",
    "    plt.xticks(ind, model_name)  \n",
    "    \n",
    "score_name = ['MSE', 'RMSE', 'MAE', 'MAD', 'MAPE', 'R square', 'R2_adjusted', 'RMSLE']\n",
    "model_name = ['Ada', 'Bagging', 'Catb', 'DT', 'EXT', 'KNN', 'LGBM' , 'RF', 'SVM', 'XG', 'ANN','BLS','ELM','LSTM']\n",
    "model_num = 14\n",
    "\n",
    "i=0\n",
    "plot_evl(model_num, model_name, scores[i], errs[i], score_name[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![2](./img/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latex table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assess = []\n",
    "for a in all_assess:\n",
    "    ass = []\n",
    "    for i in range(len(a)):\n",
    "        if(type(a[i]) ==  np.float64):\n",
    "            a[i] = round(a[i],4)\n",
    "        ass.append(str(a[i]))\n",
    "    assess.append(ass)\n",
    "sub = \" & \"\n",
    "table = \"\"\n",
    "for ae in assess:\n",
    "    t = sub.join(ae) + \" \\\\\\\\\" \n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assess = []\n",
    "for a in all_assess:\n",
    "    ass = []\n",
    "    for i in range(len(a)):\n",
    "        if(type(a[i]) ==  np.float64):\n",
    "            a[i] = round(a[i],4)\n",
    "        ass.append(str(a[i]))\n",
    "    ass.append(\"Test\")\n",
    "    assess.append(ass)\n",
    "sub = \" & \"\n",
    "table = \"\"\n",
    "for ae in assess:\n",
    "    t = sub.join(ae) + \" \\\\\\\\ \\n \\hline\" \n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save models and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "mkdir(\"./model/\")\n",
    "\n",
    "def save_model(wk):\n",
    "    path = \"./model/KNN\" + \"_\" + wk + \".m\" \n",
    "    joblib.dump(knn,path)\n",
    "    path = \"./model/DT\" + \"_\" + wk + \".m\" \n",
    "    joblib.dump(dt,path)\n",
    "    path = \"./model/SVR\" + \"_\" + wk + \".m\" \n",
    "    joblib.dump(svr,path)\n",
    "    path = \"./model/Ada\" + \"_\" + wk + \".m\" \n",
    "    joblib.dump(ada,path)\n",
    "    path = \"./model/RF\" + \"_\" + wk + \".m\" \n",
    "    joblib.dump(rf,path)\n",
    "    path = \"./model/GBDT\" + \"_\" + wk + \".m\" \n",
    "    joblib.dump(gbrg,path)\n",
    "    path = \"./model/ET\" + \"_\" + wk + \".m\" \n",
    "    joblib.dump(ext,path)\n",
    "    path = \"./model/LR\" + \"_\" + wk + \".m\" \n",
    "    joblib.dump(lr,path)\n",
    "    path = \"./model/CAT\" + \"_\" + wk + \".m\" \n",
    "    joblib.dump(cbr,path)\n",
    "    path = \"./model/LGB\" + \"_\" + wk + \".m\" \n",
    "    joblib.dump(lgbr,path)\n",
    "    path = \"./model/XGB\" + \"_\" + wk + \".m\" \n",
    "    joblib.dump(xg,path)\n",
    "    \n",
    "wk = \"add_test\"\n",
    "\n",
    "save_model(wk)\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "\n",
    "wk = \"add_test\"\n",
    "\n",
    "Seq.to_csv(\"./result/Seq_\"+wk+\".csv\",index=None)\n",
    "pred.to_csv(\"./result/pred_\"+wk+\".csv\",index=None)\n",
    "all_assess1.to_csv(\"./result/all_assess1_\"+wk+\".csv\",index=None)\n",
    "all_assess2.to_csv(\"./result/all_assess2_\"+wk+\".csv\",index=None)\n",
    "all_assess3.to_csv(\"./result/all_assess3_\"+wk+\".csv\",index=None)\n",
    "Seq.to_csv(\"./result/Seq_\"+wk+\".csv\",index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dat[3] 为 python时间戳\n",
    "nums = 0\n",
    "t = (dat[3][1:].reset_index(drop=True)  - dat[3][:-1].reset_index(drop=True)).dt.days\n",
    "if(t[t>1].shape[0]>0):\n",
    "    nums = nums+1\n",
    "print(nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转换时间戳为正常格式的string\n",
    "da['Time'] = pd.to_datetime(da['Time_s'],origin='unix',unit='ms').apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "da['datetime'] = pd.to_datetime(pd.to_datetime(da['Time_s'],origin='unix',unit='ms').apply(lambda x: x.strftime('%Y-%m-%d')),format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![17](./img/17.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datt = pd.to_datetime(da['Time_s'],origin='unix',unit='ms').apply(lambda x: x.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "da['datetime'] = pd.to_datetime(datt,format='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![18](./img/18.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清洗数据，转换日期特征\n",
    "data[\"Release_date_month\"] = data[\"Release_date_month\"].astype(str)\n",
    "data[\"Release_date_month\"] = data[\"Release_date_month\"].replace({\"January\":\"1\",\n",
    " \"February\":\"2\",\n",
    " \"March\":\"3\",\n",
    " \"April\":\"4\",\n",
    " \"May\":\"5\",\n",
    " \"June\":\"6\",\n",
    " \"July\":\"7\",\n",
    " \"August\":\"8\",\n",
    " \"September\":\"9\",\n",
    " \"October\":\"10\",\n",
    " \"November\":\"11\",\n",
    " \"December\":\"12\"})\n",
    "data[\"Release_date_month\"] = data[\"Release_date_month\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清洗数据，转换日期特征\n",
    "ld = []\n",
    "for saf in Merge_dataset[\"Release Date\"].values:\n",
    "    if(saf != \"N/A  \"):\n",
    "        saf = saf[:-2]\n",
    "        saf = saf.replace(\"Jan\",\"01\")\n",
    "        saf = saf.replace(\"Feb\",\"02\")\n",
    "        saf = saf.replace(\"Mar\",\"03\")\n",
    "        saf = saf.replace(\"Apr\",\"04\")\n",
    "        saf = saf.replace(\"May\",\"05\")\n",
    "        saf = saf.replace(\"Jun\",\"06\")\n",
    "        saf = saf.replace(\"Jul\",\"07\")\n",
    "        saf = saf.replace(\"Aug\",\"08\")\n",
    "        saf = saf.replace(\"Sep\",\"09\")\n",
    "        saf = saf.replace(\"Oct\",\"10\")\n",
    "        saf = saf.replace(\"Nov\",\"11\")\n",
    "        saf = saf.replace(\"Dec\",\"12\")\n",
    "        saf = saf.replace(\"th\",\"\").replace(\"st\",\"\").replace(\"nd\",\"\").replace(\"rd\",\"\").replace(\" \",\"-\")\n",
    "        ld.append(saf)\n",
    "    else:\n",
    "        ld.append(\"-1\")\n",
    "Merge_dataset[\"Release Date\"] = pd.DataFrame(ld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取时间特征\n",
    "D_year = []\n",
    "D_month = []\n",
    "D_day = []\n",
    "D_hour = []\n",
    "D_minute = []\n",
    "D_second = []\n",
    "for t in d[\"DateTime\"].values:\n",
    "    dt = t.split(\" \")\n",
    "    D_year.append(int(dt[0].split(\"-\")[0]))\n",
    "    D_month.append(int(dt[0].split(\"-\")[1]))\n",
    "    D_day.append(int(dt[0].split(\"-\")[2]))\n",
    "    D_hour.append(int(dt[1].split(\":\")[0]))\n",
    "    D_minute.append(int(dt[1].split(\":\")[1]))\n",
    "    D_second.append(int(dt[1].split(\":\")[2]))\n",
    "d[\"D_year\"] = pd.DataFrame(D_year)\n",
    "d[\"D_month\"] = pd.DataFrame(D_month)\n",
    "d[\"D_day\"] = pd.DataFrame(D_day)\n",
    "d[\"D_hour\"] = pd.DataFrame(D_hour)\n",
    "d[\"D_minute\"] = pd.DataFrame(D_minute)\n",
    "d[\"D_second\"] = pd.DataFrame(D_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算周特征和星期特征\n",
    "number_of_week = []\n",
    "week_day = []\n",
    "for d in pd.to_datetime(data[\"DateTime\"],format=(\"%Y-%m-%d %H:%M:%S\")):\n",
    "    week_day.append(d.isocalendar()[2])\n",
    "    number_of_week.append(d.isocalendar()[1])\n",
    "data[\"Number_of_week\"] = pd.DataFrame(number_of_week)\n",
    "data[\"Week_day\"] = pd.DataFrame(week_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计缺失\n",
    "def lack_of_analysis1(c_name):\n",
    "    num_d = 0\n",
    "    num_c = 0\n",
    "    for c in c_name:\n",
    "        dir_name = \"./video_game_csv/\"+c+\"/\"\n",
    "        li = os.listdir(dir_name)\n",
    "        for l in li:\n",
    "            path = dir_name + l\n",
    "            data = pd.read_csv(path)\n",
    "            num_c = num_c+1\n",
    "            if(data[data[\"Total\"].isnull()].shape[0]>0):\n",
    "                num_d = num_d+1\n",
    "    print(\"Missing :{}\".format(num_d/num_c))\n",
    "    return num_d,num_c\n",
    "\n",
    "def lack_of_analysis2(c_name):\n",
    "    num_d = 0\n",
    "    num_c = 0\n",
    "    for c in c_name:\n",
    "        dir_name = \"./video_game_csv/\"+c+\"/\"\n",
    "        li = os.listdir(dir_name)\n",
    "        for l in li:\n",
    "            path = dir_name + l\n",
    "            data = pd.read_csv(path)\n",
    "            num_c = num_c+1\n",
    "            if(data[data[\"Total\"]==-1].shape[0]>0):\n",
    "                num_d = num_d+1\n",
    "    print(\"Missing :{}\".format(num_d/num_c))\n",
    "    return num_d,num_c\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法：因为缺失值的是每周销量和总销量，假如某一天缺失了，\n",
    "# 并且下周的每周销量和总销量，就可以补全一个总销量的数据值。\n",
    "\n",
    "def fill_1(c_name,num_d):\n",
    "    num_f = 0\n",
    "    for c in c_name:\n",
    "        dir_name = \"./video_game_csv/\"+c+\"/\"\n",
    "        li = os.listdir(dir_name)\n",
    "        for l in li:\n",
    "            path = dir_name + l\n",
    "            data = pd.read_csv(path)\n",
    "            if(data[data[\"Total\"].isnull()].shape[0]>0):\n",
    "                data[\"Total\"] = data[\"Total\"].fillna(-1)\n",
    "                data.to_csv(path,index=0)\n",
    "                num_f = num_f+1\n",
    "    print(num_f == num_d)\n",
    "    return num_f\n",
    "\n",
    "def fill_2(c_name):\n",
    "    num_fd = 0\n",
    "    for c in c_name:\n",
    "        dir_name = \"./video_game_csv/\"+c+\"/\"\n",
    "        li = os.listdir(dir_name)\n",
    "        for l in li:\n",
    "            path = dir_name + l\n",
    "            data = pd.read_csv(path)\n",
    "            if(data[data[\"Total\"]==-1].shape[0]>0):\n",
    "                for now in data[data[\"Total\"]==-1].index.tolist():\n",
    "                    if(now<data.shape[0]-1):\n",
    "                        next_w = data.loc[now+1,\"Weekly\"]\n",
    "                        next_t = data.loc[now+1,\"Total\"]\n",
    "                        if(next_t!=-1 and next_w!='Pro'):\n",
    "                            fill = int(next_t) - int(next_w)\n",
    "                            data.loc[now,\"Total\"] = fill\n",
    "                data.to_csv(path,index=0)\n",
    "                num_fd = num_fd+1\n",
    "    return num_fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建时间序列数据\n",
    "def create_seq(data,num=2,later=1,ignore=None,y=None,drop_c=None):\n",
    "    if(drop_c!=None):\n",
    "        data = data.drop(columns=drop_c)\n",
    "    later = later-1\n",
    "    if(isinstance(data,pd.DataFrame)): # 先判断类型是否是pd.DataFrame，不是的话直接退出\n",
    "        if(ignore!=None and y!=None):\n",
    "            print(\"1\")\n",
    "            # ingure 和 y 都有指定\n",
    "            # 取出y值\n",
    "            seq = pd.DataFrame(data[y].values[num+later:].tolist(),columns=[\"Y\"])\n",
    "            for c in data.columns: # 遍历所有的列名\n",
    "                if c not in ignore: # 判断列名是否在ignore里\n",
    "                    cl = data[c].values # 取出相应列名的列\n",
    "                    cl_time = np.array([cl[n:n+num] for n in range(cl.shape[0]-num-later)]) # 按每次取num行出来，直到倒数第num+later+1行结束\n",
    "                    cl_name = [c+\"_\"+str(i+1) for i in range(num)] # 取列名\n",
    "                    data2 = pd.DataFrame(cl_time.reshape(-1,num),columns=cl_name)\n",
    "                    seq = pd.concat([data2,seq],axis=1) # 跟之前存在的数据进行拼接\n",
    "                else:\n",
    "                    seq[c] = data[c].values[num+later:]\n",
    "        elif(ignore!=None and y==None):\n",
    "            print(\"2\")\n",
    "            # ingure有指定，但是y没指定，则取最后一列\n",
    "            # 取出y值\n",
    "            seq = pd.DataFrame(data.iloc[num+later:,-1].values.tolist(),columns=[\"Y\"])\n",
    "            for c in data.columns:\n",
    "                if c not in ignore:\n",
    "                    cl = data[c].values\n",
    "                    cl_time = np.array([cl[n:n+num] for n in range(cl.shape[0]-num-later)])\n",
    "                    cl_name = [c+\"_\"+str(i+1) for i in range(num)]\n",
    "                    data2 = pd.DataFrame(cl_time.reshape(-1,num),columns=cl_name)\n",
    "                    seq = pd.concat([data2,seq],axis=1)\n",
    "                else:\n",
    "                    seq[c] = data[c].values[num+later:]\n",
    "        elif(ignore==None and y!=None):\n",
    "            print(\"3\")\n",
    "            # y有指定，但是ingure没指定，则取除去y列剩下的所有\n",
    "            # 取出y值\n",
    "            seq = pd.DataFrame(data[y].values[num+later:].tolist(),columns=[\"Y\"])\n",
    "            for c in data.columns:\n",
    "                cl = data[c].values\n",
    "                cl_time = np.array([cl[n:n+num] for n in range(cl.shape[0]-num-later)])\n",
    "                cl_name = [c+\"_\"+strstr(i+1) for i in range(num)]\n",
    "                data2 = pd.DataFrame(cl_time.reshape(-1,num),columns=cl_name)\n",
    "                seq = pd.concat([data2,seq],axis=1)\n",
    "        elif(ignore==None and y==None):\n",
    "            print(\"4\")\n",
    "            # y , ingure都没指定，则取最后一列为y,其他所有特征都做成时序数据\n",
    "            # 取出y值\n",
    "            seq = pd.DataFrame(data.iloc[num+later:,-1].values.tolist(),columns=[\"Y\"])\n",
    "            for c in data.columns:\n",
    "                cl = data[c].values\n",
    "                cl_time = np.array([cl[n:n+num] for n in range(cl.shape[0]-num-later)])\n",
    "                cl_name = [c+\"_\"+strstr(i+1) for i in range(num)]\n",
    "                data2 = pd.DataFrame(cl_time.reshape(-1,num),columns=cl_name)\n",
    "                seq = pd.concat([data2,seq],axis=1)   \n",
    "        print(\"shape:{}\".format(seq.shape))\n",
    "    else:\n",
    "        print(\"Error: type is not pd.DataFrame.\")\n",
    "        return\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_seq(data,num=2,later=1,ignore=None,y=None,drop_c=None,is_sum=1):\n",
    "    if(drop_c!=None):\n",
    "        data = data.drop(columns=drop_c)\n",
    "    column = data.columns\n",
    "    later = later-1\n",
    "    if(isinstance(data,pd.DataFrame)): # 先判断类型是否是pd.DataFrame，不是的话直接退出\n",
    "        if(ignore!=None and y!=None):\n",
    "            print(\"1\")\n",
    "            # ingure 和 y 都有指定\n",
    "            # 取出y值\n",
    "            if(is_sum==1):\n",
    "                seq = pd.DataFrame(data[y].values[num+later+is_sum:].tolist(),columns=[\"Y\"])\n",
    "            else:\n",
    "                sy = []\n",
    "                for n in range(data.shape[0]-later-is_sum-num):\n",
    "                    sy.append(np.sum(data[y].iloc[n+later+num+1:n+is_sum+later+num+1].values))\n",
    "                seq = pd.DataFrame(sy,columns=[\"Y\"])\n",
    "            for c in data.columns: # 遍历所有的列名\n",
    "                if c not in ignore: # 判断列名是否在ignore里\n",
    "                    cl = data[c].values # 取出相应列名的列\n",
    "                    cl_time = np.array([cl[n:n+num] for n in range(cl.shape[0]-num-later-is_sum)]) # 按每次取num行出来，直到倒数第num+later+1行结束\n",
    "                    cl_name = [c+\"_\"+str(i+1) for i in range(num)] # 取列名\n",
    "                    data2 = pd.DataFrame(cl_time.reshape(-1,num),columns=cl_name)\n",
    "                    seq = pd.concat([data2,seq],axis=1) # 跟之前存在的数据进行拼接\n",
    "                else:\n",
    "                    seq[c] = data[c].values[num+later+is_sum:]\n",
    "        elif(ignore!=None and y==None):\n",
    "            print(\"2\")\n",
    "            # ingure有指定，但是y没指定，则取最后一列\n",
    "            # 取出y值\n",
    "            y = -1\n",
    "            if(is_sum==1):\n",
    "                seq = pd.DataFrame(data.iloc[num+later+is_sum:,-1].values.tolist(),columns=[\"Y\"])\n",
    "            else:\n",
    "                sy = []\n",
    "                for n in range(data.shape[0]-later-is_sum-num):\n",
    "                    sy.append(np.sum(data[y].iloc[n+later+num+1:n+is_sum+later+num+1].values))\n",
    "                seq = pd.DataFrame(sy,columns=[\"Y\"])\n",
    "            for c in data.columns:\n",
    "                if c not in ignore:\n",
    "                    cl = data[c].values\n",
    "                    cl_time = np.array([cl[n:n+num] for n in range(cl.shape[0]-num-later-is_sum)])\n",
    "                    cl_name = [c+\"_\"+str(i+1) for i in range(num)]\n",
    "                    data2 = pd.DataFrame(cl_time.reshape(-1,num),columns=cl_name)\n",
    "                    seq = pd.concat([data2,seq],axis=1)\n",
    "                else:\n",
    "                    seq[c] = data[c].values[num+later+is_sum:]\n",
    "        elif(ignore==None and y!=None):\n",
    "            print(\"3\")\n",
    "            # y有指定，但是ingure没指定，则取除去y列剩下的所有\n",
    "            # 取出y值\n",
    "            if(is_sum==1):\n",
    "                seq = pd.DataFrame(data[y].values[num+later+is_sum:].tolist(),columns=[\"Y\"])\n",
    "            else:\n",
    "                sy = []\n",
    "                for n in range(data.shape[0]-later-is_sum-num):\n",
    "                    sy.append(np.sum(data[y].iloc[n+later+num+1:n+is_sum+later+num+1].values))\n",
    "                seq = pd.DataFrame(sy,columns=[\"Y\"])\n",
    "            for c in data.columns:\n",
    "                cl = data[c].values\n",
    "                cl_time = np.array([cl[n:n+num] for n in range(cl.shape[0]-num-later-is_sum)])\n",
    "                cl_name = [c+\"_\"+str(i+1) for i in range(num)]\n",
    "                data2 = pd.DataFrame(cl_time.reshape(-1,num),columns=cl_name)\n",
    "                print(data2.shape[0])\n",
    "                seq = pd.concat([data2,seq],axis=1)\n",
    "        elif(ignore==None and y==None):\n",
    "            print(\"4\")\n",
    "            # y , ingure都没指定，则取最后一列为y,其他所有特征都做成时序数据\n",
    "            # 取出y值\n",
    "            if(is_sum==1):\n",
    "                seq = pd.DataFrame(data.iloc[num+later+is_sum:,-1].values.tolist(),columns=[\"Y\"])\n",
    "            else:\n",
    "                sy = []\n",
    "                for n in range(data.shape[0]-later-is_sum-num):\n",
    "                    sy.append(np.sum(data[y].iloc[n+later+num+1:n+is_sum+later+num+1].values))\n",
    "                seq = pd.DataFrame(sy,columns=[\"Y\"])\n",
    "            for c in data.columns:\n",
    "                cl = data[c].values\n",
    "                cl_time = np.array([cl[n:n+num] for n in range(cl.shape[0]-num-later-is_sum)])\n",
    "                cl_name = [c+\"_\"+str(i+1) for i in range(num)]\n",
    "                data2 = pd.DataFrame(cl_time.reshape(-1,num),columns=cl_name)\n",
    "                seq = pd.concat([data2,seq],axis=1)   \n",
    "        print(\"shape:{}\".format(seq.shape))\n",
    "    else:\n",
    "        print(\"Error: type is not pd.DataFrame.\")\n",
    "        return\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(Data.corr(),vmax=.8, square=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_name = \"Y\"\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.distplot(np.abs(Data.drop(columns=[\"Time\"]).corr()[p_name].sort_values()[:-1].values),label=\"pearson\",bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_name = \"Y\"\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.distplot(Data.drop(columns=[\"Time\"]).corr()[p_name].sort_values()[:-1].values,label=\"pearson\",bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.drop(columns=[\"Time\"]).corr()[\"Y\"].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20), dpi= 80)\n",
    "sns.pairplot(Data, kind=\"scatter\", plot_kws=dict(s=80, edgecolor=\"white\", linewidth=2.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8), dpi= 80)\n",
    "sns.pairplot(df, kind=\"reg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use = Train.corr().sort_values(by=\"Y\")[\"Y\"][np.abs(Train.corr().sort_values(by=\"Y\")[\"Y\"].values > 0.40)].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train.corr().sort_values(by=\"Y\")[\"Y\"][np.abs(Train.corr().sort_values(by=\"Y\")[\"Y\"].values > 0.50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "y = Train[\"Y\"]\n",
    "SP = {}\n",
    "for i in Train.drop(columns=[\"Time\"]).columns:\n",
    "    sp = scipy.stats.pearsonr(Train[i],y)\n",
    "    print(\"{}  :  cor -- {} ， p-v  --- {}\".format(i,sp[0],sp[1]))\n",
    "    SP[i] = sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "s = StandardScaler()\n",
    "s.fit(Train.drop(columns=[\"Y\",\"Time\"]))\n",
    "train_x1 = s.transform(Train.drop(columns=[\"Y\",\"Time\"]))\n",
    "train_y1 = Train[\"Y\"].values\n",
    "\n",
    "rf = RandomForestRegressor(n_jobs=-1,n_estimators=100)\n",
    "rf.fit(train_x1,train_y1)\n",
    "\n",
    "feature = Train.drop(columns=[\"Y\",\"Time\"]).columns.tolist()\n",
    "importances=rf.feature_importances_\n",
    "indices=np.argsort(importances)[::-1] # 从大到小提取索引\n",
    "for f in range(train_x1.shape[1]):\n",
    "    print (\"%2d) %-*s %f\" % (f+1,30,feature[indices[f]],importances[indices[f]]))\n",
    "    \n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title('Feature Importance-RandomForest')\n",
    "plt.barh(range(train_x1.shape[1]),importances[indices],color='lightblue',align='center')\n",
    "# plt.yticks(range(train_x_n.shape[1]),feature,rotation=90)\n",
    "plt.ylim([-1,train_x1.shape[1]])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.distplot(importances)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine importance and correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = {}\n",
    "n = train_x1.shape[1]\n",
    "for f in range(train_x1.shape[1]):\n",
    "    score[feature[indices[f]]] = n\n",
    "    n = n-1\n",
    "\n",
    "fe = Train.corr().sort_values(by=\"Y\")[\"Y\"].index.tolist()[:-1]\n",
    "n = 1\n",
    "for f in range(train_x1.shape[1]):\n",
    "    score[fe[f]] = score[fe[f]] + n\n",
    "    n = n+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_results = sorted(score.items(),key=lambda item:item[1])\n",
    "y = []\n",
    "x = []\n",
    "for i in score_results:\n",
    "    y.append(i[0])\n",
    "    x.append(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 中文字体设置-黑体\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 解决保存图像是负号'-'显示为方块的问题\n",
    "sns.set(font='SimHei',font_scale=1.5)\n",
    "\n",
    "plt.figure(figsize=(10,15))\n",
    "plt.title('Feature score')\n",
    "plt.barh(range(train_x1.shape[1]),x,color='lightblue',align='center',label=y)\n",
    "y_pos = np.arange(len(y))\n",
    "plt.yticks(y_pos, y)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 中文字体设置-黑体\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 解决保存图像是负号'-'显示为方块的问题\n",
    "sns.set(font='SimHei',font_scale=1.5)\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.title('Feature score')\n",
    "plt.bar(range(train_x1.shape[1]),x,color='lightblue',align='center',label=y)\n",
    "x_pos = np.arange(len(y))\n",
    "plt.xticks(x_pos, y,rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance inflation factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![53](./img/53.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   VIF Factor columns\n",
      "0       22.95       a\n",
      "1        3.00       b\n",
      "2       12.95       c\n",
      "3        3.00       d\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "df= pd.DataFrame(\n",
    "{'a': [1, 1, 2, 3, 4],\n",
    "'b': [2, 2, 3, 2, 1],\n",
    "'c': [4, 6, 7, 8, 9],\n",
    "'d': [4, 3, 4, 5, 4]}\n",
    ")\n",
    "X= StandardScaler().fit_transform(df)\n",
    "vif= pd.DataFrame()\n",
    "vif[\"VIF Factor\"] = [variance_inflation_factor(X, i) for i in range(X.shape[1])]\n",
    "vif['columns'] = df.columns\n",
    "print(vif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9564270152505446\n",
      "22.949999999999985\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.regression.linear_model import OLS\n",
    "\n",
    "results = OLS(X[:,0], X[:,1:]).fit()\n",
    "print(results.rsquared)\n",
    "print(1/(1-results.rsquared))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解决多重共线的方法一般有以下三种：\n",
    "\n",
    "（1）向后消除法(Backward elimination)：每次循环，遍历当前还没有剔除的变量，依次计算对应的 VIF，再去除最差的那个变量（也就是VIF值最大的变量），一直循环，直至变量数目少于预期个数或者所有的变量VIF值都小于VIF阈值。一般而言 VIF > 10，认为存在共线性。\n",
    "\n",
    "（2）PCA降维：PCA降维后，所有提取的主成分间两两独立，所以不会再有共线性。\n",
    "\n",
    "（3）岭回归分析法：岭回归线性回归在线性回归的基础上新增了一个惩罚项，解决了共线性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"digit recognizor.csv\")\n",
    "X = data.iloc[:,1:]\n",
    "y = data.iloc[:,0]\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    " \n",
    "selector = VarianceThreshold()         #实例化，不填参数默认方差为0\n",
    "X_var0 = selector.fit_transform(X)         #获取删除不合格特征之后的新特征矩阵\n",
    "#也可以直接写成 X = VairanceThreshold().fit_transform(X)\n",
    "# 可能变更好，代表被滤掉的特征大部分是噪音\n",
    "# 也可能变糟糕，代表被滤掉的特征中很多都是有效特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:ML]",
   "language": "python",
   "name": "conda-env-ML-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "276px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
